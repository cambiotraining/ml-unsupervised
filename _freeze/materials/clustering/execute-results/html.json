{
  "hash": "633303ee77d30ce2a0866614a066fbb7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Clustering\n---\n\n::: {.callout-tip}\n#### Learning Objectives\n\n- By the end of this lesson, students will be able to:\n\n1. **Explain the goal and applications of clustering**  \n   - Define unsupervised learning and distinguish clustering from classification.  \n   - Cite real‚Äëworld uses (e.g., customer segmentation, anomaly detection).\n\n2. **Describe and execute hierarchical clustering methods**  \n   - Compute and interpret a dendrogram.  \n   - Apply various linkage criteria (single, complete, average, Ward‚Äôs, etc.) and distance metrics (Euclidean, Manhattan, cosine).\n\n3. **Implement and analyze K‚ÄëMeans clustering**  \n   - Articulate the iterative assignment‚Äìupdate steps and convergence conditions.  \n   - Write or follow pseudocode for K‚ÄëMeans, including centroid initialization strategies.  \n   - Compute and interpret the within‚Äëcluster variation objective.\n\n4. **Compare clustering techniques and select appropriately**  \n   - Identify strengths and weaknesses of hierarchical vs. K‚ÄëMeans approaches.  \n   - Choose between methods based on data characteristics (number of clusters, scalability, hierarchy needs).\n:::\n<!-- end callout -->\n\n\n## Warm-up Puzzle\n\n* Is the picture below fake or real?\n\n* How can a computer determine if the picture below is fake or real?\n\n![Taj Mahal bathed in the Northern Lights. Generated using the DALL-E tool.](images/taj_mahal.jpg)\n\n\n## Clustering Overview\n\nClustering is an **unsupervised learning** technique used to group similar data points together. Unlike classification, there are no pre-defined labels. Instead, the algorithm tries to discover structure in the data by maximizing intra-cluster similarity and minimizing inter-cluster similarity.\n\n**Key points:**\n\n- **Objective:** Identify natural groupings in the data.  \n\n- **Applications:** Customer segmentation, image compression, anomaly detection, document clustering.\n\n\n## Hierarchical Clustering\n\nHierarchical clustering builds a tree (dendrogram) of clusters using either a **bottom‚Äëup** (agglomerative) or **top‚Äëdown** (divisive) approach.\n\n### Agglomerative (Bottom‚ÄëUp)\n\n1. **Initialization:** Start with each data point as its own cluster.  \n\n2. **Merge Steps:**  \n   - Compute distance between every pair of clusters.  \n   - Merge the two closest clusters.  \n   - Update the distance matrix.  \n\n3. **Termination:** Repeat until all points are in a single cluster or a stopping criterion (e.g., desired number of clusters) is met.\n\n\n\n### Dendrogram\n\n```text\n        [ALL POINTS]\n         /      \\\n    Cluster A   Cluster B\n     /    \\       /    \\\n    ‚Ä¶      ‚Ä¶     ‚Ä¶      ‚Ä¶\n```\n\n\n- **Cutting the tree** at different levels yields different numbers of clusters.\n- **Linkage methods** determine how distance between clusters is computed:\n  - **Single linkage:** Minimum pairwise distance  \n  - **Complete linkage:** Maximum pairwise distance  \n  - **Average linkage:** Average pairwise distance  \n\n\n\n\n### Important Concepts\n\n::: {.callout-tip}\n- **Metric**  \n  The *metric* (or *distance function* or *dissimilarity function*) defines how you measure the distance between individual data points. Common choices include Euclidean, Manhattan (cityblock), or cosine distance. This metric determines the raw pairwise distances.\n\n\n*Manhattan distance*\n\n![Manhattan distance. Image created using DALL-E.](images/manhattan_distance.png)\n\n\n*Manhattan distance*\n\n![Manhattan distance](images/manhattan_distance_complex.png)\n\n\n*Euclidean distance*\n\nHow would a crow navigate in Manhattan? (I have never been to Manhattan, but the `internet` says there are crows in Manhattan, so it must be true).\n\n![A crow in Manhattan. Image created using DreamUp.](images/crow_manhattan.jpg)\n\n\n![Euclidean distance](images/euclidean_distance.png)\n\n\n\n- **Linkage**  \n  The *linkage* method defines how to compute the distance between two clusters based on the pairwise distances of their members. Examples:  \n  - **Single**: the distance between the closest pair of points (one from each cluster).  \n  - **Complete**: the distance between the farthest pair of points.  \n  - **Average**: the average of all pairwise distances.  \n  - **Ward**: the merge that minimizes the increase in total within‚Äëcluster variance.  \n\n*Linkage function*\n\n![Linkage function](images/linkage_function.png)\n\n:::\n<!-- end callout -->\n\n\n| Linkage Method        | How It Works                                                                                     | Intuition                                                                                       |\n|-----------------------|--------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|\n| **Single**            | Distance = minimum pairwise distance between points in the two clusters                         | ‚ÄúFriends‚Äëof‚Äëfriends‚Äù ‚Äì clusters join if any two points are close, yielding chain‚Äëlike clusters  |\n| **Complete**          | Distance = maximum pairwise distance between points in the two clusters                         | ‚ÄúEveryone must be close‚Äù ‚Äì only merge when all points are relatively near, producing compact clusters |\n| **Average (UPGMA)**   | Distance = average of all pairwise distances between points in the two clusters                 | Balances single and complete by averaging close and far pairs                                  |\n| **Weighted (WPGMA)**  | Distance = average of the previous cluster‚Äôs distance to the new cluster (equal weight per cluster) | Prevents large clusters from dominating, giving equal say to each cluster                      |\n| **Centroid**          | Distance = distance between the centroids (mean vectors) of the two clusters                    | Merges based on ‚Äúcenters of mass,‚Äù but centroids can shift non‚Äëmonotonically                   |\n| **Median (WPGMC)**    | Distance = distance between the medians of the two clusters                                      | More robust to outliers than centroid linkage, but can also invert dendrogram order            |\n| **Ward‚Äôs**            | Merge that minimizes the increase in total within‚Äëcluster sum of squares (variance)             | Keeps clusters as tight and homogeneous as possible, often resulting in evenly sized groups     |\n\n\n### Single Linkage\n- **How it works**: Measures the distance between two clusters as the smallest distance between any single point in one cluster and any single point in the other.  \n- **Intuition**: ‚ÄúFriends‚Äëof‚Äëfriends‚Äù clustering‚Äîif any two points (one from each cluster) are close, the clusters join. Can produce long, straggly chains of points.\n\n### Complete Linkage\n- **How it works**: Measures the distance between two clusters as the largest distance between any point in one cluster and any point in the other.  \n- **Intuition**: ‚ÄúEveryone must be close‚Äù‚Äîclusters merge only when all their points are relatively near each other, leading to tight, compact groups.\n\n### Average Linkage (UPGMA)\n- **How it works**: Takes the average of all pairwise distances between points in the two clusters.  \n- **Intuition**: A middle‚Äëground between single and complete linkage‚Äîbalances the effect of very close and very far pairs by averaging them.\n\n### Weighted Linkage (WPGMA)\n- **How it works**: Similar to average linkage, but treats each cluster as a single entity by averaging the distance from each original cluster to the target cluster, regardless of cluster size.  \n- **Intuition**: Prevents larger clusters from dominating the average‚Äîgives each cluster equal say in how far apart they are.\n\n### Centroid Linkage\n- **How it works**: Computes the distance between the centroids (mean vectors) of the two clusters.  \n- **Intuition**: Clusters merge based on whether their ‚Äúcenters of mass‚Äù are close. Can sometimes lead to non‚Äëmonotonic merges if centroids shift oddly.\n\n### Median Linkage (WPGMC)\n- **How it works**: Uses the median point of each cluster instead of the mean when computing distance between clusters.  \n- **Intuition**: Like centroid linkage but more robust to outliers, since the median isn‚Äôt pulled by extreme values‚Äîthough can also cause inversion issues.\n\n### Ward‚Äôs Method\n- **How it works**: At each step, merges the two clusters whose union leads to the smallest possible increase in total within‚Äëcluster variance (sum of squared deviations).  \n- **Intuition**: Always chooses the merge that keeps clusters as tight and homogeneous as possible, often yielding groups of similar size and shape.\n\n\n::: {.callout-tip}\n## Concept about distances\n\nThere is no single \"best\" distance metric for clustering‚Äîwhat works well for one dataset or problem may not work for another. The choice of distance metric (such as Euclidean, or Manhattan) depends on the nature of your data and what you want to capture about similarity. \n\nFor example, Euclidean distance works well when the scale of features is meaningful and differences are linear, while cosine distance is better for text data or situations where the direction of the data matters more than its magnitude.\n\nIt is important to experiment with different distance metrics and see which one produces clusters that make sense for your specific problem. Always check the results and, if possible, use domain knowledge to guide your choice.\n:::\n<!-- end callout -->\n\n\n## Practical\n\n* `n_clusters` in `AgglomerativeClustering` specifies the number of clusters you want the algorithm to find. After building the hierarchical tree, the algorithm will cut the tree so that exactly `n_clusters` groups are formed. For example, `n_clusters`=3 will result in 3 clusters in your data.\n\n* The default value for n_clusters in `AgglomerativeClustering` is 2.\nThe default value for linkage is `ward`. So if you do not specify these parameters, the algorithm will produce 2 clusters using `Ward` linkage.\n\n::: {#45eccd57 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data\niris = load_iris()\nX = iris.data\n\n# Fit Agglomerative Clustering\nagg = AgglomerativeClustering(n_clusters=3, linkage='ward')\nlabels = agg.fit_predict(X)\n\nprint(labels)  # Cluster assignments for each sample\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport matplotlib.pyplot as plt\n\n# Compute linkage matrix\nZ = linkage(X, method='ward')\n\n# Plot dendrogram\nplt.figure(figsize=(10, 5))\ndendrogram(Z)\nplt.title('Hierarchical Clustering Dendrogram (Iris Data)')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2\n 2 2 0 0 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 0 2 2 2 0 2 2 2 0 2 2 2 0 2\n 2 0]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](clustering_files/figure-html/cell-2-output-2.png){width=809 height=449}\n:::\n:::\n\n\n### Scale the data\n\n::: {#b9553f24 .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data\niris = load_iris()\nX = iris.data\n\n# scale the data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n```\n:::\n\n\n### Perform hierarchical clustering\n\n::: {#ada6e15c .cell execution_count=3}\n``` {.python .cell-code}\n# Fit Agglomerative Clustering\nagg = AgglomerativeClustering(n_clusters=3)#, linkage='ward')\nlabels = agg.fit_predict(X_scaled)\n\nprint(labels)  # Cluster assignments for each sample\n\n# Compute linkage matrix\nZ = linkage(X_scaled)#, method='ward')\n\n# Plot dendrogram\nplt.figure()\ndendrogram(Z)\nplt.title('Hierarchical Clustering Dendrogram (Iris Data)')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 2 1 1 1 1 1 1 1 1 0 0 0 2 0 2 0 2 0 2 2 0 2 0 2 0 2 2 2 2 0 0 0 0\n 0 0 0 0 0 2 2 2 2 0 2 0 0 2 2 2 2 0 2 2 2 2 2 0 2 2 0 0 0 0 0 0 2 0 0 0 0\n 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](clustering_files/figure-html/cell-4-output-2.png){width=590 height=449}\n:::\n:::\n\n\n## Alternative code using `sns.clustermap`\n\nThere are many ways we can perform hierarchical clustering. An alternative is to use `sns.clustermap` using the `seaborn` package. The interface is similar and it can produce professional quality plots.\n\n```python\n\nimport seaborn as sns\n\n# Basic clustermap with scaling\nsns.clustermap(X, z_score=0, cmap='RdBu_r', center=0)\n\n# Different linkage methods\nsns.clustermap(X, method='average', standard_scale=0)\n\n# Different distance metrics  \nsns.clustermap(X, metric='correlation', method='average')\n\n# Comprehensive example\nsns.clustermap(X, z_score=0, method='average', metric='correlation', \n               cmap='RdBu_r', center=0)\n```\n\n<!-- TODO: show different plots using sns.clustermap see python scripts in scripts directory sns_clustermap*.py -->\n\n\n**`z_score=0`** (Scaling Direction)\n- **What it does**: Standardizes (z-scores) the data before clustering\n- **Options**:\n  - `0`: Scale rows (genes) - each gene's expression is standardized across samples\n  - `1`: Scale columns (samples) - each sample's expression is standardized across genes\n  - `None`: No scaling (use raw data)\n- **Why use `0`**: For gene expression, you want to compare expression patterns, not absolute levels\n\n\n**`cmap='RdBu_r'`** (Color Map)\n- **What it does**: Defines the color scheme for the heatmap\n- **`'RdBu_r'`**: Red-Blue reversed (red = high, blue = low, white = middle)\n- **Other options**: `'viridis'`, `'coolwarm'`, `'seismic'`, `'plasma'`, etc.\n- **Why use it**: Intuitive for biologists (red = high expression, blue = low expression)\n\n**`center=0`** (Color Center)\n- **What it does**: Centers the color map at this value\n- **`0`**: White color represents zero (after scaling, this is the mean)\n- **Other values**: Could center at 1 (for fold-change), or other biologically meaningful values\n- **Why use it**: Makes it easy to see above/below average expression\n\n**Additional Common Parameters**\n\n<!--#### **`figsize=(width, height)`**\n- **What it does**: Controls the size of the plot\n- **Example**: `figsize=(12, 8)` for a 12√ó8 inch plot-->\n\n#### **`row_cluster=True/False`**\n- **What it does**: Whether to cluster rows (genes)\n- **Default**: `True`\n\n#### **`col_cluster=True/False`**\n- **What it does**: Whether to cluster columns (samples)\n- **Default**: `True`\n\n#### **`cbar_kws`** (Color Bar Keywords)\n- **What it does**: Customize the color bar\n- **Example**: `cbar_kws={'label': 'Expression Level', 'shrink': 0.8}`\n\n\n\n## Exercise (changing the linkage function)\n\n* Work in a group for this exercise\n\n* Let us try another linkage function\n\n* Change the linkage function in `Z = linkage(X_scaled), method='ward')`\n\n* How does the clustering change as you change this to another function?\n\n* How does this change if you do *not* scale the data?\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {#5b60f75f .cell execution_count=4}\n``` {.python .cell-code}\n# Fit Agglomerative Clustering on unscaled data with 'average' linkage\nagg = AgglomerativeClustering(linkage='average')\nlabels = agg.fit_predict(X)\n\n# Compute linkage matrix on unscaled data with 'average' linkage\nZ = linkage(X, method='average')\n\n# Plot dendrogram\nplt.figure()\ndendrogram(Z)\nplt.title('Hierarchical Clustering Dendrogram (Iris Data, Unscaled, Average Linkage)')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](clustering_files/figure-html/cell-5-output-1.png){width=628 height=449}\n:::\n:::\n\n\n:::\n<!-- end callout -->\n\n\n## Exercise (trying a different *dissimilarity* metric)\n\n::: {#1e34cf5c .cell execution_count=5}\n``` {.python .cell-code}\n# Fit Agglomerative Clustering on unscaled data with 'average' linkage and 'manhattan' distance\nagg = AgglomerativeClustering(\n    linkage='average',\n    metric='manhattan'      # use metric instead of deprecated affinity\n)\n\n\n# Compute linkage matrix on unscaled data with 'average' linkage and 'cityblock' (manhattan) distance\nZ = linkage(X, method='average', metric='cityblock')\n\n# Plot dendrogram\nplt.figure()\ndendrogram(Z)\nplt.title('Hierarchical Clustering Dendrogram (Iris Data, Unscaled, Average Linkage, Manhattan Distance)')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](clustering_files/figure-html/cell-6-output-1.png){width=789 height=449}\n:::\n:::\n\n\n## Exercise with missing data\n\nReal-world data frequently has missing values. PCA and clustering techniques can struggle on missing data.\n\nIn this exercise, you will work in a group and apply hierarchical clustering and PCA and tSNE on data which has missing values.\n\nRun the code below. All the missing data is available in the variable `missing_data`.\n\n::: {#1fbe4dd2 .cell execution_count=6}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\ndef create_synthetic_biological_data():\n    \"\"\"\n    Create synthetic gene expression data with biological structure and missingness.\n    \n    Returns:\n    - complete_data: Original data without missing values\n    - missing_data: Data with various missingness patterns\n    - true_labels: True cluster labels for evaluation\n    \"\"\"\n    #print(\"Creating synthetic biological dataset...\")\n    \n    # Parameters\n    n_samples = 100\n    n_genes = 50\n    n_clusters = 4\n    \n    # Create base data structure\n    data = np.random.normal(0, 1, (n_samples, n_genes))\n    \n    # Add biological structure (clusters)\n    cluster_size = n_samples // n_clusters\n    \n    # Cluster 1: High expression in genes 0-12, samples 0-24\n    data[0:25, 0:13] += 2.5\n    # Cluster 2: High expression in genes 13-25, samples 25-49  \n    data[25:50, 13:26] += 2.0\n    # Cluster 3: High expression in genes 26-37, samples 50-74\n    data[50:75, 26:38] += 1.8\n    # Cluster 4: Low expression in genes 38-49, samples 75-99\n    data[75:100, 38:50] -= 2.2\n    \n    # Add some noise\n    data += np.random.normal(0, 0.5, data.shape)\n    \n    # Create sample and gene names\n    sample_names = [f'Sample_{i:03d}' for i in range(n_samples)]\n    gene_names = [f'Gene_{chr(65+i//26)}{chr(65+i%26)}' for i in range(n_genes)]\n    \n    # Create DataFrame\n    complete_data = pd.DataFrame(data, index=sample_names, columns=gene_names)\n    \n    # Create true cluster labels\n    true_labels = np.repeat(range(n_clusters), cluster_size)\n    if len(true_labels) < n_samples:\n        true_labels = np.append(true_labels, [n_clusters-1] * (n_samples - len(true_labels)))\n    \n    #print(f\"Created dataset: {complete_data.shape[0]} samples √ó {complete_data.shape[1]} genes\")\n    #print(f\"True clusters: {n_clusters}\")\n    \n    return complete_data, true_labels\n\ndef introduce_missing_data_patterns(complete_data, true_labels):\n    \"\"\"\n    Introduce different types of missing data patterns.\n    \n    Parameters:\n    - complete_data: Original complete dataset\n    - true_labels: True cluster labels\n    \n    Returns:\n    - missing_data: Dataset with missing values\n    - missing_info: Information about missingness patterns\n    \"\"\"\n    #print(\"\\nIntroducing missing data patterns...\")\n    \n    missing_data = complete_data.copy()\n    missing_info = {}\n    \n    # Pattern 1: Missing Completely At Random (MCAR) - 5% random missing\n    #print(\"1. Adding MCAR missingness (5% random)...\")\n    mcar_mask = np.random.random(missing_data.shape) < 0.05\n    missing_data[mcar_mask] = np.nan\n    missing_info['MCAR'] = mcar_mask.sum()\n    \n    # Pattern 2: Missing At Random (MAR) - correlated with expression level\n    #print(\"2. Adding MAR missingness (correlated with high expression)...\")\n    # Higher chance of missing for high expression values\n    high_expr_mask = missing_data > missing_data.quantile(0.8)\n    mar_probability = np.where(high_expr_mask, 0.15, 0.02)  # 15% for high, 2% for low\n    mar_mask = np.random.random(missing_data.shape) < mar_probability\n    missing_data[mar_mask] = np.nan\n    missing_info['MAR'] = mar_mask.sum()\n    \n    # Pattern 3: Missing Not At Random (MNAR) - systematic missing\n    #print(\"3. Adding MNAR missingness (systematic missing)...\")\n    # Missing entire samples (simulating failed experiments)\n    failed_samples = np.random.choice(missing_data.index, size=8, replace=False)\n    missing_data.loc[failed_samples, :] = np.nan\n    missing_info['MNAR_samples'] = len(failed_samples)\n    \n    # Missing entire genes (simulating detection failures)\n    failed_genes = np.random.choice(missing_data.columns, size=5, replace=False)\n    missing_data.loc[:, failed_genes] = np.nan\n    missing_info['MNAR_genes'] = len(failed_genes)\n    \n    # Pattern 4: Block missingness (simulating batch effects)\n    #print(\"4. Adding block missingness (batch effects)...\")\n    # Missing blocks of data (simulating different experimental conditions)\n    block_start_row = 20\n    block_end_row = 35\n    block_start_col = 10\n    block_end_col = 20\n    missing_data.iloc[block_start_row:block_end_row, block_start_col:block_end_col] = np.nan\n    missing_info['Block'] = (block_end_row - block_start_row) * (block_end_col - block_start_col)\n    \n    # Calculate total missingness\n    total_missing = missing_data.isnull().sum().sum()\n    total_values = missing_data.size\n    missing_percentage = (total_missing / total_values) * 100\n    \n    print(f\"\\nMissing data summary:\")\n    print(f\"Total missing values: {total_missing}\")\n    print(f\"Missing percentage: {missing_percentage:.1f}%\")\n    #print(f\"MCAR: {missing_info['MCAR']} values\")\n    #print(f\"MAR: {missing_info['MAR']} values\") \n    #print(f\"MNAR samples: {missing_info['MNAR_samples']} samples\")\n    #print(f\"MNAR genes: {missing_info['MNAR_genes']} genes\")\n    #print(f\"Block missing: {missing_info['Block']} values\")\n    \n    return missing_data, missing_info\n\ndef visualize_missing_patterns(complete_data, missing_data, true_labels):\n    \"\"\"\n    Visualize the missing data patterns.\n    \"\"\"\n    print(\"\\nCreating missing data visualizations...\")\n    \n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # Plot 1: Complete data heatmap\n    plt.subplot(2, 2, 1)\n    sns.heatmap(complete_data.iloc[:50, :30], cmap='RdBu_r', center=0, \n                cbar_kws={'label': 'Expression Level'})\n    plt.title('Complete Data (First 50 samples, 30 genes)')\n    plt.xlabel('Genes')\n    plt.ylabel('Samples')\n    \n    # Plot 2: Missing data pattern\n    plt.subplot(2, 2, 2)\n    missing_mask = missing_data.iloc[:50, :30].isnull()\n    sns.heatmap(missing_mask, cmap='Reds', cbar_kws={'label': 'Missing (1=Yes, 0=No)'})\n    plt.title('Missing Data Pattern')\n    plt.xlabel('Genes')\n    plt.ylabel('Samples')\n    \n    # Plot 3: Missing data by sample\n    plt.subplot(2, 2, 3)\n    missing_by_sample = missing_data.isnull().sum(axis=1)\n    plt.bar(range(len(missing_by_sample)), missing_by_sample)\n    plt.title('Missing Values per Sample')\n    plt.xlabel('Sample Index')\n    plt.ylabel('Number of Missing Values')\n    \n    # Plot 4: Missing data by gene\n    plt.subplot(2, 2, 4)\n    missing_by_gene = missing_data.isnull().sum(axis=0)\n    plt.bar(range(len(missing_by_gene)), missing_by_gene)\n    plt.title('Missing Values per Gene')\n    plt.xlabel('Gene Index')\n    plt.ylabel('Number of Missing Values')\n    \n    plt.tight_layout()\n    plt.show()\n\n\n# Create synthetic data\ncomplete_data, true_labels = create_synthetic_biological_data()\n    \n# Introduce missing data patterns\nmissing_data, missing_info = introduce_missing_data_patterns(complete_data, true_labels)\n    \n# Visualize missing patterns\n#visualize_missing_patterns(complete_data, missing_data, true_labels)\n\n# All the missing data is available in the variable missing_data\n# fill in your code here ...\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nMissing data summary:\nTotal missing values: 1346\nMissing percentage: 26.9%\n```\n:::\n:::\n\n\n* All the missing data is available in the variable `missing_data`. Now perform hierarchical clustering, PCA and tSNE on this data.\n\n\n<!--\nSolutions: in scripts/missing_data_solutions.py \n-->\n\n## (Optional) Exercise on cancer data\n\n* You have been given some data on cancer cell lines\n\n* Team up with someone and perform hierarchical clustering on this data\n\n* You have been given some starter code to help you load the data\n\n* The data has been downloaded and processed for you (after you run the code below). \n\n* The data is in the variable named `X`\n\n<!-- TODO: find out what are labels -->\n\n<!-- TODO: silhouette plots -->\n\n::: {#c5ff44ad .cell execution_count=7}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport os\nimport requests\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport matplotlib.pyplot as plt\n\n# Load data\nX = pd.read_csv(\"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/refs/heads/main/course_files/data/cancer_data_saved_NC160.csv\", index_col=0)\n\n\nprint(\"Fetching labels from GitHub...\")\nlabs_url = 'https://raw.githubusercontent.com/neelsoumya/python_machine_learning/main/data/NCI60labs.csv'\nresponse = requests.get(labs_url)\nresponse.raise_for_status()\n# Read the raw text and split into lines.\nall_lines = response.text.strip().splitlines()\n\n# Skip the first line (the header) to match the data dimensions.\nlabs = all_lines[1:]\n\n# The labels in the file are quoted (e.g., \"CNS\"), so we remove the quotes.\nlabs = [label.strip('\"') for label in labs]\n\n# Your code below ......\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFetching labels from GitHub...\n```\n:::\n:::\n\n\n* Write your code while working in pairs or a group\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {#b552bba9 .cell execution_count=8}\n``` {.python .cell-code}\n# Hierarchical Clustering\nagg = AgglomerativeClustering(linkage='average', metric='manhattan')\ncluster_labels = agg.fit_predict(X)\n\n# Compute linkage matrix for the dendrogram\nZ = linkage(X, method='average', metric='cityblock')\n\n# Plot Dendrogram\nplt.figure()\ndendrogram(Z, labels=labs)\nplt.title('Hierarchical Clustering Dendrogram (NCI60, Average Linkage, Manhattan Distance)')\nplt.xlabel('Cell Line')\nplt.ylabel('Distance')\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](clustering_files/figure-html/cell-9-output-1.png){width=705 height=470}\n:::\n:::\n\n\n:::\n<!-- end callout -->\n\n\n\n### Exercise: try another linkage method and another distance metric\n\n\n::: {.callout-tip}\n## Important Concept (recall)\n\n* There is not \"correct\" answer in unsupervised machine learning!\n\n* So how do you know when you are done?\n:::\n<!-- end callout -->\n\n\n## Evaluating the Quality of Clusters\n\nEvaluating the quality of clusters is a crucial step in any unsupervised learning task. Since we do not have a single _correct_ answer, we use several methods that fall into three main categories:\n\n### 1. Internal Evaluation\nMeasures how good the clustering is based only on the data itself (e.g., how dense and well-separated the clusters are).\n\n### 2. External Evaluation\nMeasures how well the clustering results align with known, ground-truth labels. This is possible here because the NCI60 dataset has known cancer cell line types, which we loaded as `labs`.\n\n### 3. Visual Evaluation\nInspecting plots (like the dendrogram or PCA) to see if the groupings seem logical.\n\n---\n\nLet us add the two most common metrics: one internal and one external.\n\n---\n\n### Internal Evaluation: Silhouette Score\n\nThe **Silhouette Score** measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation).\n\n- **Score Range**: -1 to +1  \n- **Interpretation**:\n  - **+1**: The sample is far away from the neighboring clusters (very good).\n  - **0**: The sample is on or very close to the decision boundary between two neighboring clusters.\n  - **-1**: The sample is assigned to the wrong cluster.\n\n---\n\n### External Evaluation: Adjusted Rand Index (ARI)\n\nThe **Adjusted Rand Index (ARI)** measures the similarity between the true labels (`labs`) and the labels assigned by our clustering algorithm (`cluster_labels`). It accounts for chance groupings.\n\n- **Score Range**: -1 to +1  \n- **Interpretation**:\n  - **+1**: Perfect agreement between true and predicted labels.\n  - **0**: Random labeling (no correlation).\n  - **< 0**: Worse than random labeling.\n\n\n* Here is how you would implement this\n\n::: {#ff1630a4 .cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.metrics import silhouette_score, adjusted_rand_score # Import evaluation metrics\n\n# Hierarchical Clustering\nagg = AgglomerativeClustering(linkage='average', metric='manhattan')\ncluster_labels = agg.fit_predict(X)\n\n# 1. Internal Evaluation: Silhouette Score\n# Measures how well-separated clusters are based on the data itself.\nsilhouette = silhouette_score(X, cluster_labels, metric='manhattan')\nprint(\"Silhouette score\")\nprint(silhouette)\nprint(\"Score is from -1 to 1. Higher is better\")\n\n# 2. External Evaluation: Adjusted Rand Index\n# Compares our cluster labels to the true cancer type labels.\nari = adjusted_rand_score(labs, cluster_labels)\nprint(\"Adjusted Rand Index\")\nprint(ari)\nprint(\"Compares to true labels. Score is from -1 to 1. Higher is better\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSilhouette score\n0.1436950300449066\nScore is from -1 to 1. Higher is better\nAdjusted Rand Index\n0.0554671516253694\nCompares to true labels. Score is from -1 to 1. Higher is better\n```\n:::\n:::\n\n\n* Compare to literature and what others have done\n\n* Plain old visual evaluation\n\n - compare to labels of what these cell lines are (assuming this is available)\n \n<!--TODO: XX what do these labels mean? -->\n\n\n\n\n\n* Silhouette plots\n\nA silhouette plot is a visual diagnostic for clustering quality that (1) computes a silhouette value for each sample and (2) shows the distribution of those values for every cluster. It helps you see which clusters are tight and well-separated and which contain ambiguous or poorly assigned samples.\n\nTODO: simplify code below\n\n::: {#94b96fb0 .cell execution_count=12}\n``` {.python .cell-code}\n\"\"\"\nSilhouette plot utilities.\n\nDependencies:\n  pip install scikit-learn matplotlib numpy\n\nUsage:\n  from sklearn.cluster import KMeans\n  labels = KMeans(n_clusters=4, random_state=0).fit_predict(X)\n  fig = plot_silhouette(X, labels)                     # silhouette only\n  fig = plot_silhouette(X, labels, embedding=umap_emb) # silhouette + 2D embedding\n\"\"\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\ndef plot_silhouette(X, labels, metric='euclidean', embedding=None,\n                    figsize=(10, 5), title=None, show=True, save_path=None):\n    \"\"\"\n    Draw a silhouette plot for clustering labels on dataset X.\n\n    Args:\n      X : array-like, shape (n_samples, n_features)\n      labels : array-like, shape (n_samples,)\n      metric : distance metric passed to silhouette_samples (default 'euclidean')\n      embedding : optional array-like (n_samples, 2). If provided, draws a second panel\n                  showing the 2D embedding colored by cluster.\n      figsize : tuple, figure size\n      title : optional string title for the figure\n      show : if True, call plt.show()\n      save_path : path to save figure (PNG/PDF). If None, don't save.\n\n    Returns:\n      matplotlib.figure.Figure\n    \"\"\"\n    X = np.asarray(X)\n    labels = np.asarray(labels)\n    n_samples = X.shape[0]\n    unique_labels = np.unique(labels)\n    n_clusters = unique_labels.shape[0]\n    if n_clusters < 2:\n        raise ValueError(\"Need at least 2 clusters for silhouette plot (found %d).\" % n_clusters)\n\n    # compute silhouette values\n    sil_vals = silhouette_samples(X, labels, metric=metric)\n    avg_sil = silhouette_score(X, labels, metric=metric)\n\n    # Figure layout: 1 panel if no embedding, 2 panels otherwise\n    if embedding is None:\n        fig, ax1 = plt.subplots(1, 1, figsize=figsize)\n        ax2 = None\n    else:\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize, gridspec_kw={'width_ratios': [1.2, 1]})\n\n    # --- Silhouette panel ---\n    ax1.set_xlim([-0.1, 1])  # silhouette value range (silhouette can be < -0.1 sometimes)\n    # y axis range depends on number of samples\n    y_lower = 10\n    cmap = cm.nipy_spectral\n    for i, cl in enumerate(unique_labels):\n        cl_sil_vals = sil_vals[labels == cl]\n        cl_sil_vals.sort()\n        size_cluster = cl_sil_vals.shape[0]\n        y_upper = y_lower + size_cluster\n\n        color = cmap(float(i) / max(1, (n_clusters - 1)))\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, cl_sil_vals,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n        # label cluster number in the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster, str(cl))\n        y_lower = y_upper + 10  # 10 for spacing between clusters\n\n    ax1.set_title(\"Silhouette plot\" if title is None else f\"Silhouette ‚Äî {title}\")\n    ax1.set_xlabel(\"Silhouette coefficient\")\n    ax1.set_ylabel(\"Cluster\")\n    ax1.axvline(x=avg_sil, color=\"red\", linestyle=\"--\", label=f\"avg = {avg_sil:.3f}\")\n    ax1.legend(loc='upper right')\n    ax1.set_yticks([])  # we use text labels instead\n    ax1.set_xticks(np.linspace(-0.1, 1.0, 6))\n\n    # --- Embedding panel (optional) ---\n    if embedding is not None:\n        emb = np.asarray(embedding)\n        if emb.shape[0] != n_samples or emb.shape[1] != 2:\n            raise ValueError(\"embedding must be shape (n_samples, 2)\")\n        for i, cl in enumerate(unique_labels):\n            mask = labels == cl\n            color = cmap(float(i) / max(1, (n_clusters - 1)))\n            ax2.scatter(emb[mask, 0], emb[mask, 1], s=10, color=color, alpha=0.8, label=str(cl))\n        ax2.set_title(\"2D embedding (colored by cluster)\")\n        ax2.set_xlabel(\"dim 1\")\n        ax2.set_ylabel(\"dim 2\")\n        ax2.legend(title=\"cluster\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')\n\n    plt.tight_layout()\n    if save_path:\n        fig.savefig(save_path, bbox_inches='tight', dpi=200)\n    if show:\n        plt.show()\n    return fig\n\n# --------------------------\n# Example usage:\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nk = 4\nlabels = KMeans(n_clusters=k, random_state=0).fit_predict(X)\n# # optional 2D embedding (PCA or UMAP)\npca2 = PCA(n_components=2).fit_transform(X)\nplot_silhouette(X, labels, embedding=pca2, title=f\"k={k}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](clustering_files/figure-html/cell-13-output-1.png){width=951 height=470}\n:::\n\n::: {.cell-output .cell-output-display execution_count=12}\n![](clustering_files/figure-html/cell-13-output-2.png){width=951 height=470}\n:::\n:::\n\n\n* Notes on interpreting silhouette plots\n\n- Each horizontal block is a cluster; the width at a given vertical position is the silhouette value of a sample.\n\n- Values close to +1 ‚Üí sample is well matched to its own cluster and poorly matched to neighbors.\n\n- Values near 0 ‚Üí sample lies between clusters.\n\n- Negative values ‚Üí sample is likely assigned to the wrong cluster.\n\n- The red dashed line is the average silhouette score; use it as a quick summary, but always inspect per-cluster distributions ‚Äî a high average can hide poorly-formed small clusters.\n\n\n<!--\nThe silhouette value (per sample)\n\nFor sample \nùëñ\ni:\n\nùëé\n(\nùëñ\n)\na(i) = average distance from \nùëñ\ni to all other points in the same cluster (intra-cluster distance).\n\nFor every other cluster \nùê∂\nC, compute the average distance from \nùëñ\ni to points in \nùê∂\nC; let \nùëè\n(\nùëñ\n)\nb(i) be the smallest of those (the nearest other cluster's average distance).\n\nThe silhouette value is\n\nùë†\n(\nùëñ\n)\n=\nùëè\n(\nùëñ\n)\n‚àí\nùëé\n(\nùëñ\n)\nmax\n‚Å°\n(\nùëé\n(\nùëñ\n)\n,\n‚Äâ\nùëè\n(\nùëñ\n)\n)\n.\ns(i)=\nmax(a(i),b(i))\nb(i)‚àía(i)\n\t‚Äã\n\n.\n\nRange: \n‚àí\n1\n‚â§\nùë†\n(\nùëñ\n)\n‚â§\n1\n‚àí1‚â§s(i)‚â§1.\n\nùë†\n(\nùëñ\n)\n‚âà\n1\ns(i)‚âà1: sample is well matched to its own cluster and far from neighbours.\n\nùë†\n(\nùëñ\n)\n‚âà\n0\ns(i)‚âà0: sample lies between two clusters.\n\nùë†\n(\nùëñ\n)\n<\n0\ns(i)<0: sample is probably assigned to the wrong cluster (closer on average to another cluster than to its own).\n\nExample: if \nùëé\n=\n0.3\na=0.3 and \nùëè\n=\n0.6\nb=0.6, then \nùë†\n=\n(\n0.6\n‚àí\n0.3\n)\n/\n0.6\n=\n0.5\ns=(0.6‚àí0.3)/0.6=0.5 ‚Äî a reasonably good assignment.\n\nThe silhouette plot\n\nEach cluster is shown as a horizontal block.\n\nWithin each block, samples are ordered by silhouette value and displayed as horizontal bars (width = \nùë†\n(\nùëñ\n)\ns(i)).\n\nA dashed vertical line shows the average silhouette score across all samples.\n\nThe plot highlights:\n\nCluster compactness (long bars near +1),\n\nPresence of many low/negative values (bad clusters or misassignments),\n\nRelative sizes of clusters.\n-->\n\nHow to interpret (practical heuristics)\n\n- Mean silhouette ‚â≥ 0.5 ‚Üí strong structure (good clustering).\n\n- Mean silhouette ‚âà 0.25‚Äì0.5 ‚Üí weak to moderate structure; inspect clusters individually.\n\n- Mean silhouette ‚â≤ 0.25 ‚Üí little structure; clustering may be unreliable. (These are rules of thumb ‚Äî context and domain knowledge matter.)\n\n\n* Silhouette plots for hierarchical clustering\n\nTODO: simplify code below\n\n::: {#238f7b4c .cell execution_count=13}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.cluster.hierarchy import linkage, fcluster, dendrogram\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport matplotlib.pyplot as plt\n\n# X: array-like, shape (n_samples, n_features)\n# Preprocess first (e.g., PCA)\nfrom sklearn.decomposition import PCA\nX_pca = PCA(n_components=30, random_state=0).fit_transform(X)  # optional but recommended\n\n# 1) compute linkage (Ward example; uses Euclidean)\nZ = linkage(X_pca, method='ward')   # method can be 'ward', 'single', 'complete', 'average', etc.\n\n# 2) turn tree into flat labels for desired k\nk = 5\nlabels = fcluster(Z, t=k, criterion='maxclust')  # labels are 1..k\n\n# 3) silhouette computations\nsil_vals = silhouette_samples(X_pca, labels, metric='euclidean')\navg_sil = silhouette_score(X_pca, labels, metric='euclidean')\nprint(f\"Average silhouette (k={k}):\", avg_sil)\n\n# 4) (optional) plot dendrogram and silhouette side-by-side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), gridspec_kw={'width_ratios':[1,1]})\n# dendrogram (cut to show leaves; set no labels for many samples)\ndendrogram(Z, no_labels=True, ax=ax1, color_threshold=None)\nax1.set_title('Dendrogram')\n\n# silhouette plot (simple)\nfrom matplotlib import cm\nunique_labels = np.unique(labels)\ny_lower = 10\ncmap = cm.nipy_spectral\nax2.set_xlim([-0.1, 1])\nfor i, cl in enumerate(unique_labels):\n    cl_sil = np.sort(sil_vals[labels == cl])\n    size_cluster = cl_sil.shape[0]\n    y_upper = y_lower + size_cluster\n    color = cmap(float(i) / max(1, (len(unique_labels)-1)))\n    ax2.fill_betweenx(np.arange(y_lower, y_upper), 0, cl_sil, facecolor=color, edgecolor=color, alpha=0.7)\n    ax2.text(-0.05, y_lower + 0.5*size_cluster, str(cl))\n    y_lower = y_upper + 10\nax2.axvline(x=avg_sil, color=\"red\", linestyle=\"--\", label=f\"avg = {avg_sil:.3f}\")\nax2.set_title(f\"Silhouette (k={k})\")\nax2.set_xlabel(\"Silhouette coefficient\")\nax2.set_yticks([])\nax2.legend()\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAverage silhouette (k=5): 0.16730055175012304\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](clustering_files/figure-html/cell-14-output-2.png){width=1142 height=470}\n:::\n:::\n\n\n* Also compare to clusterings of other cancer cell lines\n\n* Does the cell line also show up in other datasets? (`external validation`)\n\n\n\n## K‚ÄëMeans Clustering\n\nK‚ÄëMeans is a **partitional** clustering algorithm that aims to partition the data into **K** disjoint clusters.\n\n### Algorithm Steps\n\n1. **Choose K**, the number of clusters.  \n2. **Initialization:** Randomly select K initial centroids (or use k‚Äëmeans++ for better seeding).  \n3. **Assignment Step:**  \n   ```pseudo\n   for each data point x_i:\n       assign x_i to cluster j whose centroid Œº_j is nearest (minimize ||x_i - Œº_j||¬≤)\n   ```\n4. **Update Step:**\n   ```pseudo\n   for each cluster j:\n       Œº_j = (1 / |C_j|) * sum_{x_i in C_j} x_i\n   ```\n5. **Convergence Check:**\n   - Stop when assignments no longer change, OR\n   - The change in centroids is below a threshold, OR\n   - A maximum number of iterations is reached.\n\n\n\n### Animation\n\n![Animation of k-means](images/animation_kmeans.gif)\n\n<!-- created using kmeans_movie.py and kmeans_animation.ipynb -->\n\n\n### Within‚ÄìCluster Variation\n\nIn **$K$**‚Äëmeans clustering, we partition our $n$ observations into $K$ disjoint clusters\n$\\{C_1, C_2, \\dots, C_K\\}$.  A \"good\" clustering is one for which the _within‚Äëcluster variation_ is minimized.\n\n\n### Elbow point\n\nWhen using **k‚Äëmeans clustering**, one of the key questions is: how many clusters (k) should I choose? The **elbow method** is a simple, visual way to pick a reasonable _k_ by looking at how the ‚Äúwithin‚Äëcluster‚Äù variation decreases as _k_ increases.\n\n---\n\n#### 1. The Within‚ÄëCluster Sum of Squares (WCSS)\n\nFor each choice of _k_, you run k‚Äëmeans and compute the **within‚Äëcluster sum of squares** (WCSS), also called _inertia_ or _distortion_. This is the sum of squared Euclidean distances between each point and the centroid of its cluster:\n\n![WCSS](images/wcss_equation.png)\n\n<!--\n\\[\n\\text{WCSS}(k) = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\lVert x - \\mu_i \\rVert^2\n\\]\n-->\n\n- $C_{i}$ is cluster _i_  \n- $mu_{i}$ is the centroid of cluster _i_  \n\nAs _k_ increases, WCSS will always decrease (or stay the same), because more centroids can only reduce distances.\n\n\n#### 2. Plotting WCSS versus _k_\n\n1. **Choose a range** for _k_ (e.g. 1 to 10).  \n2. **For each _k_**, fit k‚Äëmeans and record `WCSS(k)`.  \n3. **Plot** `WCSS(k)` on the _y_-axis against _k_ on the _x_-axis.  \n\nYou will get a curve that starts high at _k_ = 1 and steadily goes down as _k_ increases.\n\n---\n\n#### 3. Identifying the \"Elbow\"\n\n- At first, adding clusters dramatically reduces WCSS, because you are splitting large, heterogeneous clusters into more homogeneous groups.  \n- After some point, adding more clusters yields **diminishing returns**‚Äîeach new cluster only slightly reduces WCSS.  \n\nThe **elbow point** is the value of _k_ at which the decrease in WCSS \"bends\" most sharply: like an elbow in your arm. It balances model complexity (more clusters) against improved fit (lower WCSS).\n\n![An elbow point](images/elbow_point_clustering.png)\n\n\n### Exercise (k-means)\n\n* NOTE: The `c` parameter in `plt.scatter()` is used to specify the color of the scatter plot points. \n\n::: {#b8959747 .cell execution_count=14}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Load data\niris = load_iris()\nX = iris.data\n\n# Perform k means with k = 2\nkmeans = KMeans(n_clusters=2, random_state=2, n_init=20)\nkmeans.fit(X)\n\n# The cluster assignments of the observations are contained in kmeans.labels_\nkmeans.labels_\n\n# Plot the data, with each observation colored according to its cluster assignment.\nplt.figure()\nplt.scatter(X[:,0], X[:,1], c=kmeans.labels_)\nplt.title(\"K-means on Iris data\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](clustering_files/figure-html/cell-15-output-1.png){width=571 height=431}\n:::\n:::\n\n\n### Exercise (Evaluate k-means clusters using the within-cluster similarity)\n\n\n* Find the optimal value of the number of clusters $K$\n\n::: {#4b2e2c6a .cell execution_count=15}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Load data\niris = load_iris()\nX = iris.data\n\n# Plot WCSS (inertia) as a function of the number of clusters\nwcss = []\n\n# try a few different values of k\nk_range = range(1,11)\n\nfor k_var in k_range:\n\n   # fit kmeans\n   kmeans = KMeans(n_clusters=k_var, random_state=2)\n   kmeans.fit(X)\n\n   # append WCSS to a list\n   wcss.append(kmeans.inertia_)\n\n\n# plot\nplt.figure()\nplt.scatter( k_range , wcss )\nplt.xlabel('Number of clusters')\nplt.ylabel('Within-Cluster Sum of Squares (WCSS)')\nplt.title('Elbow Method For Optimal')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](clustering_files/figure-html/cell-16-output-1.png){width=593 height=449}\n:::\n:::\n\n\n## Choosing Between Methods\n\n### Hierarchical Clustering\n\n- No need to pre-specify number of clusters (can decide by cutting dendrogram).\n- Produces a full hierarchy of clusters.\n\n### K‚ÄëMeans\n\n- Requires pre-specifying $K$.\n\n\n---\n\n\n## Summary\n\n::: {.callout-tip}\n#### Key Points\n\n\n- Hierarchical clustering builds a tree-like structure (dendrogram) to group similar data points\n   - **Distance Metrics** How we measure similarity between points:\n   - Euclidean distance (straight-line distance)\n   - Manhattan distance (city-block distance)\n   - Cosine distance (for directional data)\n\n- **Linkage Methods** \n   - How we measure distance between clusters:\n   - **Single**: Uses closest pair of points (can create chains)\n   - **Complete**: Uses farthest pair of points (creates compact clusters)\n   - **Average**: Uses average of all pairwise distances (balanced approach)\n   - **Ward's**: Minimizes increase in variance (creates similar-sized clusters)\n\n- **Dendrogram** \n   - Visual representation showing how clusters merge:\n   - Height shows distance when clusters merged\n   - Cutting at different heights gives different numbers of clusters\n\n\n- **Key Code Patterns:**\n\n```python\nimport seaborn as sns\n\nsns.clustermap(X, method='average', metric='correlation', center=0)\n```\n\n```python\n# Basic hierarchical clustering\nfrom sklearn.cluster import AgglomerativeClustering\nagg = AgglomerativeClustering(n_clusters=3, linkage='ward')\nlabels = agg.fit_predict(X)\n\n# Create dendrogram\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nZ = linkage(X, method='ward')\ndendrogram(Z)\n```\n\n- **Important Takeaways**\n\n - **No \"Correct\" Answer** \n      - Unsupervised learning requires interpretation and domain knowledge\n      - **Multiple Methods** - Try different linkage methods and distance metrics\n      - **Evaluation is Key** - Use both internal (silhouette) and external (ARI) metrics\n\n\n:::\n<!-- end callout -->\n\n\n## Further Reading\n\n- [Introduction to Statistical Learning in Python (ISLP)](https://www.statlearning.com/)\n- [IPython notebook from ISLP book](https://github.com/intro-stat-learning/ISLP_labs/blob/stable/Ch12-unsup-lab.ipynb)\n- Scikit‚Äëlearn documentation:\n  - [AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)\n  - [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n\n- [Documentation of sns.clustermap](https://seaborn.pydata.org/archive/0.12/generated/seaborn.clustermap.html) \n\n",
    "supporting": [
      "clustering_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}