{
  "hash": "2ea811f3ec0da9daea98e663684c7896",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Conceptual and mathematical basis of PCA\n---\n\n::: {.callout-tip}\n#### Learning Objectives\n\n- Learn the concepts and mathematical basics behind PCA\n:::\n\n\n\n## Intuitive explanation of PCA\n\n[Explanation of PCA (by StatQuest)](https://www.youtube.com/watch?v=_UVHneBUBW0)\n\n\n\n## Differences between PCA and linear regression\n\nDoes the figure above look similar to linear regression? Is PCA the same as linear regression?\n\n::: {.callout-tip}\n**NOTE (IMPORTANT CONCEPT)**: PCA is _not_ linear regression. It looks similar though, does it not?\n\nLinear regression is a predictive model. PCA is _not_. You cannot use PCA to predict anything. You can use PCA to only pick out patterns in your data.\n:::\n<!-- end callout -->\n\n\n## ðŸ“Š Key Concepts\n\n\n### 1. **Scores and Loadings**\n\nWhat is being plotted on the axes (PC1 and PC2) are the `scores`.\n\nThe `scores` for each principal component are calculated as follows:\n\n$$\nPC_{1} = \\alpha X + \\beta Y + \\gamma Z + .... \n$$\n\nwhere $X$, $Y$ and $Z$ are the normalized *features*.\n\nThe constants $\\alpha$, $\\beta$, $\\gamma$ are determined by the PCA algorithm. They are called the `loadings`.\n\n### 2. **Linear combinations**\n\n\n::: {.callout-tip}\n**NOTE (IMPORTANT CONCEPT)**: The principal components are *linear combinations* of the original *features*. Hence they can be a bit difficult to interpret. \n:::\n<!-- end callout -->\n\n\n\n### 3. **Variance**\n\n* Variance = how spread out the data is.\n* PCA finds directions (principal components) that maximize variance.\n\n\n\n---\n\n## ðŸ”¬ Example: Gene Expression Data\n\n* Rows = samples (patients)\n* Columns = gene expression levels\n\n### Goal:\n\n* Reduce dimensionality from 20,000 genes to 2-3 PCs\n* Visualize patterns between patient groups (e.g., healthy vs. cancer)\n\n```python\n# Sample Python code (requires numpy, sklearn, matplotlib)\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\nX = ...  # gene expression matrix\nX_scaled = StandardScaler().fit_transform(X)\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nplt.scatter(X_pca[:, 0], X_pca[:, 1])\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA of Gene Expression')\nplt.show()\n```\n\n\n\n\n## Exercise (theoretical) {#sec-ex-theoretical}\n\n::::: {#ex-titletheor .callout-exercise}\n\n#### exercise_theoretical\n\n{{< level 2 >}}\n\nBreak up into groups and discuss the following problem:\n\n1. Shown are biological samples with scores\n\n2. The features are genes\n\n* Why are `Sample 33` and `Sample 24` separated from the rest? What can we say about `Gene1`, `Gene 2`, `Gene 3` and `Gene 4`?\n\n* Why is `Sample 2` separated from the rest? What can we say about `Gene1`, `Gene 2`, `Gene 3` and `Gene 4`?\n\n* Can we treat `Sample 2` as an outlier? Why or why not? Argue your case.\n\nThe PCA biplot is shown below:\n\n::: {#0b1b1467 .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](conceptual_basis_PCA_files/figure-html/cell-2-output-1.png){width=760 height=740}\n:::\n:::\n\n\nThe table of loadings is shown below:\n\n::: {#ce090630 .cell execution_count=2}\n\n::: {.cell-output .cell-output-stdout}\n```\n            PC1       PC2       PC3       PC4\nGene1 -0.535899  0.418181 -0.341233  0.649228\nGene2 -0.583184  0.187986 -0.268148 -0.743075\nGene3 -0.278191 -0.872806 -0.378016  0.133877\nGene4 -0.543432 -0.167319  0.817778  0.089024\n```\n:::\n:::\n\n\n:::::\n<!-- end callout -->\n\n\n\n\n\n## Summary\n\n::: {.callout-tip}\n#### Key Points\n\n- PCA is _not_ linear regression!\n- `scores` and `loadings`\n \n:::\n\n",
    "supporting": [
      "conceptual_basis_PCA_files"
    ],
    "filters": [],
    "includes": {}
  }
}