{
  "hash": "fa9776b73729e85cfb6a768369d928a6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Normalizing your data and PCA\"\nformat: html\n---\n\n# Introduction\n\nThis chapter demonstrates basic unsupervised machine learning concepts using Python.\n\n::: {.callout-tip}\n## Learning Objectives\n\n- Understand the difference between supervised and unsupervised learning.\n- Apply PCA and clustering to example data.\n- Visualize results.\n:::\n<!-- end callout -->\n\n\n## Normalization (Z-score Standardization)\n\nNormalization, specifically Z-score standardization, is a data scaling technique that transforms your data to have a mean of 0 and a standard deviation of 1. This is useful for many machine learning algorithms that are sensitive to the scale of input features.\n\nThe formula for Z-score is:\n\n$$ z = \\frac{x - \\mu}{\\sigma} $$\n\nWhere:\n- $x$ is the original data point.\n- $\\mu$ is the mean of the data.\n- $\\sigma$ is the standard deviation of the data.\n\nFor example, say you have two variables or *features* on very different scales. \n\n\n| Age | Weight (grams) |\n|-----|------------|\n| 25  | 65000      |\n| 30  | 70000      |\n| 35  | 75000      |\n| 40  | 80000      |\n| 45  | 85000      |\n| 50  | 90000      |\n| 55  | 95000      |\n| 60  | 100000     |\n| 65  | 105000     |\n| 70  | 110000     |\n| 75  | 115000     |\n| 80  | 120000     |\n\nIf these are not brought on similar scales, weight will have a dispproportionate influence on whatever machine learning model we build.\n\nHence we normalize each of the features *separately*, i.e. age is normalized relative to age and weight is normalized relative to weight.\n\n::: {#57ed65e8 .cell execution_count=1}\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal data:\nAge: mean=43.6, std=13.1\nWeight: mean=69.8, std=9.8\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-2-output-2.png){width=585 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-2-output-3.png){width=662 height=470}\n:::\n:::\n\n\n* In an ideal scenario a feature/variable such as `weight` might be transformed in the following way after normalization:\n\n::: {#858e245b .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-3-output-1.png){width=971 height=395}\n:::\n:::\n\n\n* And here is what it might look like for a feature such as `age`.\n\n::: {#1e64ef5e .cell execution_count=3}\n\n::: {.cell-output .cell-output-stdout}\n```\nZ-scored mean: -0.00, std: 1.00\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-4-output-2.png){width=971 height=395}\n:::\n:::\n\n\n::: {.callout-tip}\n**NOTE (IMPORTANT CONCEPT)**: \n\n* After normalization, the *normalized features* are on comparable scales. The features (such as `weight` and `age`) no longer have so much variation. They can be used as input to machine learning algorithms.\n\n* The rule of thumb is to (almost) always *normalize* your data before you use it in a machine learning algorithm. (There are a few exceptions and we will point this out in due course).\n\n:::\n<!-- end callout -->\n\n\n\n\n\n\n\n### Data visualization before doing PCA {#sec-datavizbeforePCA}\n\n::::: {#ex-titledaatviz .callout-exercise}\n\n#### exercise_data_visualization\n\n{{< level 1 >}}\n\nDiscuss in a group. What is wrong with the following plot?\n\n::: {#2f1eb047 .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-5-output-1.png){width=971 height=395}\n:::\n:::\n\n\n:::: {.callout-answer collapse=\"true\"}\n\n#### Looking at your data\n\nAlways look at your data before you try and machine learning technique on it. There is a 150 year old person in your data!\n\n\n\n::::\n\n:::::\n\n\n\n\n\n::: {.callout-tip}\n**NOTE (IMPORTANT CONCEPT)**: \n\n* Visualize your data before you do any normalization. If there is anything odd about your data, discuss this with the person who gave you the data or did the experiment. This could be an error in the machine that generated the data or a data entry error. If there is justification, you can remove the data point.\n\n* Then perform normalization and apply a machine learning technique.\n\n:::\n<!-- end callout -->\n\n\n\n\n## Setup\n\n::: {#d41ae00b .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n```\n:::\n\n\n## Example Data\n\n::: {#a0eac370 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-7-output-1.png){width=569 height=431}\n:::\n:::\n\n\n## PCA Example\n\n<!--open tab-->\n::: {.callout-note collapse=\"true\"}\n::: {.panel-tabset group=\"language\"}\n\n## Python\n\n::: {#a89f0810 .cell execution_count=7}\n``` {.python .cell-code}\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\nplt.scatter(X_pca[:, 0], X_pca[:, 1])\nplt.title(\"PCA Projection\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![A simple PCA plot](normalization_files/figure-html/cell-8-output-1.png){width=569 height=431}\n:::\n:::\n\n\n## R\n\n:::\n:::\n<!--close tab-->\n\n\n\n\n## Scree plot\n\nA *scree* plot is a simple graph that shows how much variance (information) each principal component explains in your data after running PCA. The x-axis shows the principal components (PC1, PC2, etc.), and the y-axis shows the proportion of variance explained by each one.\n\nYou can use a scree plot to decide how many principal components to keep: look for the point where the plot levels off (the *elbow*): this tells you that adding more components doesnâ€™t explain much more variance.\n\n::: {#54b677c9 .cell execution_count=8}\n``` {.python .cell-code}\n# Scree plot: variance explained by each component\nplt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')\nplt.title(\"Scree Plot\")\nplt.xlabel(\"Principal Component\")\nplt.ylabel(\"Variance Explained Ratio\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-9-output-1.png){width=589 height=449}\n:::\n:::\n\n\nA scree plot may have an *elbow* like the plot below.\n\n::: {#462a7a43 .cell fig.cap='An idealized scree plot' execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-10-output-1.png){width=663 height=449}\n:::\n:::\n\n\n<!--\n## Loadings\n\n::: {#0264e253 .cell execution_count=10}\n``` {.python .cell-code}\n# pca.components_.T\n#feature_names = [\"Feature 1\", \"Feature 2\"]  # Replace with your actual feature names if available\n#loadings = pd.DataFrame(pca.components_.T, columns=[\"PC1\", \"PC2\"], index=feature_names)\n#print(\"PCA Loadings:\")\n#print(loadings)\n```\n:::\n\n\n-->\n\n\n\n\n### Hands-on coding\n\n* Perform PCA on a dataset of US Arrests\n\nLoad data and install the `pca` Python package\n\n```python\n!pip install pca\n```\n\n\n\n::: {#2cac3448 .cell execution_count=12}\n``` {.python .cell-code}\nfrom pca import pca\nimport pandas as pd\n\n# Load the US Arrests data\n# Read the USArrests data directly from the GitHub raw URL\nurl = \"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/main/course_files/data/USArrests.csv\"\ndf = pd.read_csv(url, index_col=0)\n\nprint(\"US Arrests Data (first 5 rows):\")\nprint(df.head())\nprint(\"\\nData shape:\", df.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUS Arrests Data (first 5 rows):\n            Murder  Assault  UrbanPop  Rape\nState                                      \nAlabama       13.2      236        58  21.2\nAlaska        10.0      263        48  44.5\nArizona        8.1      294        80  31.0\nArkansas       8.8      190        50  19.5\nCalifornia     9.0      276        91  40.6\n\nData shape: (48, 4)\n```\n:::\n:::\n\n\nNormalize the data\n\n::: {#d4492ba3 .cell execution_count=13}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\n\nscaler_standard = StandardScaler()\ndf_scaled = scaler_standard.fit_transform(df)\n\nprint(\"\\nData shape after normalization:\", df_scaled.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nData shape after normalization: (48, 4)\n```\n:::\n:::\n\n\nPerform PCA\n\n# TODO: add labels\n\n::: {#bc85844c .cell execution_count=14}\n``` {.python .cell-code}\nmodel = pca(n_components=4)\nout = model.fit_transform(df_scaled)\nax = model.biplot(n_feat=len(df.columns), legend=False)\n```\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-15-output-1.png){width=2067 height=1271}\n:::\n:::\n\n\n* Variance explained plots\n\n::: {#29ededff .cell execution_count=15}\n``` {.python .cell-code}\nmodel.plot()\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n(<Figure size 1440x960 with 1 Axes>,\n <Axes: title={'center': 'Cumulative explained variance\\n 4 Principal Components explain [100.0%] of the variance.'}, xlabel='Principal Component', ylabel='Percentage explained variance'>)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-16-output-2.png){width=1212 height=875}\n:::\n:::\n\n\n* 3D PCA biplots\n\n::: {#6401e73c .cell execution_count=16}\n``` {.python .cell-code}\nmodel.biplot3d()\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n(<Figure size 3000x2500 with 1 Axes>,\n <Axes3D: title={'center': '4 Principal Components explain [100.0%] of the variance'}, xlabel='PC1 (61.6% expl.var)', ylabel='PC2 (24.7% expl.var)', zlabel='PC3 (9.14% expl.var)'>)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-17-output-2.png){width=1945 height=1972}\n:::\n:::\n\n\n* Loadings\n\n*Recall*\n\nWhat is being plotted on the axes (PC1 and PC2) are the `scores`.\n\nThe `scores` for each principal component are calculated as follows:\n\n$$\nPC_{1} = \\alpha X + \\beta Y + \\gamma Z + .... \n$$\n\nwhere $X$, $Y$ and $Z$ are the normalized *features*.\n\nThe constants $\\alpha$, $\\beta$, $\\gamma$ are determined by the PCA algorithm. They are called the `loadings`.\n\n::: {#7ae65601 .cell execution_count=17}\n``` {.python .cell-code}\nprint(model.results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'loadings':             1         2         3         4\nPC1  0.533785  0.583489  0.284213  0.542068\nPC2 -0.428765 -0.190485  0.865950  0.173225\nPC3 -0.331927 -0.267593 -0.386784  0.817690\nPC4 -0.648891  0.742732 -0.140542 -0.086823, 'PC':          PC1       PC2       PC3       PC4\n0   0.923886 -1.127792 -0.437720 -0.150321\n1   1.884005 -1.032585  2.032973  0.451071\n2   1.705462  0.730059  0.043498  0.833066\n3  -0.198714 -1.092074  0.111217  0.187022\n4   2.462479  1.513698  0.585558  0.340560\n5   1.453427  0.982671  1.080932 -0.000710\n6  -1.406810  1.081895 -0.661238  0.108387\n7  -0.003621  0.319738 -0.730442  0.876779\n8   2.947649 -0.070435 -0.569823  0.100756\n9   1.571384 -1.281416 -0.326932 -1.066904\n10 -0.966398  1.557165  0.034386 -0.910657\n11 -1.689257 -0.178154  0.241665  0.495788\n12  1.320695  0.653978 -0.681444  0.119479\n13 -0.561650  0.161720  0.218372 -0.425644\n14 -2.302281  0.133259  0.145716 -0.022891\n15 -0.850716  0.279295  0.013602 -0.209501\n16 -0.808869 -0.934920 -0.029023 -0.667159\n17  1.500981 -0.882536 -0.772483 -0.449015\n18 -2.444195 -0.340245 -0.083049  0.325837\n19  1.702710 -0.431039 -0.158134  0.562825\n20 -0.536401  1.454143 -0.626920  0.169570\n21  2.044350  0.144860  0.383014 -0.098068\n22 -1.742422  0.647555  0.133541 -0.073811\n23  0.932617 -2.374555 -0.724196 -0.204393\n24  0.637255  0.263934  0.369919 -0.224783\n25 -1.239466 -0.507562  0.236769 -0.123520\n26 -1.317489  0.212450  0.160150 -0.019531\n27  2.806905  0.760007  1.157898 -0.309200\n28 -2.431886  0.048021  0.018380  0.027526\n29  0.127587  1.417883 -0.775421 -0.251489\n30  1.917815 -0.148279  0.181459  0.343651\n31  1.623118  0.790157 -0.646164  0.011216\n32  1.064086 -2.207350 -0.854340  0.962604\n33 -3.038797 -0.548177  0.281399  0.250127\n34 -0.281823  0.736114 -0.041732 -0.477516\n35 -0.366423  0.292555 -0.026415 -0.012846\n36  0.003276  0.556212  0.921912  0.236698\n37 -0.941353  0.568486 -0.411608 -0.364358\n38 -0.909909  1.464948 -1.387731  0.600087\n39  1.257310 -1.914756 -0.290121  0.141580\n40 -2.038884 -0.778125  0.375435  0.109319\n41  0.935690 -0.851392  0.192734 -0.645743\n42  1.293269  0.387317 -0.490484 -0.642740\n43 -0.602262  1.466342  0.271830  0.074469\n44 -2.851337 -1.332665  0.825094  0.146559\n45 -0.153441 -0.190521  0.005751 -0.210783\n46 -0.270617  0.975724  0.604878  0.216519\n47 -2.160933 -1.375609  0.097337 -0.129911, 'explained_var': array([0.61629429, 0.86387677, 0.95532444, 1.        ]), 'variance_ratio': array([0.61629429, 0.24758248, 0.09144767, 0.04467556]), 'model': PCA(n_components=4), 'scaler': None, 'pcp': np.float64(1.0000000000000002), 'topfeat':     PC feature   loading  type\n0  PC1       2  0.583489  best\n1  PC2       3  0.865950  best\n2  PC3       4  0.817690  best\n3  PC4       2  0.742732  best\n4  PC4       1 -0.648891  weak, 'outliers':      y_proba     p_raw    y_score  y_bool  y_bool_spe  y_score_spe\n0   0.975525  0.664294   5.847636   False       False     1.457903\n1   0.708566  0.054815  15.230543   False       False     2.148419\n2   0.975525  0.407776   8.267604   False       False     1.855152\n3   0.998476  0.904339   3.432838   False       False     1.110005\n4   0.708566  0.071188  14.431529   False       False     2.890516\n5   0.975525  0.373658   8.639000   False       False     1.754449\n6   0.975525  0.457677   7.755841   False       False     1.774715\n7   0.998476  0.852925   4.046269   False       False     0.319759\n8   0.791047  0.115361  12.899320   False       False     2.948491\n9   0.975525  0.224129  10.620723   False       False     2.027628\n10  0.975525  0.383534   8.529405   False       False     1.832672\n11  0.975525  0.594216   6.474697   False       False     1.698626\n12  0.975525  0.614125   6.295892   False       False     1.473744\n13  0.998476  0.958706   2.563460   False       False     0.584469\n14  0.975525  0.413845   8.203539   False       False     2.306135\n15  0.998476  0.949615   2.739714   False       False     0.895390\n16  0.998476  0.729799   5.256893   False       False     1.236262\n17  0.975525  0.388804   8.471644   False       False     1.741210\n18  0.975525  0.278536   9.811090   False       False     2.467763\n19  0.975525  0.532463   7.038705   False       False     1.756421\n20  0.975525  0.596351   6.455470   False       False     1.549922\n21  0.975525  0.482918   7.508219   False       False     2.049476\n22  0.975525  0.571498   6.680189   False       False     1.858861\n23  0.895284  0.149214  12.044850   False       False     2.551134\n24  0.998476  0.944449   2.832091   False       False     0.689750\n25  0.998476  0.791489   4.676863   False       False     1.339364\n26  0.998476  0.855485   4.018116   False       False     1.334509\n27  0.708566  0.049146  15.558941   False       False     2.907976\n28  0.975525  0.381779   8.548750   False       False     2.432360\n29  0.975525  0.644948   6.020386   False       False     1.423612\n30  0.975525  0.538344   6.984148   False       False     1.923539\n31  0.975525  0.479709   7.539345   False       False     1.805231\n32  0.708566  0.088571  13.748143   False       False     2.450444\n33  0.708566  0.079105  14.103578   False       False     3.087845\n34  0.998476  0.932461   3.029997   False       False     0.788218\n35  0.998476  0.996014   1.259897   False       False     0.468886\n36  0.998476  0.893044   3.578107   False       False     0.556221\n37  0.998476  0.790790   4.683656   False       False     1.099691\n38  0.975525  0.218048  10.720437   False       False     1.724530\n39  0.975525  0.288707   9.673319   False       False     2.290659\n40  0.975525  0.350924   8.898577   False       False     2.182321\n41  0.975525  0.690997   5.608453   False       False     1.265063\n42  0.975525  0.621719   6.227916   False       False     1.350022\n43  0.975525  0.679982   5.707287   False       False     1.585206\n44  0.708566  0.038174  16.308267   False       False     3.147398\n45  0.998476  0.998476   0.962234   False       False     0.244627\n46  0.998476  0.829952   4.291088   False       False     1.012557\n47  0.975525  0.207258  10.902975   False       False     2.561626, 'outliers_params': {'paramT2': (np.float64(-4.625929269271485e-18), np.float64(0.9999999999999999)), 'paramSPE': (array([-9.25185854e-17, -1.38777878e-17]), array([[2.51762774e+00, 6.29946348e-17],\n       [6.29946348e-17, 1.01140077e+00]]))}}\n```\n:::\n:::\n\n\n## Exercise for normalization in PCA {#sec-pcanorm}\n\n::::: {#ex-title_pca .callout-exercise}\n\n#### exercise_pca_normalization\n\n{{< level 2 >}}\n\nWork in a group.\n\n* Try the same code above but now *without* normalisation.\n\n* What differences do you observe in PCA *with* and *without* normalization?\n\n\n:::::\n\n\n\n\n\n## Exercise (advanced)\n\nPlot prettier *publication ready* plots for PCA.\n\n::: {.callout-tip}\nLook into the documentation available here for the [PCA package](https://erdogant.github.io/pca/pages/html/Examples.html).\n:::\n<!-- end callout -->\n\n\n\n\n## Exercise (theoretical) {#sec-ex-theoretical}\n\n::::: {#ex-titletheor .callout-exercise}\n\n#### exercise_theoretical\n\n{{< level 2 >}}\n\nBreak up into groups and discuss the following problem:\n\n1. Shown are biological samples with scores\n\n2. The features are genes\n\n* Why are `Sample 33` and `Sample 24` separated from the rest? What can we say about `Gene1`, `Gene 2`, `Gene 3` and `Gene 4`?\n\n* Why is `Sample 2` separated from the rest? What can we say about `Gene1`, `Gene 2`, `Gene 3` and `Gene 4`?\n\n* Can we treat `Sample 2` as an outlier? Why or why not? Argue your case.\n\nThe PCA biplot is shown below:\n\n::: {#718259a6 .cell execution_count=18}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-19-output-1.png){width=737 height=702}\n:::\n:::\n\n\nThe table of loadings is shown below:\n\n::: {#e8fbbb82 .cell execution_count=19}\n\n::: {.cell-output .cell-output-stdout}\n```\n            PC1       PC2       PC3       PC4\nGene1 -0.535899  0.418181 -0.341233  0.649228\nGene2 -0.583184  0.187986 -0.268148 -0.743075\nGene3 -0.278191 -0.872806 -0.378016  0.133877\nGene4 -0.543432 -0.167319  0.817778  0.089024\n```\n:::\n:::\n\n\n:::::\n<!-- end callout -->\n\n\n\n\n\n## Clustering Example\n\nPCA is different to clustering where you are trying to find patterns in your data. We will encounter clustering later in the course.\n\n::: {#49889fa7 .cell fig.cap='A simple clustering' execution_count=20}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-21-output-1.png){width=600 height=452}\n:::\n:::\n\n\n## ðŸ§  PCA vs. Other Techniques\n\n* PCA is **unsupervised** (no labels used)\n* Works best for **linear** relationships\n* Alternatives:\n\n  * t-SNE for nonlinear structures\n\n---\n\n## ðŸ§¬ In Practice: Tips for Biologists\n\n* Always **standardize** data before PCA\n* Be cautious interpreting PCs biologicallyâ€”PCs are **mathematical constructs**\n\n\n\n### Goals of unsupervised learning\n\n* Finding patterns in data\n\nHere is an example from biological data (single-cell sequencing data) (the plot is from [2])[@Aschenbrenner2020].\n\n![Example tSNE ](https://gut.bmj.com/content/gutjnl/70/6/1023/F3.large.jpg)\n\n![Example heatmaps](https://gut.bmj.com/content/gutjnl/70/6/1023/F5.large.jpg)\n\n\n* Finding interesting patterns\n\nYou can also use dimensionality reduction techniques (such as PCA) to find interesting patterns in your data.\n\n::: {#6de3195b .cell execution_count=21}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-22-output-1.png){width=616 height=481}\n:::\n:::\n\n\n* Finding outliers\n\nYou can also use dimensionality reduction techniques (such as PCA) to find outliers in your data.\n\n::: {#3973a6c2 .cell execution_count=22}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-23-output-1.png){width=657 height=481}\n:::\n:::\n\n\n* Finding hypotheses\n\nAll of these can be used to generate hypotheses. These hypotheses can be tested by collecting more data.\n\n\n\n\n\n::: {.callout-tip}\n## Summary\n\n- Need to normalize data before doing dimensionality reduction\n- PCA reduces dimensionality for visualization.\n- Clustering algorithms finds clusters in unlabeled data.\n- The goal of unsupervised learning is to find patterns and form hypotheses.\n:::\n\n\n## Resources\n\n[1] [Article on normalization on Wikipedia](https://en.wikipedia.org/wiki/Standard_score)\n\n[2] Deconvolution of monocyte responses in inflammatory bowel disease reveals an IL-1 cytokine network that regulates IL-23 in genetic and acquired IL-10 resistance Gut, 2020 [link](https://gut.bmj.com/content/70/6/1023)\n\n[3] [ISLP book](https://www.statlearning.com/)\n\n[4] [Video lectures by the authors of the book Introduction to Statistical Learning in Python](https://www.youtube.com/playlist?list=PLoROMvodv4rNHU1-iPeDRH-J0cL-CrIda)\n\n[6] [Visual explanations of machine learning algorithms](https://mlu-explain.github.io)\n\n---\n\n",
    "supporting": [
      "normalization_files"
    ],
    "filters": [],
    "includes": {}
  }
}