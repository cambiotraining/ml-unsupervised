{
  "hash": "eed356c9dec1c80baba4fed58981e2b7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Normalizing your data\"\nformat: html\n---\n\n# Introduction\n\nThis chapter demonstrates basic unsupervised machine learning concepts using Python.\n\n::: {.callout-tip}\n## Learning Objectives\n\n- Refresher on Python\n- Understand the difference between supervised and unsupervised learning.\n- Apply PCA and clustering to example data.\n- Visualize results.\n:::\n\n\n## Refresher on Python\n\n<!--\nIf SSL error on Mac OS X,\nthen \n\ntry in the Terminal\n\nopen \"/Applications/Python 3.10/Install Certificates.command\"\n-->\n\n::: {#b46a8045 .cell execution_count=1}\n``` {.python .cell-code}\n# ============================================================================\n# 1. IMPORTING PACKAGES\n# ============================================================================\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# ============================================================================\n# 2. READING DATA WITH PANDAS FROM GITHUB\n# ============================================================================\n\n# GitHub URL for the diabetes data\n# Convert from GitHub web URL to raw data URL\ngithub_url = \"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/main/course_files/data/diabetes_sample_data.csv\"\n\n# Read CSV file directly from GitHub\ndiabetes_data = pd.read_csv(github_url)\n    \n# Display basic information about the data\nprint(\"\\nData shape:\", diabetes_data.shape)\nprint(\"\\nFirst 5 rows:\")\nprint(diabetes_data.head())\n        \nprint(\"\\nBasic statistics:\")\nprint(diabetes_data.describe())\n\n# ============================================================================\n# 3. PLOTTING WITH MATPLOTLIB\n# ============================================================================\n\n# Plot 1: Histogram of Age\nplt.figure(figsize=(10, 6))\nplt.hist(diabetes_data['age'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\nplt.title('Distribution of Age', fontsize=14, fontweight='bold')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.grid(True, alpha=0.3)\n#plt.savefig('age_distribution.png', dpi=300, bbox_inches='tight')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nData shape: (100, 6)\n\nFirst 5 rows:\n   patient_id   age  glucose   bmi  blood_pressure  diabetes\n0           1  62.5     97.5  29.8            71.7         0\n1           2  52.9    127.4  30.8            74.4         0\n2           3  64.7    129.7  33.4            87.5         0\n3           4  77.8    115.9  33.3            86.1         1\n4           5  51.5    135.2  21.1            79.8         1\n\nBasic statistics:\n       patient_id         age     glucose         bmi  blood_pressure  \\\ncount  100.000000  100.000000  100.000000  100.000000      100.000000   \nmean    50.500000   53.444000  140.670000   28.322000       81.066000   \nstd     29.011492   13.625024   28.611669    5.425223        8.842531   \nmin      1.000000   15.700000   82.400000   11.800000       58.800000   \n25%     25.750000   46.000000  115.800000   24.700000       74.350000   \n50%     50.500000   53.100000  142.550000   28.500000       80.500000   \n75%     75.250000   61.075000  156.175000   31.500000       86.825000   \nmax    100.000000   82.800000  221.600000   47.300000      101.900000   \n\n         diabetes  \ncount  100.000000  \nmean     0.250000  \nstd      0.435194  \nmin      0.000000  \n25%      0.000000  \n50%      0.000000  \n75%      0.250000  \nmax      1.000000  \n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-2-output-2.png){width=808 height=526}\n:::\n:::\n\n\n## Normalization (Z-score Standardization)\n\nNormalization, specifically Z-score standardization, is a data scaling technique that transforms your data to have a mean of 0 and a standard deviation of 1. This is useful for many machine learning algorithms that are sensitive to the scale of input features.\n\nThe formula for Z-score is:\n\n$$ z = \\frac{x - \\mu}{\\sigma} $$\n\nWhere:\n- $x$ is the original data point.\n- $\\mu$ is the mean of the data.\n- $\\sigma$ is the standard deviation of the data.\n\nFor example, say you have two variables or *features* on very different scales. \n\n\n| Age | Weight (grams) |\n|-----|------------|\n| 25  | 65000      |\n| 30  | 70000      |\n| 35  | 75000      |\n| 40  | 80000      |\n| 45  | 85000      |\n| 50  | 90000      |\n| 55  | 95000      |\n| 60  | 100000     |\n| 65  | 105000     |\n| 70  | 110000     |\n| 75  | 115000     |\n| 80  | 120000     |\n\nIf these are not brought on similar scales, weight will have a dispproportionate influence on whatever machine learning model we build.\n\nHence we normalize each of the features *separately*, i.e. age is normalized relative to age and weight is normalized relative to weight.\n\n::: {#20d8ddca .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# 1. Generate age and weight data\nnp.random.seed(42)\nage = np.random.normal(45, 15, 100)  # 100 people, mean age 45, std 15\nage = np.clip(age, 18, 80)  # Keep ages between 18-80\n\nweight = 70 + (age - 45) * 0.3 + np.random.normal(0, 10, 100)  # Weight correlated with age\nweight = np.clip(weight, 45, 120)  # Keep weights between 45-120 kg\n\nprint(\"Original data:\")\nprint(f\"Age: mean={age.mean():.1f}, std={age.std():.1f}\")\nprint(f\"Weight: mean={weight.mean():.1f}, std={weight.std():.1f}\")\n\n# 2. Normalize the data\nscaler = StandardScaler()\ndata = np.column_stack((age, weight))\nnormalized_data = scaler.fit_transform(data)\n\nage_normalized = normalized_data[:, 0]\nweight_normalized = normalized_data[:, 1]\n\n# Histogram: Age (Original)\nplt.figure()\nplt.hist(age, bins=20, alpha=0.7)\nplt.title('Age Distribution (Original)')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Histogram: Age (Normalized)\nplt.figure()\nplt.hist(age_normalized, bins=20, alpha=0.7)\nplt.title('Age Distribution (Normalized)')\nplt.xlabel('Age (Z-score)')\nplt.ylabel('Frequency')\nplt.grid(True, alpha=0.7)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal data:\nAge: mean=43.6, std=13.1\nWeight: mean=69.8, std=9.8\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-3-output-2.png){width=585 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-3-output-3.png){width=662 height=470}\n:::\n:::\n\n\n## Setup\n\n::: {#c89930ab .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n```\n:::\n\n\n## Example Data\n\n::: {#21728397 .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-5-output-1.png){width=569 height=431}\n:::\n:::\n\n\n## PCA Example\n\n<!--open tab-->\n::: {.callout-note collapse=\"true\"}\n::: {.panel-tabset group=\"language\"}\n\n## Python\n\n::: {#fcc1733c .cell execution_count=5}\n``` {.python .cell-code}\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\nplt.scatter(X_pca[:, 0], X_pca[:, 1])\nplt.title(\"PCA Projection\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![A simple PCA plot](normalization_files/figure-html/cell-6-output-1.png){width=569 height=431}\n:::\n:::\n\n\n## R\n\n:::\n:::\n<!--close tab-->\n\n\n\n## Clustering Example\n\n::: {#fd5676ce .cell fig.cap='A simple clustering' execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-7-output-1.png){width=569 height=431}\n:::\n:::\n\n\n## ðŸ§  PCA vs. Other Techniques\n\n* PCA is **unsupervised** (no labels used)\n* Works best for **linear** relationships\n* Alternatives:\n\n  * t-SNE for nonlinear structures\n\n---\n\n## ðŸ§¬ In Practice: Tips for Biologists\n\n* Always **standardize** data before PCA\n* Be cautious interpreting PCs biologicallyâ€”PCs are **mathematical constructs**\n\n\n## Exercise\n\n* PCA on US Arrest data\n\n### Goals of unsupervised learning\n\n* Finding patterns in data\n\nHere is an example from biological data (single-cell sequencing data) (the plot is from [2])[@Aschenbrenner2020].\n\n![Example tSNE ](https://gut.bmj.com/content/gutjnl/70/6/1023/F3.large.jpg)\n\n![Example heatmaps](https://gut.bmj.com/content/gutjnl/70/6/1023/F5.large.jpg)\n\n\n* Finding interesting patterns\n\nYou can also use dimensionality reduction techniques (such as PCA) to find interesting patterns in your data.\n\n::: {#8148cfd7 .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-8-output-1.png){width=600 height=449}\n:::\n:::\n\n\n* Finding outliers\n\nYou can also use dimensionality reduction techniques (such as PCA) to find outliers in your data.\n\n::: {#331bda27 .cell execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-9-output-1.png){width=587 height=449}\n:::\n:::\n\n\n* Finding hypotheses\n\nAll of these can be used to generate hypotheses. These hypotheses can be tested by collecting more data.\n\n\n\n::: {.callout-tip}\n## Summary\n\n- Need to normalize data before doing dimensionality reduction\n- PCA reduces dimensionality for visualization.\n- KMeans finds clusters in unlabeled data.\n:::\n\n\n## References\n\n[1] [Article on normalization on Wikipedia](https://en.wikipedia.org/wiki/Standard_score)\n\n[2] Deconvolution of monocyte responses in inflammatory bowel disease reveals an IL-1 cytokine network that regulates IL-23 in genetic and acquired IL-10 resistance Gut, 2020 [link](https://gut.bmj.com/content/70/6/1023)\n\n---\n\n",
    "supporting": [
      "normalization_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}