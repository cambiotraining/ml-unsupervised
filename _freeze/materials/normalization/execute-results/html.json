{
  "hash": "9f42f724b331d6e446174688edf1b1ef",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Normalizing your data and PCA\"\nformat: html\n---\n\n# Introduction\n\nThis chapter demonstrates basic unsupervised machine learning concepts using Python.\n\n::: {.callout-tip}\n## Learning Objectives\n\n- Refresher on Python\n- Understand the difference between supervised and unsupervised learning.\n- Apply PCA and clustering to example data.\n- Visualize results.\n:::\n\n\n## Refresher on Python\n\n<!--\nIf SSL error on Mac OS X,\nthen \n\ntry in the Terminal\n\nopen \"/Applications/Python 3.10/Install Certificates.command\"\n-->\n\n::: {#b106b1d0 .cell execution_count=1}\n``` {.python .cell-code}\n# ============================================================================\n# 1. IMPORTING PACKAGES\n# ============================================================================\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# ============================================================================\n# 2. READING DATA WITH PANDAS FROM GITHUB\n# ============================================================================\n\n# GitHub URL for the diabetes data\n# Convert from GitHub web URL to raw data URL\ngithub_url = \"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/main/course_files/data/diabetes_sample_data.csv\"\n\n# Read CSV file directly from GitHub\ndiabetes_data = pd.read_csv(github_url)\n    \n# Display basic information about the data\nprint(\"\\nData shape:\", diabetes_data.shape)\nprint(\"\\nFirst 5 rows:\")\nprint(diabetes_data.head())\n        \nprint(\"\\nBasic statistics:\")\nprint(diabetes_data.describe())\n\n# ============================================================================\n# 3. PLOTTING WITH MATPLOTLIB\n# ============================================================================\n\n# Plot 1: Histogram of Age\nplt.figure(figsize=(10, 6))\nplt.hist(diabetes_data['age'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\nplt.title('Distribution of Age', fontsize=14, fontweight='bold')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.grid(True, alpha=0.3)\n#plt.savefig('age_distribution.png', dpi=300, bbox_inches='tight')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nData shape: (100, 6)\n\nFirst 5 rows:\n   patient_id   age  glucose   bmi  blood_pressure  diabetes\n0           1  62.5     97.5  29.8            71.7         0\n1           2  52.9    127.4  30.8            74.4         0\n2           3  64.7    129.7  33.4            87.5         0\n3           4  77.8    115.9  33.3            86.1         1\n4           5  51.5    135.2  21.1            79.8         1\n\nBasic statistics:\n       patient_id         age     glucose         bmi  blood_pressure  \\\ncount  100.000000  100.000000  100.000000  100.000000      100.000000   \nmean    50.500000   53.444000  140.670000   28.322000       81.066000   \nstd     29.011492   13.625024   28.611669    5.425223        8.842531   \nmin      1.000000   15.700000   82.400000   11.800000       58.800000   \n25%     25.750000   46.000000  115.800000   24.700000       74.350000   \n50%     50.500000   53.100000  142.550000   28.500000       80.500000   \n75%     75.250000   61.075000  156.175000   31.500000       86.825000   \nmax    100.000000   82.800000  221.600000   47.300000      101.900000   \n\n         diabetes  \ncount  100.000000  \nmean     0.250000  \nstd      0.435194  \nmin      0.000000  \n25%      0.000000  \n50%      0.000000  \n75%      0.250000  \nmax      1.000000  \n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-2-output-2.png){width=808 height=526}\n:::\n:::\n\n\n## Normalization (Z-score Standardization)\n\nNormalization, specifically Z-score standardization, is a data scaling technique that transforms your data to have a mean of 0 and a standard deviation of 1. This is useful for many machine learning algorithms that are sensitive to the scale of input features.\n\nThe formula for Z-score is:\n\n$$ z = \\frac{x - \\mu}{\\sigma} $$\n\nWhere:\n- $x$ is the original data point.\n- $\\mu$ is the mean of the data.\n- $\\sigma$ is the standard deviation of the data.\n\nFor example, say you have two variables or *features* on very different scales. \n\n\n| Age | Weight (grams) |\n|-----|------------|\n| 25  | 65000      |\n| 30  | 70000      |\n| 35  | 75000      |\n| 40  | 80000      |\n| 45  | 85000      |\n| 50  | 90000      |\n| 55  | 95000      |\n| 60  | 100000     |\n| 65  | 105000     |\n| 70  | 110000     |\n| 75  | 115000     |\n| 80  | 120000     |\n\nIf these are not brought on similar scales, weight will have a dispproportionate influence on whatever machine learning model we build.\n\nHence we normalize each of the features *separately*, i.e. age is normalized relative to age and weight is normalized relative to weight.\n\n::: {#f4a69fdd .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# 1. Generate age and weight data\nnp.random.seed(42)\nage = np.random.normal(45, 15, 100)  # 100 people, mean age 45, std 15\nage = np.clip(age, 18, 80)  # Keep ages between 18-80\n\nweight = 70 + (age - 45) * 0.3 + np.random.normal(0, 10, 100)  # Weight correlated with age\nweight = np.clip(weight, 45, 120)  # Keep weights between 45-120 kg\n\nprint(\"Original data:\")\nprint(f\"Age: mean={age.mean():.1f}, std={age.std():.1f}\")\nprint(f\"Weight: mean={weight.mean():.1f}, std={weight.std():.1f}\")\n\n# 2. Normalize the data\nscaler = StandardScaler()\ndata = np.column_stack((age, weight))\nnormalized_data = scaler.fit_transform(data)\n\nage_normalized = normalized_data[:, 0]\nweight_normalized = normalized_data[:, 1]\n\n# Histogram: Age (Original)\nplt.figure()\nplt.hist(age, bins=20, alpha=0.7)\nplt.title('Age Distribution (Original)')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Histogram: Age (Normalized)\nplt.figure()\nplt.hist(age_normalized, bins=20, alpha=0.7)\nplt.title('Age Distribution (Normalized)')\nplt.xlabel('Age (Z-score)')\nplt.ylabel('Frequency')\nplt.grid(True, alpha=0.7)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal data:\nAge: mean=43.6, std=13.1\nWeight: mean=69.8, std=9.8\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-3-output-2.png){width=585 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-3-output-3.png){width=662 height=470}\n:::\n:::\n\n\n## Setup\n\n::: {#1382fa60 .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n```\n:::\n\n\n## Example Data\n\n::: {#3ffcbb03 .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-5-output-1.png){width=569 height=431}\n:::\n:::\n\n\n## PCA Example\n\n<!--open tab-->\n::: {.callout-note collapse=\"true\"}\n::: {.panel-tabset group=\"language\"}\n\n## Python\n\n::: {#fbd55f98 .cell execution_count=5}\n``` {.python .cell-code}\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\nplt.scatter(X_pca[:, 0], X_pca[:, 1])\nplt.title(\"PCA Projection\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![A simple PCA plot](normalization_files/figure-html/cell-6-output-1.png){width=569 height=431}\n:::\n:::\n\n\n## R\n\n:::\n:::\n<!--close tab-->\n\n## Scree plot\n\nA scree plot is a simple graph that shows how much variance (information) each principal component explains in your data after running PCA. The x-axis shows the principal components (PC1, PC2, etc.), and the y-axis shows the proportion of variance explained by each one.\n\nYou can use a scree plot to decide how many principal components to keep: look for the point where the plot levels off (the *elbow*): this tells you that adding more components doesnâ€™t explain much more variance.\n\n::: {#db9f45de .cell execution_count=6}\n``` {.python .cell-code}\n# Scree plot: variance explained by each component\nplt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')\nplt.title(\"Scree Plot\")\nplt.xlabel(\"Principal Component\")\nplt.ylabel(\"Variance Explained Ratio\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-7-output-1.png){width=589 height=449}\n:::\n:::\n\n\nA scree plot may have an *elbow* like the plot below.\n\n::: {#825f525d .cell fig.cap='An idealized scree plot' execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-8-output-1.png){width=663 height=449}\n:::\n:::\n\n\n<!--\n## Loadings\n\n::: {#d0274c2a .cell execution_count=8}\n``` {.python .cell-code}\n# pca.components_.T\n#feature_names = [\"Feature 1\", \"Feature 2\"]  # Replace with your actual feature names if available\n#loadings = pd.DataFrame(pca.components_.T, columns=[\"PC1\", \"PC2\"], index=feature_names)\n#print(\"PCA Loadings:\")\n#print(loadings)\n```\n:::\n\n\n-->\n\n## Clustering Example\n\nPCA is different to clustering where you are trying to find patterns in your data. We will encounter clustering later in the course.\n\n::: {#4f8ad31d .cell fig.cap='A simple clustering' execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-10-output-1.png){width=577 height=431}\n:::\n:::\n\n\n## ðŸ§  PCA vs. Other Techniques\n\n* PCA is **unsupervised** (no labels used)\n* Works best for **linear** relationships\n* Alternatives:\n\n  * t-SNE for nonlinear structures\n\n---\n\n## ðŸ§¬ In Practice: Tips for Biologists\n\n* Always **standardize** data before PCA\n* Be cautious interpreting PCs biologicallyâ€”PCs are **mathematical constructs**\n\n\n\n### Goals of unsupervised learning\n\n* Finding patterns in data\n\nHere is an example from biological data (single-cell sequencing data) (the plot is from [2])[@Aschenbrenner2020].\n\n![Example tSNE ](https://gut.bmj.com/content/gutjnl/70/6/1023/F3.large.jpg)\n\n![Example heatmaps](https://gut.bmj.com/content/gutjnl/70/6/1023/F5.large.jpg)\n\n\n* Finding interesting patterns\n\nYou can also use dimensionality reduction techniques (such as PCA) to find interesting patterns in your data.\n\n::: {#50f74f00 .cell execution_count=10}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-11-output-1.png){width=600 height=449}\n:::\n:::\n\n\n* Finding outliers\n\nYou can also use dimensionality reduction techniques (such as PCA) to find outliers in your data.\n\n::: {#814d929e .cell execution_count=11}\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-12-output-1.png){width=587 height=449}\n:::\n:::\n\n\n* Finding hypotheses\n\nAll of these can be used to generate hypotheses. These hypotheses can be tested by collecting more data.\n\n### Exercise\n\n* Perform PCA on a dataset of US Arrests\n\nLoad data\n\n```python\n!pip install pca\n```\n\n\n\n::: {#d5423908 .cell execution_count=13}\n``` {.python .cell-code}\nfrom pca import pca\nimport pandas as pd\n\n# Load the US Arrests data\n# Read the USArrests data directly from the GitHub raw URL\nurl = \"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/main/course_files/data/USArrests.csv\"\ndf = pd.read_csv(url, index_col=0)\n\nprint(\"US Arrests Data (first 5 rows):\")\nprint(df.head())\nprint(\"\\nData shape:\", df.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUS Arrests Data (first 5 rows):\n            Murder  Assault  UrbanPop  Rape\nState                                      \nAlabama       13.2      236        58  21.2\nAlaska        10.0      263        48  44.5\nArizona        8.1      294        80  31.0\nArkansas       8.8      190        50  19.5\nCalifornia     9.0      276        91  40.6\n\nData shape: (48, 4)\n```\n:::\n:::\n\n\nPerform PCA\n\n::: {#06a9f3f4 .cell execution_count=14}\n``` {.python .cell-code}\nmodel = pca(n_components=4)\nout = model.fit_transform(df)\nax = model.biplot(n_feat=len(df.columns), legend=False)\n\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n[04-08-2025 17:44:00] [pca.pca] [INFO] Extracting column labels from dataframe.\n[04-08-2025 17:44:00] [pca.pca] [INFO] Extracting row labels from dataframe.\n[04-08-2025 17:44:00] [pca.pca] [INFO] The PCA reduction is performed on the 4 columns of the input dataframe.\n[04-08-2025 17:44:00] [pca.pca] [INFO] Fit using PCA.\n[04-08-2025 17:44:00] [pca.pca] [INFO] Compute loadings and PCs.\n[04-08-2025 17:44:00] [pca.pca] [INFO] Compute explained variance.\n[04-08-2025 17:44:00] [pca.pca] [INFO] Outlier detection using Hotelling T2 test with alpha=[0.05] and n_components=[4]\n[04-08-2025 17:44:00] [pca.pca] [INFO] Multiple test correction applied for Hotelling T2 test: [fdr_bh]\n[04-08-2025 17:44:00] [pca.pca] [INFO] Outlier detection using SPE/DmodX with n_std=[3]\n[04-08-2025 17:44:00] [pca.pca] [INFO] Plot PC1 vs PC2 with loadings.\n[04-08-2025 17:44:00] [scatterd.scatterd] [INFO] Create scatterplot\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-15-output-2.png){width=2058 height=1271}\n:::\n:::\n\n\n* Variance explained plots\n\n::: {#44763832 .cell execution_count=15}\n``` {.python .cell-code}\nmodel.plot()\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n(<Figure size 1440x960 with 1 Axes>,\n <Axes: title={'center': 'Cumulative explained variance\\n 4 Principal Components explain [100.0%] of the variance.'}, xlabel='Principal Component', ylabel='Percentage explained variance'>)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-16-output-2.png){width=1212 height=875}\n:::\n:::\n\n\n* 3D PCA biplots\n\n::: {#625dbe7e .cell execution_count=16}\n``` {.python .cell-code}\nmodel.biplot3d()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n[04-08-2025 17:44:00] [pca.pca] [INFO] Plot PC1 vs PC2 vs PC3 with loadings.\n[04-08-2025 17:44:00] [scatterd.scatterd] [INFO] Create scatterplot\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n(<Figure size 3000x2500 with 1 Axes>,\n <Axes3D: title={'center': '4 Principal Components explain [100.0%] of the variance'}, xlabel='PC1 (96.4% expl.var)', ylabel='PC2 (2.88% expl.var)', zlabel='PC3 (0.59% expl.var)'>)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](normalization_files/figure-html/cell-17-output-3.png){width=1945 height=1972}\n:::\n:::\n\n\n* Loadings\n\n*Recall*\n\nWhat is being plotted on the axes (PC1 and PC2) are the `scores`.\n\nThe `scores` for each principal component are calculated as follows:\n\n$$\nPC_{1} = \\alpha X + \\beta Y + \\gamma Z + .... \n$$\n\nwhere $X$, $Y$ and $Z$ are the normalized *features*.\n\nThe constants $\\alpha$, $\\beta$, $\\gamma$ are determined by the PCA algorithm. They are called the `loadings`.\n\n::: {#6c3309ce .cell execution_count=17}\n``` {.python .cell-code}\nprint(model.results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'loadings':        Murder   Assault  UrbanPop      Rape\nPC1  0.041584  0.995187  0.048369  0.074397\nPC2 -0.045411 -0.060516  0.976922  0.199748\nPC3  0.079078 -0.066443 -0.199673  0.974404\nPC4  0.994965 -0.039074  0.058436 -0.071436, 'PC':                        PC1        PC2        PC3       PC4\nAlabama          62.104695 -11.569854  -2.571036  2.390729\nAlaska           90.091422 -18.173579  20.082295 -4.096991\nArizona         121.406646   8.601610  -1.671659 -4.364402\nArkansas         15.629716 -16.741244   0.078286 -0.535736\nCalifornia      104.776982  22.313750   6.753359 -2.808592\nColorado         32.307655  13.641366  12.194635 -1.713629\nConnecticut     -63.532892  13.048830  -8.617371 -0.704265\nDelaware         64.066928   1.238885 -11.338389 -3.746809\nFlorida         162.579815   5.968716  -2.941584  1.232499\nGeorgia          37.838646  -7.374991   3.505073  7.334706\nHawaii         -126.174440  24.510283   3.462165  3.486970\nIdaho           -54.491992  -9.374532  -1.724029 -3.356955\nIllinois         76.343221  12.752929  -5.919698  0.357727\nIndiana         -60.229054   2.944616   3.534375  1.650429\nIowa           -118.271210  -3.131830  -0.928109 -0.871706\nKansas          -58.463403   3.255756   0.183712  0.651066\nKentucky        -65.084303 -10.565646   2.013888  3.870227\nLouisiana        75.594954  -4.441346  -3.883800  4.467733\nMaine           -91.955934 -11.321876  -4.942350 -2.126798\nMaryland        126.643967  -5.245981  -2.339614 -1.946027\nMassachusetts   -23.901058  19.492807  -7.652137 -1.037690\nMichigan         82.775518   5.737532   6.429014  0.495857\nMinnesota      -101.624283   5.388593  -0.240855 -0.730665\nMississippi      84.132386 -27.589292  -5.069534  3.852208\nMissouri          5.310427   5.252112   5.375275  0.679364\nMontana         -65.182355  -9.400729   1.619068  0.240149\nNebraska        -71.776593  -0.087644   0.250121 -0.658994\nNevada           80.943626  14.930242  15.859544  0.342969\nNew Hampshire  -117.462465  -4.524273  -2.556713 -0.940127\nNew Jersey      -13.444972  23.158469  -6.442013  1.611612\nNew Mexico      112.185340  -0.553098   2.255854 -1.392284\nNew York         81.649603  15.768795  -4.749328  0.884121\nNorth Carolina  161.602001 -31.391611 -11.671292 -2.150116\nNorth Dakota   -130.202864 -15.901552  -1.609817 -2.308756\nOhio            -52.745141  12.365579   1.470217  2.032185\nOklahoma        -22.366204   3.403263  -0.611321 -0.184635\nOregon          -13.831882   3.877062   7.984332 -2.911465\nPennsylvania    -67.348024   9.029093  -3.413268  1.873292\nRhode Island      0.438586  18.381176 -17.586862 -2.321153\nSouth Carolina  104.560644 -23.736093  -2.069733  1.227265\nSouth Dakota    -88.817911 -16.443418   1.062809 -1.260376\nTennessee        14.808170  -6.549591   5.972649  3.917549\nTexas            28.636397  12.922119  -0.487939  4.239257\nUtah            -52.562195  17.735996   1.609241 -1.862148\nVermont        -127.449367 -27.090724   4.497811 -2.012857\nVirginia        -17.501029  -1.730386   0.887159  1.168243\nWashington      -27.742336  10.007474   4.624674 -2.687825\nWest Virginia   -94.265438 -22.787766  -0.667106  0.724843, 'explained_var': array([0.9643326 , 0.99313748, 0.99911593, 1.        ]), 'variance_ratio': array([9.64332599e-01, 2.88048813e-02, 5.97845474e-03, 8.84065325e-04]), 'model': PCA(n_components=4), 'scaler': None, 'pcp': np.float64(1.0000000000000002), 'topfeat':     PC   feature   loading  type\n0  PC1   Assault  0.995187  best\n1  PC2  UrbanPop  0.976922  best\n2  PC3      Rape  0.974404  best\n3  PC4    Murder  0.994965  best, 'outliers':                  y_proba     p_raw    y_score  y_bool  y_bool_spe  y_score_spe\nAlabama         0.999620  0.799234   4.601111   False       False    63.173211\nAlaska          0.999620  0.365520   8.730713   False       False    91.906165\nArizona         0.999620  0.167967  11.640753   False       False   121.710975\nArkansas        0.999620  0.993595   1.444244   False       False    22.903216\nCalifornia      0.999620  0.257562  10.107498   False       False   107.126651\nColorado        0.999620  0.946624   2.793800   False       False    35.069523\nConnecticut     0.999620  0.758702   4.989460   False       False    64.859081\nDelaware        0.999620  0.781579   4.772620   False       False    64.078905\nFlorida         0.420843  0.017535  18.538069   False       False   162.689341\nGeorgia         0.999620  0.950428   2.724717   False       False    38.550663\nHawaii          0.999620  0.104870  13.208618   False       False   128.533043\nIdaho           0.999620  0.871111   3.841755   False       False    55.292486\nIllinois        0.999620  0.635299   6.106537   False       False    77.401064\nIndiana         0.999620  0.850834   4.069115   False       False    60.300993\nIowa            0.999620  0.214721  10.775933   False       False   118.312668\nKansas          0.999620  0.878457   3.755791   False       False    58.553988\nKentucky        0.999620  0.771674   4.867169   False       False    65.936328\nLouisiana       0.999620  0.676410   5.739275   False       False    75.725310\nMaine           0.999620  0.451018   7.822307   False       False    92.650303\nMaryland        0.999620  0.141316  12.228021   False       False   126.752572\nMassachusetts   0.999620  0.968244   2.352761   False       False    30.842018\nMichigan        0.999620  0.589923   6.513389   False       False    82.974126\nMinnesota       0.999620  0.383510   8.529668   False       False   101.767047\nMississippi     0.999620  0.448614   7.846422   False       False    88.540541\nMissouri        0.999620  0.999620   0.659862   False       False     7.468957\nMontana         0.999620  0.792670   4.665365   False       False    65.856762\nNebraska        0.999620  0.769963   4.883398   False       False    71.776646\nNevada          0.999620  0.519438   7.160325   False       False    82.309068\nNew Hampshire   0.999620  0.214470  10.780144   False       False   117.549563\nNew Jersey      0.999620  0.981723   1.976515   False       False    26.778386\nNew Mexico      0.999620  0.274248   9.870286   False       False   112.186703\nNew York        0.999620  0.559034   6.793809   False       False    83.158359\nNorth Carolina  0.420843  0.009995  20.091620   False       False   164.622720\nNorth Dakota    0.999620  0.103125  13.262693   False       False   131.170291\nOhio            0.999620  0.877098   3.771862   False       False    54.175248\nOklahoma        0.999620  0.996651   1.199054   False       False    22.623645\nOregon          0.999620  0.996784   1.185440   False       False    14.364977\nPennsylvania    0.999620  0.758811   4.988439   False       False    67.950576\nRhode Island    0.999620  0.988643   1.710246   False       False    18.386407\nSouth Carolina  0.999620  0.271867   9.903461   False       False   107.220942\nSouth Dakota    0.999620  0.482905   7.508345   False       False    90.327224\nTennessee       0.999620  0.995571   1.298396   False       False    16.191944\nTexas           0.999620  0.976601   2.134707   False       False    31.416944\nUtah            0.999620  0.855304   4.020116   False       False    55.473866\nVermont         0.999620  0.094545  13.540901   False       False   130.296771\nVirginia        0.999620  0.998694   0.922137   False       False    17.586366\nWashington      0.999620  0.979528   2.047287   False       False    29.492147\nWest Virginia   0.999620  0.389782   8.460986   False       False    96.980694, 'outliers_params': {'paramT2': (np.float64(5.0330110449673766e-15), np.float64(1777.6058289930552)), 'paramSPE': (array([1.33226763e-14, 5.62512999e-15]), array([[ 7.00270263e+03, -8.77755141e-13],\n       [-8.77755141e-13,  2.09172664e+02]]))}}\n```\n:::\n:::\n\n\n## Exercise (advanced)\n\n::: {.callout-tip}\nLook into the documentation available here for the [PCA package](::: {.callout-tip}) and plot prettier *publication ready* plots.\n:::\n\n\n::: {.callout-tip}\n## Summary\n\n- Need to normalize data before doing dimensionality reduction\n- PCA reduces dimensionality for visualization.\n- KMeans finds clusters in unlabeled data.\n:::\n\n\n## References\n\n[1] [Article on normalization on Wikipedia](https://en.wikipedia.org/wiki/Standard_score)\n\n[2] Deconvolution of monocyte responses in inflammatory bowel disease reveals an IL-1 cytokine network that regulates IL-23 in genetic and acquired IL-10 resistance Gut, 2020 [link](https://gut.bmj.com/content/70/6/1023)\n\n---\n\n",
    "supporting": [
      "normalization_files"
    ],
    "filters": [],
    "includes": {}
  }
}