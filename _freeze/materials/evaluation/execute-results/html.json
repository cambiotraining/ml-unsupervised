{
  "hash": "b94cb3dd712bdd523f5688b0657858ac",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Evaluation for unsupervised machine learning\n---\n\n::: {.callout-tip}\n#### Learning Objectives\n\n- How to evaluate unsupervised machine learning techniques\n- Know the difference between **internal**, **external**, and **biological** evaluation.\n- Be able to compute and interpret common metrics (silhouette, ARI).\n- Learn practical diagnostics: silhouette plot .\n- Be able to choose metrics depending on whether you have ground truth, partial labels, or purely exploratory goals.\n- Communicate results in biologically meaningful ways (marker genes).\n\n:::\n\n\n## Conceptual framing\n\n* **Internal metrics**: use only the data + clustering labels. Measure compactness vs separation (e.g., silhouette).\n* **External metrics**: require ground truth/labels (experimental groups, annotated cell types). Use ARI, NMI, precision/recall on pairwise same/different labels.\n* **Biological validation**: compare clusters to known marker genes, pathways, experimental metadata (batch, donor), or enrichment tests. Often the most important for biologists.\n\n---\n\n## Quick metrics cheat-sheet (what they tell you)\n\n* **Explained variance (PCA)** — fraction of variance captured by components (useful for dimensionality reduction decisions).\n* **Silhouette score** (−1..1) — how well each sample fits its cluster vs nearest other cluster; good general-purpose internal metric.\n* **Calinski–Harabasz** — ratio of between/within dispersion (higher = better).\n* **Davies–Bouldin** (lower = better) — average similarity between each cluster and its most similar one.\n* **Adjusted Rand Index (ARI)** — similarity between two labelings corrected for chance (commonly 0..1).\n* **Normalized Mutual Information (NMI)** — information overlap between labelings (0..1).\n* **Trustworthiness** (for embeddings like UMAP/t-SNE) — how well local neighborhoods are preserved.\n* **Stability / reproducibility** — how consistent cluster assignments are under parameter changes.\n\n---\n\n\n## (Optional) Exercise on cancer data\n\n* You have been given some data on cancer cell lines\n\n* Team up with someone and perform hierarchical clustering on this data\n\n* You have been given some starter code to help you load the data\n\n* The data has been downloaded and processed for you (after you run the code below). \n\n* The data is in the variable named `X`\n\n<!-- TODO: find out what are labels -->\n\n<!-- TODO: silhouette plots -->\n\n::: {#3440d79a .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport os\nimport requests\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport matplotlib.pyplot as plt\n\n# Load data\nX = pd.read_csv(\"https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/refs/heads/main/course_files/data/cancer_data_saved_NC160.csv\", index_col=0)\n\n\nprint(\"Fetching labels from GitHub...\")\nlabs_url = 'https://raw.githubusercontent.com/neelsoumya/python_machine_learning/main/data/NCI60labs.csv'\nresponse = requests.get(labs_url)\nresponse.raise_for_status()\n# Read the raw text and split into lines.\nall_lines = response.text.strip().splitlines()\n\n# Skip the first line (the header) to match the data dimensions.\nlabs = all_lines[1:]\n\n# The labels in the file are quoted (e.g., \"CNS\"), so we remove the quotes.\nlabs = [label.strip('\"') for label in labs]\n\n# Your code below ......\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFetching labels from GitHub...\n```\n:::\n:::\n\n\n* Write your code while working in pairs or a group\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {#386e5c28 .cell execution_count=2}\n``` {.python .cell-code}\n# Hierarchical Clustering\nagg = AgglomerativeClustering(linkage='average', metric='manhattan')\ncluster_labels = agg.fit_predict(X)\n\n# Compute linkage matrix for the dendrogram\nZ = linkage(X, method='average', metric='cityblock')\n\n# Plot Dendrogram\nplt.figure()\ndendrogram(Z, labels=labs)\nplt.title('Hierarchical Clustering Dendrogram (NCI60, Average Linkage, Manhattan Distance)')\nplt.xlabel('Cell Line')\nplt.ylabel('Distance')\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](evaluation_files/figure-html/cell-3-output-1.png){width=705 height=470}\n:::\n:::\n\n\n:::\n<!-- end callout -->\n\n\n\n### Exercise: try another linkage method and another distance metric\n\n\n::: {.callout-tip}\n## Important Concept (recall)\n\n* There is not \"correct\" answer in unsupervised machine learning!\n\n* So how do you know when you are done? \n:::\n<!-- end callout -->\n\n\n## Evaluating the Quality of Clusters\n\nEvaluating the quality of clusters is a crucial step in any unsupervised learning task. Since we do not have a single _correct_ answer, we use several methods that fall into three main categories:\n\n### 1. Internal Evaluation\nMeasures how good the clustering is based only on the data itself (e.g., how dense and well-separated the clusters are).\n\n### 2. External Evaluation\nMeasures how well the clustering results align with known, ground-truth labels. This is possible here because the NCI60 dataset has known cancer cell line types, which we loaded as `labs`.\n\n### 3. Visual Evaluation\nInspecting plots (like the dendrogram or PCA) to see if the groupings seem logical.\n\n---\n\nLet us add the two most common metrics: one internal and one external.\n\n---\n\n### Internal Evaluation: Silhouette Score\n\nThe **Silhouette Score** measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation).\n\n- **Score Range**: -1 to +1  \n- **Interpretation**:\n  - **+1**: The sample is far away from the neighboring clusters (very good).\n  - **0**: The sample is on or very close to the decision boundary between two neighboring clusters.\n  - **-1**: The sample is assigned to the wrong cluster.\n\n---\n\n### External Evaluation: Adjusted Rand Index (ARI)\n\nThe **Adjusted Rand Index (ARI)** measures the similarity between the true labels (`labs`) and the labels assigned by our clustering algorithm (`cluster_labels`). It accounts for chance groupings.\n\n- **Score Range**: -1 to +1  \n- **Interpretation**:\n  - **+1**: Perfect agreement between true and predicted labels.\n  - **0**: Random labeling (no correlation).\n  - **< 0**: Worse than random labeling.\n\n\n* Here is how you would implement this\n\n::: {#98146b86 .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.metrics import silhouette_samples, silhouette_score, adjusted_rand_score # Import evaluation metrics\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Hierarchical Clustering\nagg = AgglomerativeClustering(linkage='average', metric='manhattan')\ncluster_labels = agg.fit_predict(X)\n\n# 1. Internal Evaluation: Silhouette Score\n# Measures how well-separated clusters are based on the data itself.\nsilhouette = silhouette_score(X, cluster_labels, metric='manhattan')\nprint(\"Silhouette score\")\nprint(silhouette)\nprint(\"Score is from -1 to 1. Higher is better\")\n\n# 2. External Evaluation: Adjusted Rand Index\n# Compares our cluster labels to the true cancer type labels.\nari = adjusted_rand_score(labs, cluster_labels)\nprint(\"Adjusted Rand Index\")\nprint(ari)\nprint(\"Compares to true labels. Score is from -1 to 1. Higher is better\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSilhouette score\n0.1436950300449066\nScore is from -1 to 1. Higher is better\nAdjusted Rand Index\n0.0554671516253694\nCompares to true labels. Score is from -1 to 1. Higher is better\n```\n:::\n:::\n\n\n* Silhouette plots\n\nA silhouette plot is a visual diagnostic for clustering quality that (1) computes a silhouette value for each sample and (2) shows the distribution of those values for every cluster. It helps you see which clusters are tight and well-separated and which contain ambiguous or poorly assigned samples.\n\n* Notes on interpreting silhouette plots\n\n- Each horizontal block is a cluster; the width at a given vertical position is the silhouette value of a sample.\n\n- Values close to +1 → sample is well matched to its own cluster and poorly matched to neighbors.\n\n- Values near 0 → sample lies between clusters.\n\n- Negative values → sample is likely assigned to the wrong cluster.\n\n- The red dashed line is the average silhouette score; use it as a quick summary, but always inspect per-cluster distributions — a high average can hide poorly-formed small clusters.\n\n::: {#ad76032d .cell execution_count=4}\n``` {.python .cell-code}\ndef plot_silhouette_simple(X, labels, metric='manhattan'):\n    \"\"\"\n    Minimal silhouette-bar plot.\n    X : array-like, shape (n_samples, n_features)\n    labels : array-like, cluster labels (integers)\n    metric : distance metric for silhouette (use same metric as clustering)\n    \"\"\"\n    unique_labels = np.unique(labels)\n    n_clusters = len(unique_labels)\n    if n_clusters < 2:\n        print(\"Need at least 2 clusters to compute silhouette.\")\n        return\n\n    # overall score\n    s_score = silhouette_score(X, labels, metric=metric)\n    print(f\"Silhouette score: {s_score:.3f}  (range -1 to 1; higher is better)\")\n\n    # per-sample silhouette values\n    sample_vals = silhouette_samples(X, labels, metric=metric)\n\n    plt.figure()\n    y_lower = 10  # starting y position for first cluster\n    for i, cl in enumerate(unique_labels):\n        vals = sample_vals[labels == cl]\n        vals.sort()\n        size = vals.shape[0]\n        y_upper = y_lower + size\n\n        # draw horizontal filled bars for this cluster\n        plt.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, vals,\n                          alpha=0.7)\n\n        # cluster label at left\n        plt.text(-0.05, y_lower + 0.5 * size, str(cl), va='center', fontsize=9)\n\n        y_lower = y_upper + 10  # 10-pixel spacing between groups\n\n    plt.axvline(x=s_score, color='k', linestyle='--', label=f'avg = {s_score:.3f}')\n    plt.xlabel('Silhouette coefficient')\n    plt.xlim(-0.1, 1.0)\n    plt.ylabel('Samples (clusters stacked)')\n    plt.title(f'Silhouette plot (n_clusters = {n_clusters})')\n    plt.legend(loc='lower right')\n    plt.ylim(0, y_lower)\n    plt.tight_layout()\n    plt.show()\n\n# Usage\n# from sklearn.cluster import AgglomerativeClustering\n# agg = AgglomerativeClustering(linkage='average', metric='manhattan')\n# cluster_labels = agg.fit_predict(X)\nplot_silhouette_simple(X, cluster_labels, metric='manhattan')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSilhouette score: 0.144  (range -1 to 1; higher is better)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](evaluation_files/figure-html/cell-5-output-2.png){width=662 height=470}\n:::\n:::\n\n\n\n\n::: {#c9a160fe .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](evaluation_files/figure-html/cell-7-output-1.png){width=951 height=470}\n:::\n\n::: {.cell-output .cell-output-display execution_count=6}\n![](evaluation_files/figure-html/cell-7-output-2.png){width=951 height=470}\n:::\n:::\n\n\n* Notes on interpreting silhouette plots\n\n- Each horizontal block is a cluster; the width at a given vertical position is the silhouette value of a sample.\n\n- Values close to +1 → sample is well matched to its own cluster and poorly matched to neighbors.\n\n- Values near 0 → sample lies between clusters.\n\n- Negative values → sample is likely assigned to the wrong cluster.\n\n- The red dashed line is the average silhouette score; use it as a quick summary, but always inspect per-cluster distributions — a high average can hide poorly-formed small clusters.\n\n\n* Compare to literature and what others have done\n\n* Plain old visual evaluation\n\n - compare to labels of what these cell lines are (assuming this is available)\n \n<!--TODO: XX what do these labels mean? -->\n\n\n\n\n\n<!--\nThe silhouette value (per sample)\n\nFor sample \n𝑖\ni:\n\n𝑎\n(\n𝑖\n)\na(i) = average distance from \n𝑖\ni to all other points in the same cluster (intra-cluster distance).\n\nFor every other cluster \n𝐶\nC, compute the average distance from \n𝑖\ni to points in \n𝐶\nC; let \n𝑏\n(\n𝑖\n)\nb(i) be the smallest of those (the nearest other cluster's average distance).\n\nThe silhouette value is\n\n𝑠\n(\n𝑖\n)\n=\n𝑏\n(\n𝑖\n)\n−\n𝑎\n(\n𝑖\n)\nmax\n⁡\n(\n𝑎\n(\n𝑖\n)\n,\n \n𝑏\n(\n𝑖\n)\n)\n.\ns(i)=\nmax(a(i),b(i))\nb(i)−a(i)\n\t​\n\n.\n\nRange: \n−\n1\n≤\n𝑠\n(\n𝑖\n)\n≤\n1\n−1≤s(i)≤1.\n\n𝑠\n(\n𝑖\n)\n≈\n1\ns(i)≈1: sample is well matched to its own cluster and far from neighbours.\n\n𝑠\n(\n𝑖\n)\n≈\n0\ns(i)≈0: sample lies between two clusters.\n\n𝑠\n(\n𝑖\n)\n<\n0\ns(i)<0: sample is probably assigned to the wrong cluster (closer on average to another cluster than to its own).\n\nExample: if \n𝑎\n=\n0.3\na=0.3 and \n𝑏\n=\n0.6\nb=0.6, then \n𝑠\n=\n(\n0.6\n−\n0.3\n)\n/\n0.6\n=\n0.5\ns=(0.6−0.3)/0.6=0.5 — a reasonably good assignment.\n\nThe silhouette plot\n\nEach cluster is shown as a horizontal block.\n\nWithin each block, samples are ordered by silhouette value and displayed as horizontal bars (width = \n𝑠\n(\n𝑖\n)\ns(i)).\n\nA dashed vertical line shows the average silhouette score across all samples.\n\nThe plot highlights:\n\nCluster compactness (long bars near +1),\n\nPresence of many low/negative values (bad clusters or misassignments),\n\nRelative sizes of clusters.\n-->\n\nHow to interpret (practical heuristics)\n\n- Mean silhouette ≳ 0.5 → strong structure (good clustering).\n\n- Mean silhouette ≈ 0.25–0.5 → weak to moderate structure; inspect clusters individually.\n\n- Mean silhouette ≲ 0.25 → little structure; clustering may be unreliable. (These are rules of thumb — context and domain knowledge matter.)\n\n\n* Silhouette plots for hierarchical clustering\n\nTODO: simplify code below\n\n::: {#13369fca .cell execution_count=9}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.cluster.hierarchy import linkage, fcluster, dendrogram\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport matplotlib.pyplot as plt\n\n# X: array-like, shape (n_samples, n_features)\n# Preprocess first (e.g., PCA)\nfrom sklearn.decomposition import PCA\nX_pca = PCA(n_components=30, random_state=0).fit_transform(X)  # optional but recommended\n\n# 1) compute linkage (Ward example; uses Euclidean)\nZ = linkage(X_pca, method='ward')   # method can be 'ward', 'single', 'complete', 'average', etc.\n\n# 2) turn tree into flat labels for desired k\nk = 5\nlabels = fcluster(Z, t=k, criterion='maxclust')  # labels are 1..k\n\n# 3) silhouette computations\nsil_vals = silhouette_samples(X_pca, labels, metric='euclidean')\navg_sil = silhouette_score(X_pca, labels, metric='euclidean')\nprint(f\"Average silhouette (k={k}):\", avg_sil)\n\n# 4) (optional) plot dendrogram and silhouette side-by-side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), gridspec_kw={'width_ratios':[1,1]})\n# dendrogram (cut to show leaves; set no labels for many samples)\ndendrogram(Z, no_labels=True, ax=ax1, color_threshold=None)\nax1.set_title('Dendrogram')\n\n# silhouette plot (simple)\nfrom matplotlib import cm\nunique_labels = np.unique(labels)\ny_lower = 10\ncmap = cm.nipy_spectral\nax2.set_xlim([-0.1, 1])\nfor i, cl in enumerate(unique_labels):\n    cl_sil = np.sort(sil_vals[labels == cl])\n    size_cluster = cl_sil.shape[0]\n    y_upper = y_lower + size_cluster\n    color = cmap(float(i) / max(1, (len(unique_labels)-1)))\n    ax2.fill_betweenx(np.arange(y_lower, y_upper), 0, cl_sil, facecolor=color, edgecolor=color, alpha=0.7)\n    ax2.text(-0.05, y_lower + 0.5*size_cluster, str(cl))\n    y_lower = y_upper + 10\nax2.axvline(x=avg_sil, color=\"red\", linestyle=\"--\", label=f\"avg = {avg_sil:.3f}\")\nax2.set_title(f\"Silhouette (k={k})\")\nax2.set_xlabel(\"Silhouette coefficient\")\nax2.set_yticks([])\nax2.legend()\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAverage silhouette (k=5): 0.16730055175012304\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](evaluation_files/figure-html/cell-10-output-2.png){width=1142 height=470}\n:::\n:::\n\n\n* Also compare to clusterings of other cancer cell lines\n\n* Does the cell line also show up in other datasets? (`external validation`)\n\n\n\n<!--\n\n# Practical code snippets (copy into a Jupyter notebook)\n\nReplace `X` with your feature matrix (cells × genes or PCA components), and `y_true` with any ground-truth labels you might have.\n\n```python\n# Required packages\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import (\n    silhouette_score,\n    calinski_harabasz_score,\n    davies_bouldin_score,\n    adjusted_rand_score,\n    normalized_mutual_info_score\n)\nfrom sklearn.manifold import trustworthiness\nfrom sklearn.model_selection import train_test_split\n\n# 1) PCA explained variance (dimensionality reduction check)\npca = PCA(n_components=20).fit(X)\nprint(\"Explained variance ratio (first 10):\", pca.explained_variance_ratio_[:10])\nprint(\"Cumulative:\", np.cumsum(pca.explained_variance_ratio_)[:10])\n\n# 2) Run simple clustering and internal metrics\nk = 6\nkm = KMeans(n_clusters=k, random_state=0).fit(X)\nlabels = km.labels_\nprint(\"Silhouette:\", silhouette_score(X, labels))\nprint(\"Calinski-Harabasz:\", calinski_harabasz_score(X, labels))\nprint(\"Davies-Bouldin:\", davies_bouldin_score(X, labels))\n\n# 3) If you have ground-truth labels (y_true)\n# print(\"ARI:\", adjusted_rand_score(y_true, labels))\n# print(\"NMI:\", normalized_mutual_info_score(y_true, labels))\n\n# 4) Trustworthiness for an embedding (e.g., embedding = UMAP_result)\n# trust = trustworthiness(X, embedding, n_neighbors=15)\n# print(\"Trustworthiness of embedding:\", trust)\n\n# 5) Stability via subsampling\ndef stability_score(clustering_fn, X, n_runs=10, sample_frac=0.8):\n    from sklearn.metrics import adjusted_rand_score\n    labels_list = []\n    rng = np.random.RandomState(0)\n    for i in range(n_runs):\n        idx = rng.choice(np.arange(X.shape[0]), size=int(sample_frac*X.shape[0]), replace=False)\n        labels_sub = clustering_fn(X[idx])\n        labels_list.append((idx, labels_sub))\n    # compute mean ARI between overlapping samples\n    aris = []\n    for i in range(len(labels_list)):\n        for j in range(i+1, len(labels_list)):\n            idx_i, lab_i = labels_list[i]\n            idx_j, lab_j = labels_list[j]\n            common = np.intersect1d(idx_i, idx_j)\n            if len(common) < 10:\n                continue\n            map_i = {v: k for k, v in enumerate(idx_i)}\n            map_j = {v: k for k, v in enumerate(idx_j)}\n            lab_i_common = [lab_i[map_i[c]] for c in common]\n            lab_j_common = [lab_j[map_j[c]] for c in common]\n            aris.append(adjusted_rand_score(lab_i_common, lab_j_common))\n    return np.mean(aris)\n\n# Example clustering function for kmeans on raw X\nclustering_fn = lambda Xsub: KMeans(n_clusters=k, random_state=0).fit(Xsub).labels_\n# print(\"Stability (mean ARI):\", stability_score(clustering_fn, X, n_runs=6))\n```\n\n---\n\n# Visual diagnostics \n\n* **Silhouette plot**: for each cluster show silhouette widths; points near 1 are clean, negative means bad assignment.\n* **Elbow plot**: inertia/within-cluster sum of squares vs k (use with caution).\n* **UMAP/t-SNE** colored by cluster and colored by known biological metadata (donor, batch, cell cycle) — if clusters align with batch, that's a red flag.\n* **Heatmap** of marker genes (or top variable genes) with clusters as columns — helps biological interpretation.\n* **Violin/boxplots** of expression of canonical marker genes across clusters.\n* **Confusion matrix** comparing cluster labels vs known labels (if available).\n* **Stability heatmap** of ARI between parameter settings (k values, min\\_samples for DBSCAN, perplexity for t-SNE).\n\n---\n\n# Biological validation techniques\n\n* Check expression of known **marker genes** per cluster — violin plots, fraction of cells expressing gene, median expression.\n* **Enrichment tests**: test if clusters are enriched for GO terms, pathways, or cell type marker sets.\n* Check **batch/donor composition** per cluster — if one donor dominates a cluster, suspect batch effect.\n* Use **pairwise marker identification** (differential expression between clusters) and then ask: does this match known biology?\n\n---\n\n# Hands-on classroom plan (90–120 minutes)\n\n1. **10 min** — Intuition + categories (internal / external / biological).\n2. **20 min** — Demo: run k-means on a tiny single-cell toy (PCA → kmeans), compute silhouette / CH / DB. Show silhouette plot and UMAP colored by clusters.\n3. **25 min** — Students exercise (paired): try k = 2..10, make elbow and silhouette; pick “best” k and justify biologically.\n4. **20 min** — Introduce external metrics (ARI/NMI). Give a partial ground truth (e.g., FACS labels) and compare clusterings. Interpret mismatch.\n5. **15 min** — Stability exercise: subsample and compute ARI between runs. Discuss parameter sensitivity (UMAP perplexity/embedding variance).\n6. **Final 10 min** — Discuss pitfalls & how to report results.\n\n---\n\n# Example mini-assignments (graded)\n\n1. **Compare 3 clustering algorithms** (k-means, hierarchical, DBSCAN) on a provided scRNA dataset: produce internal metric table, UMAPs, marker heatmap, and short writeup (max 500 words) interpreting differences and recommending one method.\n2. **Stability report**: for chosen clustering, show results of 10 subsamples, plot ARI distribution, and explain what it implies for reproducibility.\n3. **Biological validation**: pick 5 marker genes and show their distribution across clusters; perform enrichment test and conclude whether clusters are biologically meaningful.\n\n---\n\n# How to grade / rubric (suggested)\n\n* **40%** biological interpretability (marker genes, enrichment)\n* **30%** technical diagnostics (internal metrics, stability)\n* **20%** visualization & clarity (UMAPs, heatmaps, silhouette plot)\n* **10%** reproducibility (notebooks that run, parameter reporting)\n\n---\n\n# Common pitfalls to teach explicitly\n\n* Relying on a single metric (e.g., optimizing silhouette only) — always combine metrics + biology.\n* Over-interpreting t-SNE/UMAP global structure — these emphasize local neighborhoods; check trustworthiness.\n* Ignoring batch/donor effects — clusters that reflect batches are misleading.\n* Cherry-picking k or parameters to match prior beliefs — require pre-registered analysis choices or show sensitivity analyses.\n\n---\n\n# Quick “cheat-sheet” (one-paragraph version to hand out)\n\n* **No ground truth?** Use internal metrics (silhouette, CH, DB) + stability checks + marker gene verification.\n* **Have partial labels?** Use ARI/NMI to compare and treat labels as guides, not absolute truth.\n* **Embedding checks:** report PCA explained variance and trustworthiness for UMAP/t-SNE.\n* **Visualize:** always show embedding + gene heatmap + violin plots for markers.\n* **Reproducibility:** report parameters and run at least one sensitivity/subsampling analysis.\n\n---\n\n\n-->\n\n\n\n\n## Summary\n\n::: {.callout-tip}\n#### Key Points\n\n- We learnt evaluation is difficult in unsupervised machine learning!\n:::\n\n",
    "supporting": [
      "evaluation_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}