{
  "hash": "0ccc18ce537ff25dfe8524926d2a1472",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Hands-on exercises (Applications of unsupervised machine learning)\n---\n\n::: {.callout-tip}\n#### Learning Objectives\n\n- Understand real-world scenarios where unsupervised learning is applied\n- Identify situations where PCA and other dimensionality reduction techniques may not be effective\n- Practical examples of data that you try unsupervised learning techniques on\n- Learn how to evaluate the performance of unsupervised learning methods\n- Interpret and communicate the results of these models to each other\n:::\n<!-- end callout -->\n\n\n## When PCA does *not* work\n\n* When *not* to apply PCA or when PCA does *not* work\n\n### Key Takeaways: When PCA Fails\n\n### 1. **Non-linear Data Structures**\n- **Non-linearity**: Data that lies on curved surfaces or when data has non-linear relationships.\n- **Single-cell data**: Biological data where cell types form non-linear clusters in high-dimensional space\n\n### 2. **Categorical Features**\n- PCA works poorly with categorical data unless properly encoded\n- One-hot encoding categorical features can create sparse, high-dimensional data where PCA may not capture meaningful structure\n\n<!--\n### 3. **When Variance Direction ≠ Structure Direction**\n- PCA maximizes variance along principal components, but this doesn't always align with the true underlying structure\n- The lesson shows how PCA on XOR data captures only ~15% variance per component, failing to reveal the true four-cluster structure\n-->\n\n## Better Alternatives\n\n### **t-SNE** (t-Distributed Stochastic Neighbor Embedding)\n- **Best for**: Non-linear dimensionality reduction and visualization\n- **Key parameter**: Perplexity (try values 5-50)\n- **Use case**: Single-cell data, biological expression data, any non-linear clustering\n\n<!--\n### **Autoencoders**\n- **Best for**: Complex non-linear relationships in deep learning contexts\n- **Use case**: When you need to learn complex representations\n-->\n\n### **Hierarchical Clustering + Heatmaps**\n- **Best for**: Categorical data and understanding relationships between samples\n- **Use case**: When you want to see how samples group together based on multiple features\n\n\n\n### Code to demonstrate how PCA can fail\n\n* Generate synthetic biological expression data: matrix of 200 samples × 10 genes, where Gene_1 and Gene_2 follow a clustering (four corner clusters) and the remaining genes are just Gaussian noise. You can see from the scatter of Gene_1 vs Gene_2 that the true structure is non-linear and not aligned with any single variance direction: PCA would fail to unfold these clusters into separate principal components.\n\n::: {#289ed626 .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](applications_files/figure-html/cell-2-output-1.png){width=600 height=449}\n:::\n:::\n\n\n* Perform PCA on this data\n\n::: {#7a856c36 .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Apply PCA\npca = PCA()\npcs = pca.fit_transform(df)\n\n\n# Scatter plot of the first two principal components\nplt.figure()\nplt.scatter(pcs[:, 0], pcs[:, 1])\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA on Synthetic Biological Dataset')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](applications_files/figure-html/cell-3-output-1.png){width=587 height=449}\n:::\n:::\n\n\n* Let us try tSNE on this data\n\n::: {#949986ce .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\ntsne = TSNE()\ntsne_results = tsne.fit_transform(df)\n\n# plot\nplt.figure()\nplt.scatter(tsne_results[:,0], tsne_results[:,1])\nplt.xlabel('t-SNE component 1')\nplt.ylabel('t-SNE component 2')\nplt.title('t-SNE on Synthetic Biological Dataset')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](applications_files/figure-html/cell-4-output-1.png){width=600 height=449}\n:::\n:::\n\n\n* What if we try different values of *perplexity*?\n\n::: {#11b26465 .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](applications_files/figure-html/cell-5-output-1.png){width=1910 height=470}\n:::\n:::\n\n\n#### What if data has categorical features?\n\n* PCA may not work if you have categorical features\n\n::: {#a2dc4169 .cell execution_count=5}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Generate synthetic data with categorical features\nnp.random.seed(42)\nn_samples = 200\n\n# Categorical features\nspecies = np.random.choice(['mouse', 'rat', 'human'], size=n_samples)\ntissue = np.random.choice(['liver', 'brain', 'heart'], size=n_samples)\ncondition = np.random.choice(['healthy', 'diseased'], size=n_samples)\n\n# Create DataFrame\ndf_cat = pd.DataFrame({\n    'species': species,\n    'tissue': tissue,\n    'condition': condition\n})\n\nprint(df_cat.head())\n\n# One-hot encode the categorical features\nencoder = OneHotEncoder(sparse_output=False)\nencoded_data = encoder.fit_transform(df_cat)\n\n# Apply PCA\npca = PCA(n_components=2)\npcs = pca.fit_transform(encoded_data)\n\n# Plot PCA result\nplt.figure(figsize=(6,5))\nplt.scatter(pcs[:, 0], pcs[:, 1], alpha=0.7)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA on One-Hot Encoded Categorical Data')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Show the one-hot encoded feature names\nencoded_feature_names = encoder.get_feature_names_out(df_cat.columns)\nencoded_df = pd.DataFrame(encoded_data, columns=encoded_feature_names)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  species tissue condition\n0   human  liver  diseased\n1   mouse  brain  diseased\n2   human  liver  diseased\n3   human  brain  diseased\n4   mouse  brain   healthy\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](applications_files/figure-html/cell-6-output-2.png){width=566 height=470}\n:::\n:::\n\n\n* We can split by disease/healthy, etc.\n\n::: {#4c535eb3 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](applications_files/figure-html/cell-7-output-1.png){width=950 height=758}\n:::\n:::\n\n\n* TODO: XX tSNE and color by tissue type\n\n\n* Hierarchical clustering\n\n**Leaves**: Each leaf at the bottom of the dendrogram represents one sample from your dataset.\n\n**Branches**: The branches connect the samples and groups of samples. The height of the branch represents the distance (dissimilarity) between the clusters being merged.\n\n**Height of Merges**: Taller branches indicate that the clusters being merged are more dissimilar, while shorter branches indicate more similar clusters.\n\n**Clusters**: By drawing a horizontal line across the dendrogram at a certain distance, you can define clusters. All samples below that line that are connected by branches form a cluster.\n\n* In the context of your one-hot encoded categorical data (species, tissue, condition), the dendrogram shows how samples are grouped based on their combinations of these categorical features.\n\n* Samples with the same or very similar combinations of categories will be closer together in the dendrogram and merge at lower distances.\n\n* The structure of the dendrogram reflects the relationships and similarities between the different combinations of species, tissue, and condition present in your synthetic dataset.\n\n::: {#572ee2f2 .cell execution_count=7}\n``` {.python .cell-code}\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom matplotlib import pyplot as plt\n\n# Assume 'encoded_data' exists from the previous one-hot encoding step\nlinked = linkage(encoded_data, 'ward')\n\n# plot dendrogram\nplt.figure()\ndendrogram(linked, \n            orientation='top',\n            distance_sort='descending',\n            show_leaf_counts=True)\nplt.title('Hierarchical Clustering Dendrogram on One-Hot Encoded Categorical Data')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](applications_files/figure-html/cell-8-output-1.png){width=623 height=449}\n:::\n:::\n\n\n* Heatmaps\n\nHeatmaps are a great way to visualize data and clustering\n\n::: {#467a8a36 .cell execution_count=8}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assume 'encoded_df' exists from the previous one-hot encoding step\n\nplt.figure(figsize=(12, 8)) # Adjust figure size as needed\nsns.heatmap(encoded_df.T, cmap='viridis', cbar_kws={'label': 'Encoded Value (0 or 1)'}) \n\n# Transpose for features on y-axis\nplt.title('Heatmap of One-Hot Encoded Categorical Data')\nplt.xlabel('Sample Index')\nplt.ylabel('Encoded Feature')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](applications_files/figure-html/cell-9-output-1.png){width=1078 height=758}\n:::\n:::\n\n\n## How to evaluate unsupervised learning methods\n\nTODO: XX Potentially NC160 data exercise can live here\n\n\n\n## Exercises\n\n* Break up into small groups and work on any one of the following small projects.\n\n### Project using electronic healthcare records data {#sec-exr_title_EHR}\n\n::::: {#ex-title-EHR .callout-exercise}\n\n#### Electronic healthcare records data\n\n{{< level 3 >}}\n\n\nFor this exercise we will be using some data from hospital electronic healthcare records (EHR). No knowledge of biology/healthcare is required for this.\n\n \n:::: {.callout-answer collapse=\"true\"}\n\n#### Project briefing\n\nHere is a brief code snippet to help you load the data and get started.\n\nYou have to follow the following steps:\n\n* Data Loading and Preprocessing: Loading a diabetes dataset and normalizing numerical features.\n\n* Dimensionality Reduction: Applying PCA and t-SNE to reduce the dimensions of the data for visualization and analysis.\n\n* Clustering: Performing K-Means clustering on the reduced data to identify potential patient subgroups.\n\n* Visualization: Visualizing the data in lower dimensions and the identified clusters to gain insights.\n\n::: {.panel-tabset group=\"language\"}\n\n\n## Python\n\n::: {#43ea007b .cell execution_count=9}\n``` {.python .cell-code}\n!pip install pandas scikit-learn seaborn matplotlib openml\n```\n:::\n\n\n::: {#45014e4b .cell execution_count=10}\n``` {.python .cell-code}\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nimport openml\n\n#######################\n# Load diabetes data \n#######################\n\nurl = \"https://raw.githubusercontent.com/neelsoumya/python_machine_learning/main/data/diabetes_kaggle.csv\"\ndf = pd.read_csv(url)\n\n######################################\n# Perform data munging and filtering\n######################################\n\n# Normalize numeric columns\nnumeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\nscaler = MinMaxScaler()\ndf_normalized = df.copy()\ndf_normalized[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n\n# Filter: Glucose > 0.5 and BMI < 0.3 (normalized values)\nfiltered_df = df_normalized[\n    (df_normalized['Glucose'] > 0.5) &\n    (df_normalized['BMI'] < 0.3)\n]\n\nprint(filtered_df.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Pregnancies   Glucose  BloodPressure  SkinThickness   Insulin       BMI  \\\n9       0.470588  0.628141       0.786885       0.000000  0.000000  0.000000   \n49      0.411765  0.527638       0.000000       0.000000  0.000000  0.000000   \n50      0.058824  0.517588       0.655738       0.111111  0.096927  0.289121   \n145     0.000000  0.512563       0.614754       0.232323  0.000000  0.000000   \n239     0.000000  0.522613       0.622951       0.000000  0.000000  0.274218   \n\n     DiabetesPedigreeFunction       Age  Outcome  \n9                    0.065756  0.550000      1.0  \n49                   0.096926  0.050000      0.0  \n50                   0.176345  0.016667      0.0  \n145                  0.210931  0.000000      0.0  \n239                  0.215201  0.100000      0.0  \n```\n:::\n:::\n\n\n* Visualize the data\n\n::: {#100fd510 .cell execution_count=11}\n``` {.python .cell-code}\n# Histogram\nplt.figure()\nsns.histplot(df_normalized['Glucose'], bins=30)\nplt.title('Distribution of Normalized Glucose')\nplt.xlabel('Normalized Glucose')\nplt.ylabel('Frequency')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](applications_files/figure-html/cell-12-output-1.png){width=585 height=449}\n:::\n:::\n\n\n* Now visualize the other variables? Do you notice anything interesting/weird about them?\n\n* Perform PCA\n\n::: {#9a978dd8 .cell execution_count=12}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Exclude the target column for PCA\nfeatures = df_normalized.drop(columns=['Outcome'])\n\n# Apply PCA\n# This is where you fill in your code .........\n```\n:::\n\n\n* Fill in the rest of the code with your group members\\\n\n* Perform PCA and visualize it\n\n\n\n* Evaluation (how to interpret the PCA plots?)\n\n- In `plt.scatter`, the `c` parameter controls the marker colour (or colours).\n\n- the `alpha` parameter controls the transparency (opacity) of the markers.\n\n-  When passing numbers, you must specify `cmap` (colour map) to control the gradient mapping (otherwise the default colourmap is used)\n\n-  Let us colour by `BMI` now\n\n::: {#f9db3a6e .cell execution_count=14}\n``` {.python .cell-code}\n# Visualize PCA results colored by BMI\nplt.figure()\nscatter = plt.scatter(pca_df['PC1'], pca_df['PC2'], c=df_normalized['BMI'],\n                     cmap='viridis', alpha=0.7)\nplt.colorbar(scatter, label='BMI (normalized)')\nplt.title('PCA of Diabetes Dataset - Coloured by BMI')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](applications_files/figure-html/cell-15-output-1.png){width=571 height=431}\n:::\n:::\n\n\n* Do you see any patterns?\n\n* Now colour by `Pregnancies`\n\n* Try other features: `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `DiabetesPedigreeFunction`, `Age`\n\n* Try spotting any patterns and discuss this in your group\n\n* _Recall_: The primary goal is to uncover hidden patterns, structures, and relationships within the data. \n\n- This can lead to the generation of new hypotheses about the underlying phenomena, which can then be tested in follow-up studies using statistical methods or through the application of supervised machine learning techniques with labeled data.\n\n- Essentially, unsupervised learning helps us explore the data and formulate questions that can be further investigated.\n\n- However it is never the end of the data science pipeline but leads to more targetted investigations. \n\n* Now try tSNE on this data\n\n::: {#7895156f .cell execution_count=15}\n``` {.python .cell-code}\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Exclude the target column for t-SNE\nfeatures = df_normalized.drop(columns=['Outcome'])\n\n# Apply t-SNE\n# This is where you fill in your code .........\n```\n:::\n\n\n* Perform tSNE on this data\n\n\n\n* Vary the `perplexity` parameter\n\n* Now let us color the tSNE plot by `BMI`\n\n::: {#9e29896f .cell execution_count=17}\n``` {.python .cell-code}\n# Exclude the target column for t-SNE\n# Already done (so commenting out)\n# features_for_tsne = df_normalized.drop(columns=['Outcome', 'Cluster'])\n\n# Apply t-SNE\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\ntsne_results = tsne.fit_transform(features)\n\n# Create a DataFrame for the t-SNE results\ntsne_df = pd.DataFrame(data=tsne_results, columns=['TSNE1', 'TSNE2'])\n#tsne_df['Outcome'] = df_normalized['Outcome'].values\n#tsne_df['Cluster'] = df_normalized['Cluster'].values\n\n# Visualize t-SNE colored by BMI\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(tsne_df['TSNE1'], tsne_df['TSNE2'], c=df_normalized['BMI'],\n                     cmap='viridis', alpha=0.7, s=50)\nplt.colorbar(scatter, label='BMI (normalized)')\nplt.title('t-SNE of Diabetes Dataset - Colored by BMI')\nplt.xlabel('t-SNE Component 1')\nplt.ylabel('t-SNE Component 2')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](applications_files/figure-html/cell-18-output-1.png){width=786 height=671}\n:::\n:::\n\n\n* Now colour the tSNE plot by some other feature. Try `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `DiabetesPedigreeFunction`, `Age`\n\n* Do you observe any patterns? Discuss in your group.\n\n* Perform hierarchical clustering on this data\n\n::: {#b2f6a982 .cell execution_count=18}\n``` {.python .cell-code}\nimport pandas as pd\nfrom scipy.cluster.hierarchy import linkage, dendrogram, fcluster\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Exclude the target column for clustering\nfeatures = df_normalized.drop(columns=['Outcome'])\n\n# Perform hierarchical clustering\n# This is where you fill in your code .........\n```\n:::\n\n\n\n\n* Plot heatmaps (heatmaps are a great way to visualize your data)\n\n::: {#b4f994b5 .cell execution_count=20}\n``` {.python .cell-code}\n# Plot a heatmap of the normalized feature values for all samples\nplt.figure()\nsns.heatmap(features, cmap='viridis', cbar=True)\nplt.title('Heatmap of Normalized Feature Values (All Samples)')\nplt.xlabel('Features')\nplt.ylabel('Sample Index')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](applications_files/figure-html/cell-21-output-1.png){width=564 height=610}\n:::\n:::\n\n\n* Perform k-means on this data.\n\n\n\n* Discuss in your group the outcome of this project.\n\n - What are your key findings? \n - Do you think we can find partitions of patients/clusters of patients?\n - What can you do with these partitions?\n\n\n\n## R\n\n\n:::\n\n::::\n\n:::::\n\n \n\n::: {.callout-hint collapse=”true”}\n\nWork in a group!\n\n:::\n\n\n\n\n\n\n### Project using single-cell sequencing data {#sec-exr_singlecell}\n\n::::: {#single-cell .callout-exercise}\n\n#### Single-cell sequencing\n\n{{< level 3 >}}\n\n\nFor this exercise we will be using some single-cell sequencing data. No biological expertise is required for this.\n\n \n:::: {.callout-answer collapse=\"true\"}\n\n#### Exercise\n\nHere is a brief code snippet to help you load the data and get started.\n\nYou have to follow the following steps:\n\n* Data Loading and Preprocessing: Loading a single-cell sequencing dataset and normalizing features.\n\n* XX\n\n::: {.panel-tabset group=\"language\"}\n\n\n## Python\n\n\n## R\n\n\n:::\n\n::::\n\n:::::\n\n \n\n::: {.callout-hint collapse=”true”}\n\nWork in a group!\n\n:::\n\n\n\n\n\n\n\n\n### exercise_title {#sec-exr_title}\n\n::::: {#ex-title .callout-exercise}\n\n#### exercise_title\n\n{{< level 3 >}}\n\n\nFor this exercise we will be using the data from `data/surveys.csv`.\n\n \n:::: {.callout-answer collapse=\"true\"}\n\n#### subheading in answer\n\nDescription of answer.\n\n\n::: {.panel-tabset group=\"language\"}\n\n## R\n\n\n## Python\n\n:::\n\n::::\n\n:::::\n\n \n\n::: {.callout-hint collapse=”true”}\n\nThis is a hint\n\n:::\n\n\n\n\n\n## Summary\n\n::: {.callout-tip}\n#### Key Points\n\n- Understand real-world scenarios where unsupervised learning is applied\n- Identify situations where PCA and other dimensionality reduction techniques may not be effective\n- Practical examples of data that you try unsupervised learning techniques on\n:::\n\n\n\n## Resources\n\n- [Introduction to Statistical Learning in Python (ISLP) book](https://www.statlearning.com/)\n\n- [Video lectures by the authors of the book Introduction to Statistical Learning in Python](https://www.youtube.com/playlist?list=PLoROMvodv4rNHU1-iPeDRH-J0cL-CrIda)\n\n",
    "supporting": [
      "applications_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}