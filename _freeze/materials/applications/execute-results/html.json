{
  "hash": "b886511ba631ade842900f822ee42467",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Hands-on exercises (Applications of unsupervised machine learning)\n---\n\n::: {.callout-tip}\n#### Learning Objectives\n\n- Understand real-world scenarios where unsupervised learning is applied\n- Identify situations where PCA and other dimensionality reduction techniques may not be effective\n- Practical examples of data that you try unsupervised learning techniques on\n- Learn how to evaluate the performance of unsupervised learning methods\n- Interpret and communicate the results of these models to each other\n:::\n<!-- end callout -->\n\n\n## When PCA does *not* work\n\n* When *not* to apply PCA or when PCA does *not* work\n\n### Key Takeaways: When PCA Fails\n\n### 1. **Non-linear Data Structures**\n- **Non-linearity**: Data that lies on curved surfaces or when data has non-linear relationships.\n- **Single-cell data**: Biological data where cell types form non-linear clusters in high-dimensional space\n\n### 2. **Categorical Features**\n- PCA works poorly with categorical data unless properly encoded\n- One-hot encoding categorical features can create sparse, high-dimensional data where PCA may not capture meaningful structure\n\n<!--\n### 3. **When Variance Direction ≠ Structure Direction**\n- PCA maximizes variance along principal components, but this doesn't always align with the true underlying structure\n- The lesson shows how PCA on XOR data captures only ~15% variance per component, failing to reveal the true four-cluster structure\n-->\n\n## Better Alternatives\n\n### **t-SNE** (t-Distributed Stochastic Neighbor Embedding)\n- **Best for**: Non-linear dimensionality reduction and visualization\n- **Key parameter**: Perplexity (try values 5-50)\n- **Use case**: Single-cell data, biological expression data, any non-linear clustering\n\n<!--\n### **Autoencoders**\n- **Best for**: Complex non-linear relationships in deep learning contexts\n- **Use case**: When you need to learn complex representations\n-->\n\n### **Hierarchical Clustering + Heatmaps**\n- **Best for**: Categorical data and understanding relationships between samples\n- **Use case**: When you want to see how samples group together based on multiple features\n\n\n\n### Code to demonstrate how PCA can fail\n\n* Generate synthetic biological expression data: matrix of 200 samples × 10 genes, where Gene_1 and Gene_2 follow a clustering (four corner clusters) and the remaining genes are just Gaussian noise. You can see from the scatter of Gene_1 vs Gene_2 that the true structure is non-linear and not aligned with any single variance direction: PCA would fail to unfold these clusters into separate principal components.\n\n::: {#9545a975 .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](applications_files/figure-html/cell-2-output-1.png){width=600 height=449}\n:::\n:::\n\n\n* Perform PCA on this data\n\n::: {#05655a99 .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Apply PCA\npca = PCA()\npcs = pca.fit_transform(df)\n\n\n# Scatter plot of the first two principal components\nplt.figure()\nplt.scatter(pcs[:, 0], pcs[:, 1])\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA on Synthetic Biological Dataset')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](applications_files/figure-html/cell-3-output-1.png){width=587 height=449}\n:::\n:::\n\n\n* Let us try tSNE on this data\n\n::: {#98b9e2d2 .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\ntsne = TSNE()\ntsne_results = tsne.fit_transform(df)\n\n# plot\nplt.figure()\nplt.scatter(tsne_results[:,0], tsne_results[:,1])\nplt.xlabel('t-SNE component 1')\nplt.ylabel('t-SNE component 2')\nplt.title('t-SNE on Synthetic Biological Dataset')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](applications_files/figure-html/cell-4-output-1.png){width=600 height=449}\n:::\n:::\n\n\n* What if we try different values of *perplexity*?\n\n::: {#f238baae .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](applications_files/figure-html/cell-5-output-1.png){width=1910 height=470}\n:::\n:::\n\n\n#### What if data has categorical features?\n\n* PCA may not work if you have categorical features\n\n::: {#78c838f4 .cell execution_count=5}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Generate synthetic data with categorical features\nnp.random.seed(42)\nn_samples = 200\n\n# Categorical features\nspecies = np.random.choice(['mouse', 'rat', 'human'], size=n_samples)\ntissue = np.random.choice(['liver', 'brain', 'heart'], size=n_samples)\ncondition = np.random.choice(['healthy', 'diseased'], size=n_samples)\n\n# Create DataFrame\ndf_cat = pd.DataFrame({\n    'species': species,\n    'tissue': tissue,\n    'condition': condition\n})\n\nprint(df_cat.head())\n\n# One-hot encode the categorical features\nencoder = OneHotEncoder(sparse_output=False)\nencoded_data = encoder.fit_transform(df_cat)\n\n# Apply PCA\npca = PCA(n_components=2)\npcs = pca.fit_transform(encoded_data)\n\n# Plot PCA result\nplt.figure(figsize=(6,5))\nplt.scatter(pcs[:, 0], pcs[:, 1], alpha=0.7)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA on One-Hot Encoded Categorical Data')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Show the one-hot encoded feature names\nencoded_feature_names = encoder.get_feature_names_out(df_cat.columns)\nencoded_df = pd.DataFrame(encoded_data, columns=encoded_feature_names)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  species tissue condition\n0   human  liver  diseased\n1   mouse  brain  diseased\n2   human  liver  diseased\n3   human  brain  diseased\n4   mouse  brain   healthy\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](applications_files/figure-html/cell-6-output-2.png){width=566 height=470}\n:::\n:::\n\n\n* We can split by disease/healthy, etc.\n\n::: {#604306dc .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](applications_files/figure-html/cell-7-output-1.png){width=950 height=758}\n:::\n:::\n\n\n* TODO: XX tSNE and color by tissue type\n\n\n* Hierarchical clustering\n\n**Leaves**: Each leaf at the bottom of the dendrogram represents one sample from your dataset.\n\n**Branches**: The branches connect the samples and groups of samples. The height of the branch represents the distance (dissimilarity) between the clusters being merged.\n\n**Height of Merges**: Taller branches indicate that the clusters being merged are more dissimilar, while shorter branches indicate more similar clusters.\n\n**Clusters**: By drawing a horizontal line across the dendrogram at a certain distance, you can define clusters. All samples below that line that are connected by branches form a cluster.\n\n* In the context of your one-hot encoded categorical data (species, tissue, condition), the dendrogram shows how samples are grouped based on their combinations of these categorical features.\n\n* Samples with the same or very similar combinations of categories will be closer together in the dendrogram and merge at lower distances.\n\n* The structure of the dendrogram reflects the relationships and similarities between the different combinations of species, tissue, and condition present in your synthetic dataset.\n\n::: {#3804e9c0 .cell execution_count=7}\n``` {.python .cell-code}\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom matplotlib import pyplot as plt\n\n# Assume 'encoded_data' exists from the previous one-hot encoding step\nlinked = linkage(encoded_data, 'ward')\n\n# plot dendrogram\nplt.figure()\ndendrogram(linked, \n            orientation='top',\n            distance_sort='descending',\n            show_leaf_counts=True)\nplt.title('Hierarchical Clustering Dendrogram on One-Hot Encoded Categorical Data')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](applications_files/figure-html/cell-8-output-1.png){width=623 height=449}\n:::\n:::\n\n\n* Heatmaps\n\nHeatmaps are a great way to visualize data and clustering\n\n::: {#12057d42 .cell execution_count=8}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assume 'encoded_df' exists from the previous one-hot encoding step\n\nplt.figure(figsize=(12, 8)) # Adjust figure size as needed\nsns.heatmap(encoded_df.T, cmap='viridis', cbar_kws={'label': 'Encoded Value (0 or 1)'}) \n\n# Transpose for features on y-axis\nplt.title('Heatmap of One-Hot Encoded Categorical Data')\nplt.xlabel('Sample Index')\nplt.ylabel('Encoded Feature')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](applications_files/figure-html/cell-9-output-1.png){width=1078 height=758}\n:::\n:::\n\n\n## How to evaluate unsupervised learning methods\n\nTODO: XX Potentially NC160 data exercise can live here\n\n\n\n## Exercises\n\n### exercise_title {#sec-exr_title}\n\n::::: {#ex-title .callout-exercise}\n\n#### exercise_title\n\n{{< level 3 >}}\n\n\nFor this exercise we will be using the data from `data/surveys.csv`.\n\n \n:::: {.callout-answer collapse=\"true\"}\n\n#### subheading in answer\n\nDescription of answer.\n\n\n::: {.panel-tabset group=\"language\"}\n\n## R\n\n\n## Python\n\n:::\n\n::::\n\n:::::\n\n \n\n::: {.callout-hint collapse=”true”}\n\nThis is a hint\n\n:::\n\n\n\n## Hands-on exercises\n\n\nApplications and practical exercises (hands-on exercises).\n\n\n\n## Summary\n\n::: {.callout-tip}\n#### Key Points\n\n- Last section of the page is a bulleted summary of the key points\n:::\n\n\n\n## Resources\n\n- [ISLP book](https://www.statlearning.com/)\n\n- [Video lectures by the authors of the book Introduction to Statistical Learning in Python](https://www.youtube.com/playlist?list=PLoROMvodv4rNHU1-iPeDRH-J0cL-CrIda)\n\n",
    "supporting": [
      "applications_files"
    ],
    "filters": [],
    "includes": {}
  }
}