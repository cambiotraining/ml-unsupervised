---
title: Evaluation for unsupervised machine learning
---

::: {.callout-tip}
#### Learning Objectives

- How to evaluate unsupervised machine learning techniques
- Know the difference between **internal**, **external**, and **biological** evaluation.
- Be able to compute and interpret common metrics (silhouette).
- Be able to choose metrics depending on whether you have ground truth, partial labels, or purely exploratory goals.
- Communicate results in biologically meaningful ways (marker genes).

:::


## Conceptual framing

* **Internal metrics**: use only the data + clustering labels. Measure compactness vs separation (e.g., silhouette).
* **External metrics**: require ground truth/labels (experimental groups, annotated cell types). 
<!--Use ARI, NMI, precision/recall on pairwise same/different labels.-->
* **Biological validation**: compare clusters to known marker genes, pathways, experimental metadata (batch, donor), or enrichment tests. Often the most important for biologists.

---

## Quick metrics cheat-sheet (what they tell you)

* **Explained variance (PCA)** — fraction of variance captured by components (useful for dimensionality reduction decisions).
* **Silhouette score** (−1..1) — how well each sample fits its cluster vs nearest other cluster; good general-purpose internal metric.
<!--
* **Calinski–Harabasz** — ratio of between/within dispersion (higher = better).
* **Davies–Bouldin** (lower = better) — average similarity between each cluster and its most similar one.
* **Adjusted Rand Index (ARI)** — similarity between two labelings corrected for chance (commonly 0..1).
* **Normalized Mutual Information (NMI)** — information overlap between labelings (0..1).
* **Trustworthiness** (for embeddings like UMAP/t-SNE) — how well local neighborhoods are preserved.
-->
* **Stability / reproducibility** — how consistent cluster assignments are under parameter changes.

---


## (Optional) Exercise on cancer data

* You have been given some data on cancer cell lines

* Team up with someone and perform hierarchical clustering on this data

* You have been given some starter code to help you load the data

* The data has been downloaded and processed for you (after you run the code below). 

* The data is in the variable named `X`

<!-- TODO: find out what are labels -->

<!-- TODO: silhouette plots -->

```{python ch4-exercise-cancer-starter-code}
#| warning: false

import numpy as np
import pandas as pd
import os
import requests
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# Load data
X = pd.read_csv("https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/refs/heads/main/course_files/data/cancer_data_saved_NC160.csv", index_col=0)


print("Fetching labels from GitHub...")
labs_url = 'https://raw.githubusercontent.com/neelsoumya/python_machine_learning/main/data/NCI60labs.csv'
response = requests.get(labs_url)
response.raise_for_status()
# Read the raw text and split into lines.
all_lines = response.text.strip().splitlines()

# Skip the first line (the header) to match the data dimensions.
labs = all_lines[1:]

# The labels in the file are quoted (e.g., "CNS"), so we remove the quotes.
labs = [label.strip('"') for label in labs]

# Your code below ......
```

* Write your code while working in pairs or a group

::: {.callout-note collapse="true"}
## Click to expand
```{python ch4-exercise-cancer-solution}
#| warning: false

# Hierarchical Clustering
agg = AgglomerativeClustering(linkage='average', metric='manhattan')
cluster_labels = agg.fit_predict(X)

# Compute linkage matrix for the dendrogram
Z = linkage(X, method='average', metric='cityblock')

# Plot Dendrogram
plt.figure()
dendrogram(Z, labels=labs)
plt.title('Hierarchical Clustering Dendrogram (NCI60, Average Linkage, Manhattan Distance)')
plt.xlabel('Cell Line')
plt.ylabel('Distance')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()
```
:::
<!-- end callout -->



### Exercise: try another linkage method and another distance metric


::: {.callout-tip}
## Important Concept (recall)

* There is no "correct" answer in unsupervised machine learning!

* So how do you know when you are done? 
:::
<!-- end callout -->


## Evaluating the Quality of Clusters

Evaluating the quality of clusters is a crucial step in any unsupervised learning task. Since we do not have a single _correct_ answer, we use several methods that fall into three main categories:

### 1. Internal Evaluation
Measures how good the clustering is based only on the data itself (e.g., how dense and well-separated the clusters are).

### 2. External Evaluation
Measures how well the clustering results align with known, ground-truth labels. This is possible here because the NCI60 dataset has known cancer cell line types, which we loaded as `labs`.

### 3. Visual Evaluation
Inspecting plots (like the dendrogram or PCA) to see if the groupings seem logical.

---

Let us add the two most common metrics: one internal and one external.

---

### Internal Evaluation: Silhouette Score

The **Silhouette Score** measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation).

- **Score Range**: -1 to +1  
- **Interpretation**:
  - **+1**: The sample is far away from the neighboring clusters (very good).
  - **0**: The sample is on or very close to the decision boundary between two neighboring clusters.
  - **-1**: The sample is assigned to the wrong cluster.

---

### External Evaluation: Adjusted Rand Index (ARI)

The **Adjusted Rand Index (ARI)** measures the similarity between the true labels (`labs`) and the labels assigned by our clustering algorithm (`cluster_labels`). It accounts for chance groupings.

- **Score Range**: -1 to +1  
- **Interpretation**:
  - **+1**: Perfect agreement between true and predicted labels.
  - **0**: Random labeling (no correlation).
  - **< 0**: Worse than random labeling.


* Here is how you would implement this

```{python ch4-evaluation}
#| warning: false

from sklearn.metrics import silhouette_samples, silhouette_score, adjusted_rand_score # Import evaluation metrics
import numpy as np
import matplotlib.pyplot as plt

# Hierarchical Clustering
agg = AgglomerativeClustering(linkage='average', metric='manhattan')
cluster_labels = agg.fit_predict(X)

# 1. Internal Evaluation: Silhouette Score
# Measures how well-separated clusters are based on the data itself.
silhouette = silhouette_score(X, cluster_labels, metric='manhattan')
print("Silhouette score")
print(silhouette)
print("Score is from -1 to 1. Higher is better")

# 2. External Evaluation: Adjusted Rand Index
# Compares our cluster labels to the true cancer type labels.
ari = adjusted_rand_score(labs, cluster_labels)
print("Adjusted Rand Index")
print(ari)
print("Compares to true labels. Score is from -1 to 1. Higher is better")
```


### Intuition of silhouette score

Think of each data point as asking two questions:

- **How close am I to points in my own cluster?** (call this **a**)  
- **How close would I be, on average, to the _nearest other_ cluster?** (call this **b**)

The silhouette value for the point compares those two answers:

- If **a** is much smaller than **b** (you are much closer to your own cluster than to any other), the silhouette is close to **+1** → *great fit*.  
- If **a ≈ b**, silhouette is near **0** → *on the boundary between clusters*.  
- If **a > b**, silhouette is **negative** → *probably misassigned* (you are closer to another cluster than your own).

Numerically:

\[
s = \frac{b - a}{\max(a, b)}
\]

So

\[
s \in [-1,\,1]
\]



<!--
* Silhouette plots

A silhouette plot is a visual diagnostic for clustering quality that (1) computes a silhouette value for each sample and (2) shows the distribution of those values for every cluster. It helps you see which clusters are tight and well-separated and which contain ambiguous or poorly assigned samples.

* Notes on interpreting silhouette plots

- Each horizontal block is a cluster; the width at a given vertical position is the silhouette value of a sample.

- Values close to +1 → sample is well matched to its own cluster and poorly matched to neighbors.

- Values near 0 → sample lies between clusters.

- Negative values → sample is likely assigned to the wrong cluster.

- The red dashed line is the average silhouette score; use it as a quick summary, but always inspect per-cluster distributions — a high average can hide poorly-formed small clusters.
-->


```{python ch3-eval-sil-plot-simple}
#| warning: false
#| echo: false
#| output: false

def plot_silhouette_simple(X, labels, metric='manhattan'):
    """
    Minimal silhouette-bar plot.
    X : array-like, shape (n_samples, n_features)
    labels : array-like, cluster labels (integers)
    metric : distance metric for silhouette (use same metric as clustering)
    """
    unique_labels = np.unique(labels)
    n_clusters = len(unique_labels)
    if n_clusters < 2:
        print("Need at least 2 clusters to compute silhouette.")
        return

    # overall score
    s_score = silhouette_score(X, labels, metric=metric)
    print(f"Silhouette score: {s_score:.3f}  (range -1 to 1; higher is better)")

    # per-sample silhouette values
    sample_vals = silhouette_samples(X, labels, metric=metric)

    plt.figure()
    y_lower = 10  # starting y position for first cluster
    for i, cl in enumerate(unique_labels):
        vals = sample_vals[labels == cl]
        vals.sort()
        size = vals.shape[0]
        y_upper = y_lower + size

        # draw horizontal filled bars for this cluster
        plt.fill_betweenx(np.arange(y_lower, y_upper),
                          0, vals,
                          alpha=0.7)

        # cluster label at left
        plt.text(-0.05, y_lower + 0.5 * size, str(cl), va='center', fontsize=9)

        y_lower = y_upper + 10  # 10-pixel spacing between groups

    plt.axvline(x=s_score, color='k', linestyle='--', label=f'avg = {s_score:.3f}')
    plt.xlabel('Silhouette coefficient')
    plt.xlim(-0.1, 1.0)
    plt.ylabel('Samples (clusters stacked)')
    plt.title(f'Silhouette plot (n_clusters = {n_clusters})')
    plt.legend(loc='lower right')
    plt.ylim(0, y_lower)
    plt.tight_layout()
    plt.show()

# Usage
# from sklearn.cluster import AgglomerativeClustering
# agg = AgglomerativeClustering(linkage='average', metric='manhattan')
# cluster_labels = agg.fit_predict(X)
plot_silhouette_simple(X, cluster_labels, metric='manhattan')
```

```{python ch3-eval-simple-sil-plot-otherpackages}
#| warning: false
#| echo: false
#| output: false

# Alternative
# pip install scikit-plot
#import matplotlib.pyplot as plt
#import scikitplot as skplt

# assuming X and cluster_labels already exist
#skplt.metrics.plot_silhouette(X, cluster_labels)
#plt.show()

# Alternative
#from yellowbrick.cluster import SilhouetteVisualizer
#from sklearn.cluster import AgglomerativeClustering
#import matplotlib.pyplot as plt

#est = AgglomerativeClustering(n_clusters=3, linkage='average', metric='manhattan')
#viz = SilhouetteVisualizer(est)   # will call .fit(X) internally
#viz.fit(X)
#viz.show()
```



```{python ch-eval-silhouette-plots}
#| warning: false
#| echo: false
#| output: false

"""
Silhouette plot utilities.

Dependencies:
  pip install scikit-learn matplotlib numpy

Usage:
  from sklearn.cluster import KMeans
  labels = KMeans(n_clusters=4, random_state=0).fit_predict(X)
  fig = plot_silhouette(X, labels)                     # silhouette only
  fig = plot_silhouette(X, labels, embedding=umap_emb) # silhouette + 2D embedding
"""
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
from sklearn.metrics import silhouette_samples, silhouette_score

def plot_silhouette(X, labels, metric='euclidean', embedding=None,
                    figsize=(10, 5), title=None, show=True, save_path=None):
    """
    Draw a silhouette plot for clustering labels on dataset X.

    Args:
      X : array-like, shape (n_samples, n_features)
      labels : array-like, shape (n_samples,)
      metric : distance metric passed to silhouette_samples (default 'euclidean')
      embedding : optional array-like (n_samples, 2). If provided, draws a second panel
                  showing the 2D embedding colored by cluster.
      figsize : tuple, figure size
      title : optional string title for the figure
      show : if True, call plt.show()
      save_path : path to save figure (PNG/PDF). If None, don't save.

    Returns:
      matplotlib.figure.Figure
    """
    X = np.asarray(X)
    labels = np.asarray(labels)
    n_samples = X.shape[0]
    unique_labels = np.unique(labels)
    n_clusters = unique_labels.shape[0]
    if n_clusters < 2:
        raise ValueError("Need at least 2 clusters for silhouette plot (found %d)." % n_clusters)

    # compute silhouette values
    sil_vals = silhouette_samples(X, labels, metric=metric)
    avg_sil = silhouette_score(X, labels, metric=metric)

    # Figure layout: 1 panel if no embedding, 2 panels otherwise
    if embedding is None:
        fig, ax1 = plt.subplots(1, 1, figsize=figsize)
        ax2 = None
    else:
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize, gridspec_kw={'width_ratios': [1.2, 1]})

    # --- Silhouette panel ---
    ax1.set_xlim([-0.1, 1])  # silhouette value range (silhouette can be < -0.1 sometimes)
    # y axis range depends on number of samples
    y_lower = 10
    cmap = cm.nipy_spectral
    for i, cl in enumerate(unique_labels):
        cl_sil_vals = sil_vals[labels == cl]
        cl_sil_vals.sort()
        size_cluster = cl_sil_vals.shape[0]
        y_upper = y_lower + size_cluster

        color = cmap(float(i) / max(1, (n_clusters - 1)))
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, cl_sil_vals,
                          facecolor=color, edgecolor=color, alpha=0.7)
        # label cluster number in the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster, str(cl))
        y_lower = y_upper + 10  # 10 for spacing between clusters

    ax1.set_title("Silhouette plot" if title is None else f"Silhouette — {title}")
    ax1.set_xlabel("Silhouette coefficient")
    ax1.set_ylabel("Cluster")
    ax1.axvline(x=avg_sil, color="red", linestyle="--", label=f"avg = {avg_sil:.3f}")
    ax1.legend(loc='upper right')
    ax1.set_yticks([])  # we use text labels instead
    ax1.set_xticks(np.linspace(-0.1, 1.0, 6))

    # --- Embedding panel (optional) ---
    if embedding is not None:
        emb = np.asarray(embedding)
        if emb.shape[0] != n_samples or emb.shape[1] != 2:
            raise ValueError("embedding must be shape (n_samples, 2)")
        for i, cl in enumerate(unique_labels):
            mask = labels == cl
            color = cmap(float(i) / max(1, (n_clusters - 1)))
            ax2.scatter(emb[mask, 0], emb[mask, 1], s=10, color=color, alpha=0.8, label=str(cl))
        ax2.set_title("2D embedding (colored by cluster)")
        ax2.set_xlabel("dim 1")
        ax2.set_ylabel("dim 2")
        ax2.legend(title="cluster", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')

    plt.tight_layout()
    if save_path:
        fig.savefig(save_path, bbox_inches='tight', dpi=200)
    if show:
        plt.show()
    return fig

# --------------------------
# Example usage:
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
k = 4
labels = KMeans(n_clusters=k, random_state=0).fit_predict(X)
# # optional 2D embedding (PCA or UMAP)
pca2 = PCA(n_components=2).fit_transform(X)
plot_silhouette(X, labels, embedding=pca2, title=f"k={k}")
```

<!--
* Notes on interpreting silhouette plots

- Each horizontal block is a cluster; the width at a given vertical position is the silhouette value of a sample.

- Values close to +1 → sample is well matched to its own cluster and poorly matched to neighbors.

- Values near 0 → sample lies between clusters.

- Negative values → sample is likely assigned to the wrong cluster.

- The red dashed line is the average silhouette score; use it as a quick summary, but always inspect per-cluster distributions — a high average can hide poorly-formed small clusters.
-->

* Compare to literature and what others have done

* Plain old visual evaluation

 - compare to labels of what these cell lines are (assuming this is available)
 
<!--TODO: XX what do these labels mean? -->

```{python ch4-labs-visual-evaluation}
#| warning: false
#| echo: false
#| output: false

import numpy as np
import requests
import io
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# URLs for the raw data and labels from GitHub repository
data_url = 'https://raw.githubusercontent.com/neelsoumya/python_machine_learning/main/data/NCI60data.npy'
labs_url = 'https://raw.githubusercontent.com/neelsoumya/python_machine_learning/main/data/NCI60labs.csv'

# Load Data from GitHub
response = requests.get(data_url)
response.raise_for_status()
X = np.load(io.BytesIO(response.content))
print("NCI60 data loaded successfully from GitHub.")

# ALTERNATIVE
#  Download data from https://github.com/cambiotraining/ml-unsupervised/tree/main/course_files/data
#  Use pandas to load data 

# Load Labels from GitHub
print("Fetching labels from GitHub...")
response = requests.get(labs_url)
response.raise_for_status()
# Read the raw text and split into lines.
all_lines = response.text.strip().splitlines()

# Skip the first line (the header) to match the data dimensions.
labs = all_lines[1:]

# The labels in the file are quoted (e.g., "CNS"), so we remove the quotes.
labs = [label.strip('"') for label in labs]

# The data is in the variable named X
print(X[:5])
print(X.shape)


dendrogram(Z, labels=labs)

print(labs)

plt.figure()
plt.title('Hierarchical Clustering Dendrogram (NCI60)')
plt.xlabel('Cell Line (True Labels)')
plt.ylabel('Distance')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()
```

```{python ch3-eval-more-code}
#| warning: false
#| echo: false
#| output: false

# Required packages
import numpy as np
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import (
    silhouette_score,
    calinski_harabasz_score,
    davies_bouldin_score,
    adjusted_rand_score,
    normalized_mutual_info_score
)
from sklearn.manifold import trustworthiness
from sklearn.model_selection import train_test_split

# 1) PCA explained variance (dimensionality reduction check)
pca = PCA(n_components=20).fit(X)
print("Explained variance ratio (first 10):", pca.explained_variance_ratio_[:10])
print("Cumulative:", np.cumsum(pca.explained_variance_ratio_)[:10])

# 2) Run simple clustering and internal metrics
k = 6
km = KMeans(n_clusters=k, random_state=0).fit(X)
labels = km.labels_
print("Silhouette:", silhouette_score(X, labels))
print("Calinski-Harabasz:", calinski_harabasz_score(X, labels))
print("Davies-Bouldin:", davies_bouldin_score(X, labels))

# 3) If you have ground-truth labels (y_true)
# print("ARI:", adjusted_rand_score(y_true, labels))
# print("NMI:", normalized_mutual_info_score(y_true, labels))

# 4) Trustworthiness for an embedding (e.g., embedding = UMAP_result)
# trust = trustworthiness(X, embedding, n_neighbors=15)
# print("Trustworthiness of embedding:", trust)

# 5) Stability via subsampling
def stability_score(clustering_fn, X, n_runs=10, sample_frac=0.8):
    from sklearn.metrics import adjusted_rand_score
    labels_list = []
    rng = np.random.RandomState(0)
    for i in range(n_runs):
        idx = rng.choice(np.arange(X.shape[0]), size=int(sample_frac*X.shape[0]), replace=False)
        labels_sub = clustering_fn(X[idx])
        labels_list.append((idx, labels_sub))
    # compute mean ARI between overlapping samples
    aris = []
    for i in range(len(labels_list)):
        for j in range(i+1, len(labels_list)):
            idx_i, lab_i = labels_list[i]
            idx_j, lab_j = labels_list[j]
            # find intersection
            common = np.intersect1d(idx_i, idx_j)
            if len(common) < 10:
                continue
            # map labels for those common indices
            map_i = {v: k for k, v in enumerate(idx_i)}
            map_j = {v: k for k, v in enumerate(idx_j)}
            lab_i_common = [lab_i[map_i[c]] for c in common]
            lab_j_common = [lab_j[map_j[c]] for c in common]
            aris.append(adjusted_rand_score(lab_i_common, lab_j_common))
    return np.mean(aris)

# Example clustering function for kmeans on raw X
clustering_fn = lambda Xsub: KMeans(n_clusters=k, random_state=0).fit(Xsub).labels_
# print("Stability (mean ARI):", stability_score(clustering_fn, X, n_runs=6))
```




<!--
The silhouette value (per sample)

For sample 
𝑖
i:

𝑎
(
𝑖
)
a(i) = average distance from 
𝑖
i to all other points in the same cluster (intra-cluster distance).

For every other cluster 
𝐶
C, compute the average distance from 
𝑖
i to points in 
𝐶
C; let 
𝑏
(
𝑖
)
b(i) be the smallest of those (the nearest other cluster's average distance).

The silhouette value is

𝑠
(
𝑖
)
=
𝑏
(
𝑖
)
−
𝑎
(
𝑖
)
max
⁡
(
𝑎
(
𝑖
)
,
 
𝑏
(
𝑖
)
)
.
s(i)=
max(a(i),b(i))
b(i)−a(i)
	​

.

Range: 
−
1
≤
𝑠
(
𝑖
)
≤
1
−1≤s(i)≤1.

𝑠
(
𝑖
)
≈
1
s(i)≈1: sample is well matched to its own cluster and far from neighbours.

𝑠
(
𝑖
)
≈
0
s(i)≈0: sample lies between two clusters.

𝑠
(
𝑖
)
<
0
s(i)<0: sample is probably assigned to the wrong cluster (closer on average to another cluster than to its own).

Example: if 
𝑎
=
0.3
a=0.3 and 
𝑏
=
0.6
b=0.6, then 
𝑠
=
(
0.6
−
0.3
)
/
0.6
=
0.5
s=(0.6−0.3)/0.6=0.5 — a reasonably good assignment.

The silhouette plot

Each cluster is shown as a horizontal block.

Within each block, samples are ordered by silhouette value and displayed as horizontal bars (width = 
𝑠
(
𝑖
)
s(i)).

A dashed vertical line shows the average silhouette score across all samples.

The plot highlights:

Cluster compactness (long bars near +1),

Presence of many low/negative values (bad clusters or misassignments),

Relative sizes of clusters.
-->

How to interpret (practical heuristics)

- Mean silhouette ≳ 0.5 → strong structure (good clustering).

- Mean silhouette ≈ 0.25–0.5 → weak to moderate structure; inspect clusters individually.

- Mean silhouette ≲ 0.25 → little structure; clustering may be unreliable. (These are rules of thumb — context and domain knowledge matter.)


<!--
* Silhouette plots for hierarchical clustering

-->

```{python ch-eval-silhouette-hclust}
#| warning: false
#| echo: false
#| output: false

import numpy as np
from scipy.cluster.hierarchy import linkage, fcluster, dendrogram
from sklearn.metrics import silhouette_samples, silhouette_score
import matplotlib.pyplot as plt

# X: array-like, shape (n_samples, n_features)
# Preprocess first (e.g., PCA)
from sklearn.decomposition import PCA
X_pca = PCA(n_components=30, random_state=0).fit_transform(X)  # optional but recommended

# 1) compute linkage (Ward example; uses Euclidean)
Z = linkage(X_pca, method='ward')   # method can be 'ward', 'single', 'complete', 'average', etc.

# 2) turn tree into flat labels for desired k
k = 5
labels = fcluster(Z, t=k, criterion='maxclust')  # labels are 1..k

# 3) silhouette computations
sil_vals = silhouette_samples(X_pca, labels, metric='euclidean')
avg_sil = silhouette_score(X_pca, labels, metric='euclidean')
print(f"Average silhouette (k={k}):", avg_sil)

# 4) (optional) plot dendrogram and silhouette side-by-side
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), gridspec_kw={'width_ratios':[1,1]})
# dendrogram (cut to show leaves; set no labels for many samples)
dendrogram(Z, no_labels=True, ax=ax1, color_threshold=None)
ax1.set_title('Dendrogram')

# silhouette plot (simple)
from matplotlib import cm
unique_labels = np.unique(labels)
y_lower = 10
cmap = cm.nipy_spectral
ax2.set_xlim([-0.1, 1])
for i, cl in enumerate(unique_labels):
    cl_sil = np.sort(sil_vals[labels == cl])
    size_cluster = cl_sil.shape[0]
    y_upper = y_lower + size_cluster
    color = cmap(float(i) / max(1, (len(unique_labels)-1)))
    ax2.fill_betweenx(np.arange(y_lower, y_upper), 0, cl_sil, facecolor=color, edgecolor=color, alpha=0.7)
    ax2.text(-0.05, y_lower + 0.5*size_cluster, str(cl))
    y_lower = y_upper + 10
ax2.axvline(x=avg_sil, color="red", linestyle="--", label=f"avg = {avg_sil:.3f}")
ax2.set_title(f"Silhouette (k={k})")
ax2.set_xlabel("Silhouette coefficient")
ax2.set_yticks([])
ax2.legend()
plt.tight_layout()
plt.show()
```

* Also compare to clusterings of other cancer cell lines

* Does the cell line also show up in other datasets? (`external validation`)



<!--

# Practical code snippets (copy into a Jupyter notebook)

Replace `X` with your feature matrix (cells × genes or PCA components), and `y_true` with any ground-truth labels you might have.

```python
# Required packages
import numpy as np
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import (
    silhouette_score,
    calinski_harabasz_score,
    davies_bouldin_score,
    adjusted_rand_score,
    normalized_mutual_info_score
)
from sklearn.manifold import trustworthiness
from sklearn.model_selection import train_test_split

# 1) PCA explained variance (dimensionality reduction check)
pca = PCA(n_components=20).fit(X)
print("Explained variance ratio (first 10):", pca.explained_variance_ratio_[:10])
print("Cumulative:", np.cumsum(pca.explained_variance_ratio_)[:10])

# 2) Run simple clustering and internal metrics
k = 6
km = KMeans(n_clusters=k, random_state=0).fit(X)
labels = km.labels_
print("Silhouette:", silhouette_score(X, labels))
print("Calinski-Harabasz:", calinski_harabasz_score(X, labels))
print("Davies-Bouldin:", davies_bouldin_score(X, labels))

# 3) If you have ground-truth labels (y_true)
# print("ARI:", adjusted_rand_score(y_true, labels))
# print("NMI:", normalized_mutual_info_score(y_true, labels))

# 4) Trustworthiness for an embedding (e.g., embedding = UMAP_result)
# trust = trustworthiness(X, embedding, n_neighbors=15)
# print("Trustworthiness of embedding:", trust)

# 5) Stability via subsampling
def stability_score(clustering_fn, X, n_runs=10, sample_frac=0.8):
    from sklearn.metrics import adjusted_rand_score
    labels_list = []
    rng = np.random.RandomState(0)
    for i in range(n_runs):
        idx = rng.choice(np.arange(X.shape[0]), size=int(sample_frac*X.shape[0]), replace=False)
        labels_sub = clustering_fn(X[idx])
        labels_list.append((idx, labels_sub))
    # compute mean ARI between overlapping samples
    aris = []
    for i in range(len(labels_list)):
        for j in range(i+1, len(labels_list)):
            idx_i, lab_i = labels_list[i]
            idx_j, lab_j = labels_list[j]
            common = np.intersect1d(idx_i, idx_j)
            if len(common) < 10:
                continue
            map_i = {v: k for k, v in enumerate(idx_i)}
            map_j = {v: k for k, v in enumerate(idx_j)}
            lab_i_common = [lab_i[map_i[c]] for c in common]
            lab_j_common = [lab_j[map_j[c]] for c in common]
            aris.append(adjusted_rand_score(lab_i_common, lab_j_common))
    return np.mean(aris)

# Example clustering function for kmeans on raw X
clustering_fn = lambda Xsub: KMeans(n_clusters=k, random_state=0).fit(Xsub).labels_
# print("Stability (mean ARI):", stability_score(clustering_fn, X, n_runs=6))
```

---

# Visual diagnostics 

* **Silhouette plot**: for each cluster show silhouette widths; points near 1 are clean, negative means bad assignment.
* **Elbow plot**: inertia/within-cluster sum of squares vs k (use with caution).
* **UMAP/t-SNE** colored by cluster and colored by known biological metadata (donor, batch, cell cycle) — if clusters align with batch, that's a red flag.
* **Heatmap** of marker genes (or top variable genes) with clusters as columns — helps biological interpretation.
* **Violin/boxplots** of expression of canonical marker genes across clusters.
* **Confusion matrix** comparing cluster labels vs known labels (if available).
* **Stability heatmap** of ARI between parameter settings (k values, min\_samples for DBSCAN, perplexity for t-SNE).

---

# Biological validation techniques

* Check expression of known **marker genes** per cluster — violin plots, fraction of cells expressing gene, median expression.
* **Enrichment tests**: test if clusters are enriched for GO terms, pathways, or cell type marker sets.
* Check **batch/donor composition** per cluster — if one donor dominates a cluster, suspect batch effect.
* Use **pairwise marker identification** (differential expression between clusters) and then ask: does this match known biology?

---

# Hands-on classroom plan (90–120 minutes)

1. **10 min** — Intuition + categories (internal / external / biological).
2. **20 min** — Demo: run k-means on a tiny single-cell toy (PCA → kmeans), compute silhouette / CH / DB. Show silhouette plot and UMAP colored by clusters.
3. **25 min** — Students exercise (paired): try k = 2..10, make elbow and silhouette; pick “best” k and justify biologically.
4. **20 min** — Introduce external metrics (ARI/NMI). Give a partial ground truth (e.g., FACS labels) and compare clusterings. Interpret mismatch.
5. **15 min** — Stability exercise: subsample and compute ARI between runs. Discuss parameter sensitivity (UMAP perplexity/embedding variance).
6. **Final 10 min** — Discuss pitfalls & how to report results.

---

# Example mini-assignments (graded)

1. **Compare 3 clustering algorithms** (k-means, hierarchical, DBSCAN) on a provided scRNA dataset: produce internal metric table, UMAPs, marker heatmap, and short writeup (max 500 words) interpreting differences and recommending one method.
2. **Stability report**: for chosen clustering, show results of 10 subsamples, plot ARI distribution, and explain what it implies for reproducibility.
3. **Biological validation**: pick 5 marker genes and show their distribution across clusters; perform enrichment test and conclude whether clusters are biologically meaningful.

---

# How to grade / rubric (suggested)

* **40%** biological interpretability (marker genes, enrichment)
* **30%** technical diagnostics (internal metrics, stability)
* **20%** visualization & clarity (UMAPs, heatmaps, silhouette plot)
* **10%** reproducibility (notebooks that run, parameter reporting)

---

# Common pitfalls to teach explicitly

* Relying on a single metric (e.g., optimizing silhouette only) — always combine metrics + biology.
* Over-interpreting t-SNE/UMAP global structure — these emphasize local neighborhoods; check trustworthiness.
* Ignoring batch/donor effects — clusters that reflect batches are misleading.
* Cherry-picking k or parameters to match prior beliefs — require pre-registered analysis choices or show sensitivity analyses.

---

# Quick “cheat-sheet” (one-paragraph version to hand out)

* **No ground truth?** Use internal metrics (silhouette, CH, DB) + stability checks + marker gene verification.
* **Have partial labels?** Use ARI/NMI to compare and treat labels as guides, not absolute truth.
* **Embedding checks:** report PCA explained variance and trustworthiness for UMAP/t-SNE.
* **Visualize:** always show embedding + gene heatmap + violin plots for markers.
* **Reproducibility:** report parameters and run at least one sensitivity/subsampling analysis.

---


-->



## Summary

::: {.callout-tip}
#### Key Points

- We learnt evaluation is difficult in unsupervised machine learning!
:::
