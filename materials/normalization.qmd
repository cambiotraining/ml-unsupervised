---
title: "Normalizing your data and PCA"
format: html
---

# Introduction

This chapter demonstrates basic unsupervised machine learning concepts using Python.

::: {.callout-tip}
## Learning Objectives

- Refresher on Python
- Understand the difference between supervised and unsupervised learning.
- Apply PCA and clustering to example data.
- Visualize results.
:::


## Refresher on Python

<!--
If SSL error on Mac OS X,
then 

try in the Terminal

open "/Applications/Python 3.10/Install Certificates.command"
-->

```{python ch2-python-refresher}

# ============================================================================
# 1. IMPORTING PACKAGES
# ============================================================================

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# ============================================================================
# 2. READING DATA WITH PANDAS FROM GITHUB
# ============================================================================

# GitHub URL for the diabetes data
# Convert from GitHub web URL to raw data URL
github_url = "https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/main/course_files/data/diabetes_sample_data.csv"

# Read CSV file directly from GitHub
diabetes_data = pd.read_csv(github_url)
    
# Display basic information about the data
print("\nData shape:", diabetes_data.shape)
print("\nFirst 5 rows:")
print(diabetes_data.head())
        
print("\nBasic statistics:")
print(diabetes_data.describe())

# ============================================================================
# 3. PLOTTING WITH MATPLOTLIB
# ============================================================================

# Plot 1: Histogram of Age
plt.figure(figsize=(10, 6))
plt.hist(diabetes_data['age'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
plt.title('Distribution of Age', fontsize=14, fontweight='bold')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)
#plt.savefig('age_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

```


## Normalization (Z-score Standardization)

Normalization, specifically Z-score standardization, is a data scaling technique that transforms your data to have a mean of 0 and a standard deviation of 1. This is useful for many machine learning algorithms that are sensitive to the scale of input features.

The formula for Z-score is:

$$ z = \frac{x - \mu}{\sigma} $$

Where:
- $x$ is the original data point.
- $\mu$ is the mean of the data.
- $\sigma$ is the standard deviation of the data.

For example, say you have two variables or *features* on very different scales. 


| Age | Weight (grams) |
|-----|------------|
| 25  | 65000      |
| 30  | 70000      |
| 35  | 75000      |
| 40  | 80000      |
| 45  | 85000      |
| 50  | 90000      |
| 55  | 95000      |
| 60  | 100000     |
| 65  | 105000     |
| 70  | 110000     |
| 75  | 115000     |
| 80  | 120000     |

If these are not brought on similar scales, weight will have a dispproportionate influence on whatever machine learning model we build.

Hence we normalize each of the features *separately*, i.e. age is normalized relative to age and weight is normalized relative to weight.

```{python ch2-gen_data}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# 1. Generate age and weight data
np.random.seed(42)
age = np.random.normal(45, 15, 100)  # 100 people, mean age 45, std 15
age = np.clip(age, 18, 80)  # Keep ages between 18-80

weight = 70 + (age - 45) * 0.3 + np.random.normal(0, 10, 100)  # Weight correlated with age
weight = np.clip(weight, 45, 120)  # Keep weights between 45-120 kg

print("Original data:")
print(f"Age: mean={age.mean():.1f}, std={age.std():.1f}")
print(f"Weight: mean={weight.mean():.1f}, std={weight.std():.1f}")

# 2. Normalize the data
scaler = StandardScaler()
data = np.column_stack((age, weight))
normalized_data = scaler.fit_transform(data)

age_normalized = normalized_data[:, 0]
weight_normalized = normalized_data[:, 1]

# Histogram: Age (Original)
plt.figure()
plt.hist(age, bins=20, alpha=0.7)
plt.title('Age Distribution (Original)')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)
plt.show()

# Histogram: Age (Normalized)
plt.figure()
plt.hist(age_normalized, bins=20, alpha=0.7)
plt.title('Age Distribution (Normalized)')
plt.xlabel('Age (Z-score)')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.7)

plt.tight_layout()
plt.show()

```

## Setup

```{python}
#| echo: true
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
```

## Example Data

```{python}
#| echo: false

# Generate synthetic data
np.random.seed(42)
X = np.vstack([
    np.random.normal(loc=[0, 0], scale=1, size=(50, 2)),
    np.random.normal(loc=[5, 5], scale=1, size=(50, 2))
])
plt.scatter(X[:, 0], X[:, 1])
plt.title("Synthetic Data")
plt.show()
```

## PCA Example

<!--open tab-->
::: {.callout-note collapse="true"}
::: {.panel-tabset group="language"}

## Python

```{python}
#| echo: true
#| fig-cap: "A simple PCA plot"

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.title("PCA Projection")
plt.show()
```

## R

:::
:::
<!--close tab-->

## Scree plot

A scree plot is a simple graph that shows how much variance (information) each principal component explains in your data after running PCA. The x-axis shows the principal components (PC1, PC2, etc.), and the y-axis shows the proportion of variance explained by each one.

You can use a scree plot to decide how many principal components to keep: look for the point where the plot levels off (the *elbow*): this tells you that adding more components doesnâ€™t explain much more variance.


```{python ch2-screeplot}

# Scree plot: variance explained by each component
plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')
plt.title("Scree Plot")
plt.xlabel("Principal Component")
plt.ylabel("Variance Explained Ratio")
plt.show()

```

A scree plot may have an *elbow* like the plot below.

```{python ch2-screeplot-ideal}
#| echo: false
#| fig.cap: "An idealized scree plot"

from sklearn.datasets import make_blobs

# Generate synthetic data
X, y = make_blobs(n_samples=100, n_features=5, centers=3, random_state=42)

# Print the shape of the generated data
# print(X.shape)

from sklearn.preprocessing import StandardScaler

# Standardize the data X
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Initialize PCA with all components
pca = PCA(n_components=X_scaled.shape[1])

# Fit PCA to the standardized data
pca.fit(X_scaled)

# Transform the standardized data
X_pca = pca.transform(X_scaled)

# Create a figure and axes for the plot
plt.figure(figsize=(8, 5))

# Plot the explained variance ratio
plt.plot(range(1, pca.n_components_ + 1), pca.explained_variance_ratio_, marker='o', linestyle='--')

# Add title and labels
plt.title('Scree Plot')
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')

# Add grid
plt.grid(True)

# Display the plot
plt.show()

```

<!--
## Loadings

```{python ch2-loadings}

# pca.components_.T
#feature_names = ["Feature 1", "Feature 2"]  # Replace with your actual feature names if available
#loadings = pd.DataFrame(pca.components_.T, columns=["PC1", "PC2"], index=feature_names)
#print("PCA Loadings:")
#print(loadings)

```
-->

## Clustering Example

PCA is different to clustering where you are trying to find patterns in your data. We will encounter clustering later in the course.

```{python}
#| echo: false
#| fig.cap: "A simple clustering"

kmeans = KMeans(n_clusters=2, random_state=42)
labels = kmeans.fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels)
plt.title("KMeans Clustering")
plt.show()
```



## ðŸ§  PCA vs. Other Techniques

* PCA is **unsupervised** (no labels used)
* Works best for **linear** relationships
* Alternatives:

  * t-SNE for nonlinear structures

---

## ðŸ§¬ In Practice: Tips for Biologists

* Always **standardize** data before PCA
* Be cautious interpreting PCs biologicallyâ€”PCs are **mathematical constructs**



### Goals of unsupervised learning

* Finding patterns in data

Here is an example from biological data (single-cell sequencing data) (the plot is from [2])[@Aschenbrenner2020].

![Example tSNE ](https://gut.bmj.com/content/gutjnl/70/6/1023/F3.large.jpg)

![Example heatmaps](https://gut.bmj.com/content/gutjnl/70/6/1023/F5.large.jpg)


* Finding interesting patterns

You can also use dimensionality reduction techniques (such as PCA) to find interesting patterns in your data.

```{python ch2-outliers-pca}
#| echo : false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Set random seed
np.random.seed(42)

# 1. Generate data with outliers
# Main cluster
main_data = np.random.multivariate_normal(
    mean=[0, 0, 0],
    cov=[[1, 0.5, 0.3],
         [0.5, 1, 0.4],
         [0.3, 0.4, 1]],
    size=950
)

# Outliers
outliers = np.random.multivariate_normal(
    mean=[5, 5, 5],
    cov=[[0.5, 0.1, 0.1],
         [0.1, 0.5, 0.1],
         [0.1, 0.1, 0.5]],
    size=50
)

# Combine data
data = np.vstack([main_data, outliers])

# 2. Apply PCA
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)
pca = PCA(n_components=2)
data_pca = pca.fit_transform(data_scaled)

# 3. Plot PCA
plt.figure()
plt.scatter(data_pca[:-50, 0], data_pca[:-50, 1], c='blue', alpha=0.6, label='Normal Data')
plt.scatter(data_pca[-50:, 0], data_pca[-50:, 1], c='red', alpha=0.8, s=100, label='Outliers')
plt.title('PCA Projection - Outliers Highlighted')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

```

* Finding outliers

You can also use dimensionality reduction techniques (such as PCA) to find outliers in your data.


```{python ch2-one_outlier}
#| echo : false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Set random seed
np.random.seed(42)

# 1. Generate data with one outlier
# Main cluster
main_data = np.random.multivariate_normal(
    mean=[0, 0, 0],
    cov=[[1, 0.5, 0.3],
         [0.5, 1, 0.4],
         [0.3, 0.4, 1]],
    size=999
)

# Single outlier point
outlier = np.array([[10, 10, 10]])  # One extreme outlier point

# Combine data
data = np.vstack([main_data, outlier])

# 2. Apply PCA
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)
pca = PCA(n_components=2)
data_pca = pca.fit_transform(data_scaled)

# 3. Plot PCA
plt.figure()
plt.scatter(data_pca[:-1, 0], data_pca[:-1, 1], c='blue', alpha=0.6, label='Normal Data')
plt.scatter(data_pca[-1, 0], data_pca[-1, 1], c='red', alpha=0.8, s=200, label='Single Outlier')
plt.title('PCA Projection - Single Outlier Highlighted')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

```


* Finding hypotheses

All of these can be used to generate hypotheses. These hypotheses can be tested by collecting more data.

### Exercise

* Perform PCA on a dataset of US Arrests

Load data

```python
!pip install pca
```

```{python ch2-exercise-pca}
#| echo: false
#| output: false
#| warning: false

!pip install pca
```

```{python ch2-exercise-pca-loaddata}

from pca import pca
import pandas as pd

# Load the US Arrests data
# Read the USArrests data directly from the GitHub raw URL
url = "https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/main/course_files/data/USArrests.csv"
df = pd.read_csv(url, index_col=0)

print("US Arrests Data (first 5 rows):")
print(df.head())
print("\nData shape:", df.shape)
```


Perform PCA

```{python ch2-pca-usarrests}

model = pca(n_components=4)
out = model.fit_transform(df)
ax = model.biplot(n_feat=len(df.columns), legend=False)


```

* Variance explained plots

```{python ch2-exercise-pca-var}}
model.plot()

```

* 3D PCA biplots

```{python ch2-exercise-pca-3d}}
model.biplot3d()

```

* Loadings

*Recall*

What is being plotted on the axes (PC1 and PC2) are the `scores`.

The `scores` for each principal component are calculated as follows:

$$
PC_{1} = \alpha X + \beta Y + \gamma Z + .... 
$$

where $X$, $Y$ and $Z$ are the normalized *features*.

The constants $\alpha$, $\beta$, $\gamma$ are determined by the PCA algorithm. They are called the `loadings`.


```{python ch2-loadings-package}
print(model.results)
```

## Exercise (advanced)

::: {.callout-tip}
Look into the documentation available here for the [PCA package](::: {.callout-tip}) and plot prettier *publication ready* plots.
:::


::: {.callout-tip}
## Summary

- Need to normalize data before doing dimensionality reduction
- PCA reduces dimensionality for visualization.
- KMeans finds clusters in unlabeled data.
:::


## References

[1] [Article on normalization on Wikipedia](https://en.wikipedia.org/wiki/Standard_score)

[2] Deconvolution of monocyte responses in inflammatory bowel disease reveals an IL-1 cytokine network that regulates IL-23 in genetic and acquired IL-10 resistance Gut, 2020 [link](https://gut.bmj.com/content/70/6/1023)

---
