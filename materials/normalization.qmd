---
title: "Normalizing your data"
format: html
---

# Introduction

This Quarto notebook demonstrates basic unsupervised machine learning concepts using Python.

::: {.callout-tip}
## Learning Objectives

- Refresher on Python
- Understand the difference between supervised and unsupervised learning.
- Apply PCA and clustering to example data.
- Visualize results.
:::


## Refresher on Python

<!--
If SSL error on Mac OS X,
then 

try in the Terminal

open "/Applications/Python 3.10/Install Certificates.command"
-->

```{python ch2-python-refresher}

# ============================================================================
# 1. IMPORTING PACKAGES
# ============================================================================

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# ============================================================================
# 2. READING DATA WITH PANDAS FROM GITHUB
# ============================================================================

# GitHub URL for the diabetes data
# Convert from GitHub web URL to raw data URL
github_url = "https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/main/course_files/data/diabetes_sample_data.csv"

# Read CSV file directly from GitHub
diabetes_data = pd.read_csv(github_url)
    
# Display basic information about the data
print("\nData shape:", diabetes_data.shape)
print("\nFirst 5 rows:")
print(diabetes_data.head())
        
print("\nBasic statistics:")
print(diabetes_data.describe())

# ============================================================================
# 3. PLOTTING WITH MATPLOTLIB
# ============================================================================

# Plot 1: Histogram of Age
plt.figure(figsize=(10, 6))
plt.hist(diabetes_data['age'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
plt.title('Distribution of Age', fontsize=14, fontweight='bold')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)
#plt.savefig('age_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

```


## Normalization (Z-score Standardization)

Normalization, specifically Z-score standardization, is a data scaling technique that transforms your data to have a mean of 0 and a standard deviation of 1. This is useful for many machine learning algorithms that are sensitive to the scale of input features.

The formula for Z-score is:

$$ z = \frac{x - \mu}{\sigma} $$

Where:
- $x$ is the original data point.
- $\mu$ is the mean of the data.
- $\sigma$ is the standard deviation of the data.


```{python ch2-longtailed}
import numpy as np
import pandas as pd

# Sample data from a long-tailed (exponential) distribution
np.random.seed(42)
data = np.random.exponential(scale=2.0, size=1000)

# Calculate mean and standard deviation
mean = np.mean(data)
std_dev = np.std(data)

# Apply Z-score normalization
normalized_data = (data - mean) / std_dev

print("Original Data (first 10):", data[:10])
print("Mean:", mean)
print("Standard Deviation:", std_dev)
print("Normalized Data (first 10):", normalized_data[:10])

# You can also use scikit-learn's StandardScaler for this
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
normalized_data_sklearn = scaler.fit_transform(data.reshape(-1, 1))

```

```{python ch2-normalization-plot}

# TODO:
#. 1. generate age and weight data
#. 2. normalize those
#  3. visualize those

import matplotlib.pyplot as plt

# Plot original data histogram
plt.figure(figsize=(8, 4))
plt.hist(data, bins=10, color='skyblue', edgecolor='black', alpha=0.7, density=True)
plt.title('Original Data Distribution')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)
plt.show()

# Plot normalized data histogram
plt.figure(figsize=(8, 4))
plt.hist(normalized_data, bins=10, color='salmon', edgecolor='black', alpha=0.7, density=True)
plt.title('Normalized Data Distribution (Z-score)')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)
plt.show()

```

## Setup

```{python}
#| echo: true
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
```

## Example Data

```{python}
#| echo: true
# Generate synthetic data
np.random.seed(42)
X = np.vstack([
    np.random.normal(loc=[0, 0], scale=1, size=(50, 2)),
    np.random.normal(loc=[5, 5], scale=1, size=(50, 2))
])
plt.scatter(X[:, 0], X[:, 1])
plt.title("Synthetic Data")
plt.show()
```

## PCA Example

```{python}
#| echo: true
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.title("PCA Projection")
plt.show()
```

## Clustering Example

```{python}
#| echo: true
kmeans = KMeans(n_clusters=2, random_state=42)
labels = kmeans.fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels)
plt.title("KMeans Clustering")
plt.show()
```

## Conclusion

- PCA reduces dimensionality for visualization.
- KMeans finds clusters in unlabeled data.

## References

[1] [Article on normalization on Wikipedia](https://en.wikipedia.org/wiki/Standard_score)

---
