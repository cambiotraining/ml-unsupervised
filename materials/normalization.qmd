---
title: "Normalizing your data"
format: html
---

# Introduction

This chapter demonstrates basic unsupervised machine learning concepts using Python.

::: {.callout-tip}
## Learning Objectives

- Refresher on Python
- Understand the difference between supervised and unsupervised learning.
- Apply PCA and clustering to example data.
- Visualize results.
:::


## Refresher on Python

<!--
If SSL error on Mac OS X,
then 

try in the Terminal

open "/Applications/Python 3.10/Install Certificates.command"
-->

```{python ch2-python-refresher}

# ============================================================================
# 1. IMPORTING PACKAGES
# ============================================================================

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# ============================================================================
# 2. READING DATA WITH PANDAS FROM GITHUB
# ============================================================================

# GitHub URL for the diabetes data
# Convert from GitHub web URL to raw data URL
github_url = "https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/main/course_files/data/diabetes_sample_data.csv"

# Read CSV file directly from GitHub
diabetes_data = pd.read_csv(github_url)
    
# Display basic information about the data
print("\nData shape:", diabetes_data.shape)
print("\nFirst 5 rows:")
print(diabetes_data.head())
        
print("\nBasic statistics:")
print(diabetes_data.describe())

# ============================================================================
# 3. PLOTTING WITH MATPLOTLIB
# ============================================================================

# Plot 1: Histogram of Age
plt.figure(figsize=(10, 6))
plt.hist(diabetes_data['age'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
plt.title('Distribution of Age', fontsize=14, fontweight='bold')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)
#plt.savefig('age_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

```


## Normalization (Z-score Standardization)

Normalization, specifically Z-score standardization, is a data scaling technique that transforms your data to have a mean of 0 and a standard deviation of 1. This is useful for many machine learning algorithms that are sensitive to the scale of input features.

The formula for Z-score is:

$$ z = \frac{x - \mu}{\sigma} $$

Where:
- $x$ is the original data point.
- $\mu$ is the mean of the data.
- $\sigma$ is the standard deviation of the data.

For example, say you have two variables or *features* on very different scales. 


| Age | Weight (grams) |
|-----|------------|
| 25  | 65000      |
| 30  | 70000      |
| 35  | 75000      |
| 40  | 80000      |
| 45  | 85000      |
| 50  | 90000      |
| 55  | 95000      |
| 60  | 100000     |
| 65  | 105000     |
| 70  | 110000     |
| 75  | 115000     |
| 80  | 120000     |

If these are not brought on similar scales, weight will have a dispproportionate influence on whatever machine learning model we build.

Hence we normalize each of the features *separately*, i.e. age is normalized relative to age and weight is normalized relative to weight.

```{python ch2-gen_data}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# 1. Generate age and weight data
np.random.seed(42)
age = np.random.normal(45, 15, 100)  # 100 people, mean age 45, std 15
age = np.clip(age, 18, 80)  # Keep ages between 18-80

weight = 70 + (age - 45) * 0.3 + np.random.normal(0, 10, 100)  # Weight correlated with age
weight = np.clip(weight, 45, 120)  # Keep weights between 45-120 kg

print("Original data:")
print(f"Age: mean={age.mean():.1f}, std={age.std():.1f}")
print(f"Weight: mean={weight.mean():.1f}, std={weight.std():.1f}")

# 2. Normalize the data
scaler = StandardScaler()
data = np.column_stack((age, weight))
normalized_data = scaler.fit_transform(data)

age_normalized = normalized_data[:, 0]
weight_normalized = normalized_data[:, 1]

# Histogram: Age (Original)
plt.figure()
plt.hist(age, bins=20, alpha=0.7)
plt.title('Age Distribution (Original)')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)
plt.show()

# Histogram: Age (Normalized)
plt.figure()
plt.hist(age_normalized, bins=20, alpha=0.7)
plt.title('Age Distribution (Normalized)')
plt.xlabel('Age (Z-score)')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.7)

plt.tight_layout()
plt.show()

```

## Setup

```{python}
#| echo: true
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
```

## Example Data

```{python}
#| echo: false

# Generate synthetic data
np.random.seed(42)
X = np.vstack([
    np.random.normal(loc=[0, 0], scale=1, size=(50, 2)),
    np.random.normal(loc=[5, 5], scale=1, size=(50, 2))
])
plt.scatter(X[:, 0], X[:, 1])
plt.title("Synthetic Data")
plt.show()
```

## PCA Example

<!--open tab-->
::: {.callout-note collapse="true"}
::: {.panel-tabset group="language"}

## Python

```{python}
#| echo: true
#| fig-cap: "A simple PCA plot"

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.title("PCA Projection")
plt.show()
```

## R

:::
:::
<!--close tab-->



## Clustering Example

```{python}
#| echo: false
#| fig.cap: "A simple clustering"

kmeans = KMeans(n_clusters=2, random_state=42)
labels = kmeans.fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels)
plt.title("KMeans Clustering")
plt.show()
```



## ðŸ§  PCA vs. Other Techniques

* PCA is **unsupervised** (no labels used)
* Works best for **linear** relationships
* Alternatives:

  * t-SNE for nonlinear structures

---

## ðŸ§¬ In Practice: Tips for Biologists

* Always **standardize** data before PCA
* Be cautious interpreting PCs biologicallyâ€”PCs are **mathematical constructs**


## Exercise

* PCA on US Arrest data

```{python ch2-exercise-pca}

```


### Goals of unsupervised learning

* Finding patterns in data

Here is an example from biological data (single-cell sequencing data) (the plot is from [2])[@Aschenbrenner2020].

![Example tSNE ](https://gut.bmj.com/content/gutjnl/70/6/1023/F3.large.jpg)

![Example heatmaps](https://gut.bmj.com/content/gutjnl/70/6/1023/F5.large.jpg)


* Finding interesting patterns

You can also use dimensionality reduction techniques (such as PCA) to find interesting patterns in your data.

```{python ch2-outliers-pca}
#| echo : false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Set random seed
np.random.seed(42)

# 1. Generate data with outliers
# Main cluster
main_data = np.random.multivariate_normal(
    mean=[0, 0, 0],
    cov=[[1, 0.5, 0.3],
         [0.5, 1, 0.4],
         [0.3, 0.4, 1]],
    size=950
)

# Outliers
outliers = np.random.multivariate_normal(
    mean=[5, 5, 5],
    cov=[[0.5, 0.1, 0.1],
         [0.1, 0.5, 0.1],
         [0.1, 0.1, 0.5]],
    size=50
)

# Combine data
data = np.vstack([main_data, outliers])

# 2. Apply PCA
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)
pca = PCA(n_components=2)
data_pca = pca.fit_transform(data_scaled)

# 3. Plot PCA
plt.figure()
plt.scatter(data_pca[:-50, 0], data_pca[:-50, 1], c='blue', alpha=0.6, label='Normal Data')
plt.scatter(data_pca[-50:, 0], data_pca[-50:, 1], c='red', alpha=0.8, s=100, label='Outliers')
plt.title('PCA Projection - Outliers Highlighted')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

```

* Finding outliers

You can also use dimensionality reduction techniques (such as PCA) to find outliers in your data.


```{python ch2-one_outlier}
#| echo : false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Set random seed
np.random.seed(42)

# 1. Generate data with one outlier
# Main cluster
main_data = np.random.multivariate_normal(
    mean=[0, 0, 0],
    cov=[[1, 0.5, 0.3],
         [0.5, 1, 0.4],
         [0.3, 0.4, 1]],
    size=999
)

# Single outlier point
outlier = np.array([[10, 10, 10]])  # One extreme outlier point

# Combine data
data = np.vstack([main_data, outlier])

# 2. Apply PCA
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)
pca = PCA(n_components=2)
data_pca = pca.fit_transform(data_scaled)

# 3. Plot PCA
plt.figure()
plt.scatter(data_pca[:-1, 0], data_pca[:-1, 1], c='blue', alpha=0.6, label='Normal Data')
plt.scatter(data_pca[-1, 0], data_pca[-1, 1], c='red', alpha=0.8, s=200, label='Single Outlier')
plt.title('PCA Projection - Single Outlier Highlighted')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

```


* Finding hypotheses

All of these can be used to generate hypotheses. These hypotheses can be tested by collecting more data.



::: {.callout-tip}
## Summary

- Need to normalize data before doing dimensionality reduction
- PCA reduces dimensionality for visualization.
- KMeans finds clusters in unlabeled data.
:::


## References

[1] [Article on normalization on Wikipedia](https://en.wikipedia.org/wiki/Standard_score)

[2] Deconvolution of monocyte responses in inflammatory bowel disease reveals an IL-1 cytokine network that regulates IL-23 in genetic and acquired IL-10 resistance Gut, 2020 [link](https://gut.bmj.com/content/70/6/1023)

---
