---
title: "Normalizing your data"
format: html
---

# Introduction

This chapter demonstrates basic unsupervised machine learning concepts using Python.

::: {.callout-tip}
## Learning Objectives

- Refresher on Python
- Understand the difference between supervised and unsupervised learning.
- Apply PCA and clustering to example data.
- Visualize results.
:::


## Refresher on Python

<!--
If SSL error on Mac OS X,
then 

try in the Terminal

open "/Applications/Python 3.10/Install Certificates.command"
-->

```{python ch2-python-refresher}

# ============================================================================
# 1. IMPORTING PACKAGES
# ============================================================================

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# ============================================================================
# 2. READING DATA WITH PANDAS FROM GITHUB
# ============================================================================

# GitHub URL for the diabetes data
# Convert from GitHub web URL to raw data URL
github_url = "https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/main/course_files/data/diabetes_sample_data.csv"

# Read CSV file directly from GitHub
diabetes_data = pd.read_csv(github_url)
    
# Display basic information about the data
print("\nData shape:", diabetes_data.shape)
print("\nFirst 5 rows:")
print(diabetes_data.head())
        
print("\nBasic statistics:")
print(diabetes_data.describe())

# ============================================================================
# 3. PLOTTING WITH MATPLOTLIB
# ============================================================================

# Plot 1: Histogram of Age
plt.figure(figsize=(10, 6))
plt.hist(diabetes_data['age'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
plt.title('Distribution of Age', fontsize=14, fontweight='bold')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)
#plt.savefig('age_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

```


## Normalization (Z-score Standardization)

Normalization, specifically Z-score standardization, is a data scaling technique that transforms your data to have a mean of 0 and a standard deviation of 1. This is useful for many machine learning algorithms that are sensitive to the scale of input features.

The formula for Z-score is:

$$ z = \frac{x - \mu}{\sigma} $$

Where:
- $x$ is the original data point.
- $\mu$ is the mean of the data.
- $\sigma$ is the standard deviation of the data.

For example, say you have two variables or *features* on very different scales. 


| Age | Weight (grams) |
|-----|------------|
| 25  | 65000      |
| 30  | 70000      |
| 35  | 75000      |
| 40  | 80000      |
| 45  | 85000      |
| 50  | 90000      |
| 55  | 95000      |
| 60  | 100000     |
| 65  | 105000     |
| 70  | 110000     |
| 75  | 115000     |
| 80  | 120000     |

If these are not brought on similar scales, weight will have a dispproportionate influence on whatever machine learning model we build.

Hence we normalize each of the features *separately*, i.e. age is normalized relative to age and weight is normalized relative to weight.

```{python ch2-gen_data}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# 1. Generate age and weight data
np.random.seed(42)
age = np.random.normal(45, 15, 100)  # 100 people, mean age 45, std 15
age = np.clip(age, 18, 80)  # Keep ages between 18-80

weight = 70 + (age - 45) * 0.3 + np.random.normal(0, 10, 100)  # Weight correlated with age
weight = np.clip(weight, 45, 120)  # Keep weights between 45-120 kg

print("Original data:")
print(f"Age: mean={age.mean():.1f}, std={age.std():.1f}")
print(f"Weight: mean={weight.mean():.1f}, std={weight.std():.1f}")

# 2. Normalize the data
scaler = StandardScaler()
data = np.column_stack((age, weight))
normalized_data = scaler.fit_transform(data)

age_normalized = normalized_data[:, 0]
weight_normalized = normalized_data[:, 1]

# Histogram: Age (Original)
plt.figure()
plt.hist(age, bins=20, alpha=0.7)
plt.title('Age Distribution (Original)')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)
plt.show()

# Histogram: Age (Normalized)
plt.figure()
plt.hist(age_normalized, bins=20, alpha=0.7)
plt.title('Age Distribution (Normalized)')
plt.xlabel('Age (Z-score)')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.7)

plt.tight_layout()
plt.show()

```

## Setup

```{python}
#| echo: true
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
```

## Example Data

```{python}
#| echo: false

# Generate synthetic data
np.random.seed(42)
X = np.vstack([
    np.random.normal(loc=[0, 0], scale=1, size=(50, 2)),
    np.random.normal(loc=[5, 5], scale=1, size=(50, 2))
])
plt.scatter(X[:, 0], X[:, 1])
plt.title("Synthetic Data")
plt.show()
```

## PCA Example

<!--open tab-->
::: {.callout-note collapse="true"}
::: {.panel-tabset group="language"}

## Python

```{python}
#| echo: true
#| fig-cap: "A simple PCA plot"

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.title("PCA Projection")
plt.show()
```

## R

:::
:::
<!--close tab-->



## Clustering Example

```{python}
#| echo: false
#| fig.cap: "A simple clustering"

kmeans = KMeans(n_clusters=2, random_state=42)
labels = kmeans.fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels)
plt.title("KMeans Clustering")
plt.show()
```



## ðŸ§  PCA vs. Other Techniques

* PCA is **unsupervised** (no labels used)
* Works best for **linear** relationships
* Alternatives:

  * t-SNE for nonlinear structures

---

## ðŸ§¬ In Practice: Tips for Biologists

* Always **standardize** data before PCA
* Be cautious interpreting PCs biologicallyâ€”PCs are **mathematical constructs**



### Goals of unsupervised learning

* Finding patterns in data

Here is an example from biological data (single-cell sequencing data) (the plot is from [2])[@Aschenbrenner2020].

![Example tSNE ](https://gut.bmj.com/content/gutjnl/70/6/1023/F3.large.jpg)

![Example heatmaps](https://gut.bmj.com/content/gutjnl/70/6/1023/F5.large.jpg)


* Finding interesting patterns

You can also use dimensionality reduction techniques (such as PCA) to find interesting patterns in your data.

```{python ch2-outliers-pca}
#| echo : false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Set random seed
np.random.seed(42)

# 1. Generate data with outliers
# Main cluster
main_data = np.random.multivariate_normal(
    mean=[0, 0, 0],
    cov=[[1, 0.5, 0.3],
         [0.5, 1, 0.4],
         [0.3, 0.4, 1]],
    size=950
)

# Outliers
outliers = np.random.multivariate_normal(
    mean=[5, 5, 5],
    cov=[[0.5, 0.1, 0.1],
         [0.1, 0.5, 0.1],
         [0.1, 0.1, 0.5]],
    size=50
)

# Combine data
data = np.vstack([main_data, outliers])

# 2. Apply PCA
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)
pca = PCA(n_components=2)
data_pca = pca.fit_transform(data_scaled)

# 3. Plot PCA
plt.figure()
plt.scatter(data_pca[:-50, 0], data_pca[:-50, 1], c='blue', alpha=0.6, label='Normal Data')
plt.scatter(data_pca[-50:, 0], data_pca[-50:, 1], c='red', alpha=0.8, s=100, label='Outliers')
plt.title('PCA Projection - Outliers Highlighted')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

```

* Finding outliers

You can also use dimensionality reduction techniques (such as PCA) to find outliers in your data.


```{python ch2-one_outlier}
#| echo : false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Set random seed
np.random.seed(42)

# 1. Generate data with one outlier
# Main cluster
main_data = np.random.multivariate_normal(
    mean=[0, 0, 0],
    cov=[[1, 0.5, 0.3],
         [0.5, 1, 0.4],
         [0.3, 0.4, 1]],
    size=999
)

# Single outlier point
outlier = np.array([[10, 10, 10]])  # One extreme outlier point

# Combine data
data = np.vstack([main_data, outlier])

# 2. Apply PCA
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)
pca = PCA(n_components=2)
data_pca = pca.fit_transform(data_scaled)

# 3. Plot PCA
plt.figure()
plt.scatter(data_pca[:-1, 0], data_pca[:-1, 1], c='blue', alpha=0.6, label='Normal Data')
plt.scatter(data_pca[-1, 0], data_pca[-1, 1], c='red', alpha=0.8, s=200, label='Single Outlier')
plt.title('PCA Projection - Single Outlier Highlighted')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

```


* Finding hypotheses

All of these can be used to generate hypotheses. These hypotheses can be tested by collecting more data.

### Exercise

* Perform PCA on a dataset of US Arrests

```{python ch2-exercise-pca}}
#| echo: true

!pip install pca

from pca import pca
import pandas as pd

# Load the US Arrests data
us_arrests_data = {
    'Murder': [13.2, 10.0, 8.1, 8.8, 9.0, 7.9, 3.3, 5.9, 15.4, 17.4, 5.3, 2.6, 10.4, 7.2, 2.2, 6.0, 9.7, 15.4, 2.1, 11.3, 2.7, 16.5, 9.0, 6.0, 4.3, 12.1, 2.5, 0.8, 7.1, 11.4, 13.0, 6.8, 4.5, 12.2, 9.7, 6.6, 3.4, 14.4, 10.9, 11.1, 13.0, 6.3, 3.4, 1.4, 12.5, 9.0, 6.9, 4.2, 12.9, 5.3],
    'Assault': [236, 263, 294, 190, 276, 204, 110, 238, 335, 211, 46, 120, 249, 113, 56, 115, 109, 249, 83, 300, 72, 251, 120, 151, 96, 255, 53, 62, 178, 188, 337, 142, 169, 332, 293, 336, 86, 279, 159, 285, 337, 51, 120, 82, 257, 275, 243, 233, 337, 80],
    'UrbanPop': [58, 48, 80, 50, 91, 78, 77, 72, 80, 60, 83, 54, 62, 72, 66, 75, 82, 64, 39, 67, 70, 53, 65, 56, 51, 83, 51, 39, 71, 70, 91, 72, 74, 68, 87, 82, 45, 70, 53, 72, 80, 60, 67, 53, 95, 73, 76, 58, 90, 44],
    'Rape': [21.2, 44.5, 31.0, 19.5, 40.6, 38.7, 11.1, 15.8, 31.9, 25.8, 20.2, 14.2, 24.0, 21.0, 11.3, 18.0, 16.3, 22.2, 7.8, 27.8, 16.1, 17.5, 16.8, 15.6, 9.0, 35.1, 8.1, 11.6, 26.8, 23.0, 16.1, 21.4, 24.8, 18.0, 26.2, 20.2, 12.8, 22.5, 18.5, 25.5, 22.9, 11.6, 18.9, 8.3, 28.9, 24.9, 21.9, 17.1, 26.4, 12.8]
}

# State names
states = ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']

# Create DataFrame
df = pd.DataFrame(us_arrests_data, index=states)

print("US Arrests Data (first 5 rows):")
print(df.head())
print("\nData shape:", df.shape)


model = pca(n_components=4)
out = model.fit_transform(df)   # reduces df to 2 PCs
ax = model.biplot(n_feat=len(df.columns), legend=False)


```

* Variance explained plots

```{python ch2-exercise-pca-var}}
model.plot()

```

* 3D PCA biplots

```{python ch2-exercise-pca-3d}}
model.biplot3d()

```


## Exercise (advanced)

::: {.callout-tip}
Look into the documentation available here for the [PCA package](::: {.callout-tip}) and plot prettier *publication ready* plots.
:::


::: {.callout-tip}
## Summary

- Need to normalize data before doing dimensionality reduction
- PCA reduces dimensionality for visualization.
- KMeans finds clusters in unlabeled data.
:::


## References

[1] [Article on normalization on Wikipedia](https://en.wikipedia.org/wiki/Standard_score)

[2] Deconvolution of monocyte responses in inflammatory bowel disease reveals an IL-1 cytokine network that regulates IL-23 in genetic and acquired IL-10 resistance Gut, 2020 [link](https://gut.bmj.com/content/70/6/1023)

---
