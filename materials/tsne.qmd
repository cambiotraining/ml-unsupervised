---
title: tSNE
---

::: {.callout-tip}
#### Learning Objectives

- Bulleted list of learning objectives
- Why PCA does not work sometimes
- The intution around the *curse of dimensionality*
- What is tSNE?
- How to use tSNE
:::


<!-- TODO: explain curse better -->

## The curse of dimensionality

In very high-dimensional spaces, almost all the "volume" of a dataset lives near its corners, and pairwise Euclidean distances between points tend to concentrate around a single value. 


## Simplified explanations

Think of each cell as a point in a space where each gene‚Äôs activity is its own ‚Äúaxis.‚Äù When you have only a few genes (low dimensions), you can tell cells apart by how far apart they sit in that space. But as you add more genes, almost every cell ends up about the same distance from every other cell‚Äîso you lose any useful sense of ‚Äúclose‚Äù or ‚Äúfar.‚Äù



Imagine you're trying to find similar cells in a dataset. As you measure more features (dimensions), it becomes harder to find truly similar cells, even though you have more information.

## Visual Example: Finding Similar Points

### 1 Dimension (1 feature)
```
Feature 1: [0]----[1]----[2]----[3]----[4]----[5]
           A      B      C      D      E      F

Points A and B are close (distance = 1)
Points A and F are far (distance = 5)
```

### 2 Dimensions (2 features)
```
Feature 2: 5 |     F
           4 |  E
           3 |     D
           2 |  C
           1 |     B
           0 |A
             0  1  2  3  4  5  Feature 1

Points A and B are still close
Points A and F are still far
```

### 3+ Dimensions (3+ features)
```
Feature 3: 5 |     F
           4 |  E
           3 |     D
           2 |  C
           1 |     B
           0 |A
             0  1  2  3  4  5  Feature 1
             
Feature 4, 5, 6... (more dimensions)

As dimensions increase:
- All points become equally distant from each other
- "Close" and "far" lose meaning
- Finding similar cells becomes impossible
```

## Why This Happens: The "Empty Space" Problem


2D Circle - most area near the edge:

```
    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
  ‚ñà‚ñà‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚ñà‚ñà
 ‚ñà‚ñà‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚ñà‚ñà
‚ñà‚ñà‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚ñà‚ñà
‚ñà‚ñà‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚ñà‚ñà
 ‚ñà‚ñà‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚ñà‚ñà
  ‚ñà‚ñà‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚ñà‚ñà
    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà

```

::: {.callout-tip}
**NOTE (IMPORTANT CONCEPT)**: 

* ALL volume concentrates at the "surface"
The interior becomes essentially EMPTY!

* Your data points all end up at the edges, far apart from each other.


:::
<!-- end callout -->




**Why k-Means fails:**  
k-Means tries to draw boundaries around groups by asking ‚ÄúWhich centroid (group center) is each cell closest to?‚Äù In very high‚Äìgene spaces, every cell is nearly the same distance from all centroids. Small moves of the centroids don‚Äôt change which cells get assigned to them, so k-Means can‚Äôt find real groupings.

**Why t-SNE helps:**  
t-SNE ignores the idea of absolute distance and instead asks, ‚ÄúWhich cells are each cell‚Äôs few nearest neighbors?‚Äù It builds a map that keeps those local neighborhoods intact. In the final 2D picture, cells that were neighbors in the huge gene space stay neighbors on the screen, while cells that weren‚Äôt neighbors get pushed apart. This way, you still see meaningful clusters (e.g., cell types) even when dealing with hundreds or thousands of genes.

## TLDR (Simple explanation)

t-SNE (pronounced "tee-snee") is a tool that helps us look at complex data by making it easier to see patterns.

### Imagine this:
- You have a big box of mixed beads. Each bead has many features: color, size, shape, weight, etc.
- It is hard to see how the beads are similar or different just by looking at all these features at once.

### What t-SNE does:
- t-SNE takes all those features and creates a simple map (like a 2D picture).
- In this map, beads that are similar to each other are placed close together.
- Beads that are very different are placed far apart.


## Pictorial explanation of tSNE

High-dimensional beads (hard to see groups):

[üî¥] [üîµ] [üü¢] [üü°] [üî¥] [üü¢] [üîµ] [üü°] [üî¥] [üü¢] [üîµ] [üü°]

Each bead has many features (color, size, shape, etc.)

        |
        v

t-SNE makes a simple 2D map:

[üî¥]   [üî¥]   [üî¥]
         |      |
[üîµ]   [üîµ]   [üîµ]

[üü¢]   [üü¢]   [üü¢]

[üü°]   [üü°]   [üü°]

Now, similar beads are grouped together.


**In summary:**  
t-SNE is like a magic tool that turns complicated data into a simple picture, so we can easily see groups and patterns‚Äîeven if we do not understand the math behind it!

### Why is this useful?
- It helps us see groups or clusters in our data.
- We can spot patterns, like which beads are most alike, or if there are outliers.
- Emphasis on preserving local structure.

## Why t‚ÄëSNE Works in High Dimensions
 - Bypasses global distance concentration by focusing on nearest neighbors.

<!-- TODO: intuition behind tSNE 

TODO: The explanation of t-SNE could be improved. Currently you refer to it as a "magic tool", but the more detailed explanation (in the collapsed box) is very technical and hard to follow. A middle ground with a simple intuitive explanation would help. StatQuest video is excellent for this.


TODO: Perplexity is mentioned but not explained, which is worth clarifying.

-->


## Intuitive explanation of tSNE

[Explanation of tSNE (by StatQuest)](https://www.youtube.com/watch?v=NEaUSP4YerM)



---

## What does tSNE look like compared to PCA?

```{python mnist}
#| echo: false
#| warning: false

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_openml
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# 1. Load MNIST data
mnist = fetch_openml('mnist_784', version=1, as_frame=False)
X, y = mnist['data'], mnist['target'].astype(int)

# Run k-Means with k=10
k = 10
km = KMeans(n_clusters=k, random_state=42)
clusters = km.fit_predict(X)
inertia = km.inertia_

# 2-D PCA projection of the original MNIST data
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X)

plt.figure(figsize=(8,6))
scatter = plt.scatter(
    X_pca[:,0], X_pca[:,1],
    c=y, cmap='tab10', s=5, alpha=0.6
)
plt.title('MNIST 2-D PCA, colored by actual digits')
plt.xlabel('PC 1')
plt.ylabel('PC 2')
plt.legend(*scatter.legend_elements(), title='Digit', loc='best')
plt.tight_layout()
plt.show()

# Use a smaller subset of the data for t-SNE to speed up computation
n_samples = 5000  # You can adjust this number
X_subset = X[:n_samples]
clusters_subset = clusters[:n_samples]

tsne = TSNE(n_components=2, perplexity=30, early_exaggeration=12,
            learning_rate=200, max_iter=1000, random_state=42)
X_tsne = tsne.fit_transform(X_subset)

plt.figure(figsize=(8,6))
scatter = plt.scatter(
    X_tsne[:,0], X_tsne[:,1],
    c=clusters_subset, cmap='tab10', s=5, alpha=0.6
)
plt.title(f'MNIST 2-D t-SNE (subset of {n_samples} samples), colored by k-Means cluster')
plt.xlabel('t-SNE dim 1')
plt.ylabel('t-SNE dim 2')
plt.legend(*scatter.legend_elements(), title='Cluster', loc='best')
plt.tight_layout()
plt.show()

```


::: {.callout-tip collapse="true"}

### The complex explanation

In very high-dimensional spaces, almost all the ‚Äúvolume‚Äù of a dataset lives near its corners, and pairwise Euclidean distances between points tend to concentrate around a single value. As dimension $n$ grows, the volume of an inscribed ball in the hypercube $[-1,1]^n$ shrinks toward zero, and the ratio

$$
\frac{\max d - \min d}{\min d}
$$

for distances $d$ between random points rapidly approaches zero. Intuitively, ‚Äúnearest‚Äù and ‚Äúfarthest‚Äù neighbors become indistinguishable, so any method that relies on global distances (like **k-Means**) loses its ability to meaningfully separate points into clusters.

**k-Means clustering** exemplifies this breakdown: it repeatedly assigns each point to its nearest centroid based on squared-distance comparisons. When all inter-point distances look almost the same, tiny shifts in centroid positions barely affect those assignments, leading to noisy labels and flat optimization landscapes with no clear gradients. In practice, k-Means can ‚Äúget stuck‚Äù or fail to discover any meaningful grouping once dimensions rise into the dozens or hundreds.

**t-SNE** sidesteps these problems by focusing only on **local** similarities rather than global distances. It first converts pairwise distances in the high-dimensional space into a distribution of affinities $p_{ij}$ using Gaussian kernels centered on each point. Then it searches for a low-dimensional embedding whose Student-t affinity distribution $q_{ij}$ best matches $p_{ij}$. By emphasizing the preservation of each point‚Äôs nearest neighbors and using a heavy-tailed low-dimensional kernel to push dissimilar points apart, t-SNE highlights local clusters even when global geometry has become uninformative‚Äîmaking it a far more effective visualization and exploratory tool in very high dimensions.
:::
<!-- end callout -->



## Simple code to perform tSNE (hands-on exercise)

```{python ch3-tsne-prac}

import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.manifold import TSNE

# Load the Iris dataset
iris = datasets.load_iris()
X = iris.data            # The features (measurements)
y = iris.target          # The species labels (0, 1, 2)

# Run t-SNE to reduce the data to 2 dimensions
tsne = TSNE(n_components=2, random_state=0, perplexity=30)
X_2d = tsne.fit_transform(X)

# Plot the results, one species at a time
plt.figure(figsize=(8, 6))

# Setosa (label 0)
plt.scatter(X_2d[y == 0, 0], X_2d[y == 0, 1], color='red', label='setosa')

# Versicolor (label 1)
plt.scatter(X_2d[y == 1, 0], X_2d[y == 1, 1], color='green', label='versicolor')

# Virginica (label 2)
plt.scatter(X_2d[y == 2, 0], X_2d[y == 2, 1], color='blue', label='virginica')

plt.xlabel("t-SNE feature 1")
plt.ylabel("t-SNE feature 2")
plt.title("t-SNE visualization of the Iris dataset")
plt.legend()
plt.show()
```


## Exercise: tSNE is stochastic

* Stochasticity: play around with the `random_state` parameter. Does your tSNE plot look different to the person you are seated next to?

* Play around with the `perplexity` parameter (pair up with someone)

* Which value of `perplexity` should you use?



## Exercise: Building intuition on how to use tSNE


* [Pitfalls in using tSNE](https://distill.pub/2016/misread-tsne/)

* Let us read the paper *How to use t-SNE effectively* [@Wattenberg2016].

* Distances are not preserved

* Normal does not always look normal


## Key Concept

::: {.callout-tip}
#### Key Points

- Recall that unsupervised machine learning can help you come up with new hypotheses
- Vary the perplexity parameter: ideally your patterns or hypotheses should be true even if you change perplexity

:::
<!--end callout -->


## Exercise: hands-on practical applying tSNE to another dataset

<!--* XX TODO: FILL IN THE BLANKS EXERCISE

* XX TODO: Load real data from pandas -->

* Load the US Arrests data in Python and perform tSNE on this (pair up with a person)

* How would you evaluate this?

* Vary the `perplexity` parameter

* Some code to help you get started is here:

```{python ch3-tsne-usarrest_simplecode}
#| eval: false
#| echo: true

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

# Load the US Arrests data
url = "https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/main/course_files/data/USArrests.csv"
df = pd.read_csv(url, index_col=0)

# Prepare the data for t-SNE
X = df.values  # Convert to numpy array

# Fill in your code here ..........

```

```{python ch3-tsne-usarrest_simplecode}
#| eval: false
#| echo: false
#| output: false
#| warning: false

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

# Load the US Arrests data
url = "https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/main/course_files/data/USArrests.csv"
df = pd.read_csv(url, index_col=0)

# Prepare the data for t-SNE
X = df.values  # Convert to numpy array

# Run t-SNE to reduce the data to 2 dimensions
tsne = TSNE(n_components=2, random_state=42)
X_2d = tsne.fit_transform(X)

# Plot the results
plt.figure(figsize=(10, 8))
plt.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.7)

plt.xlabel("t-SNE feature 1")
plt.ylabel("t-SNE feature 2")
plt.title("t-SNE visualization of US Arrests data")
plt.show()
```

```{python ch3-tsne_usarrestdata}
#| eval: false
#| echo: false
#| output: false
#| warning: false

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

# Load the US Arrests data
url = "https://raw.githubusercontent.com/cambiotraining/ml-unsupervised/main/course_files/data/USArrests.csv"
df = pd.read_csv(url, index_col=0)

# Prepare the data for t-SNE
X = df.values  # Convert to numpy array

# Run t-SNE to reduce the data to 2 dimensions
tsne = TSNE(n_components=2, random_state=42)
X_2d = tsne.fit_transform(X)

# Plot the results
plt.figure(figsize=(10, 8))
plt.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.7)

# Add state labels to the points
for i, state in enumerate(df.index):
    plt.annotate(state, (X_2d[i, 0], X_2d[i, 1]), 
                xytext=(5, 5), textcoords='offset points', 
                fontsize=8)

plt.xlabel("t-SNE feature 1")
plt.ylabel("t-SNE feature 2")
plt.title("t-SNE visualization of US Arrests data")
plt.tight_layout()
plt.show()

```


<!-- TODO: but it might be good if the course went a little further than what‚Äôs typically found in most introductory resources. Some additional methods worth at least mentioning are:

MDS (multidimensional scaling)
PCoA (principal coordinates analysis)

-->


## Summary

::: {.callout-tip}
#### Key Points

- High dimensions make global distances meaningless.
- Methods that leverage local structure (t‚ÄëSNE) can still find patterns.
- tSNE is stochastic and can be hard to interpret
- Vary the perplexity parameter (ideally your patterns or hypotheses should be true even if you change perplexity)

:::
<!--end callout -->


## References

[1] [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/)
